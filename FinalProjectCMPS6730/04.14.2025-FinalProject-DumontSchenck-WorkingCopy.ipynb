{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19323,"status":"ok","timestamp":1744731189842,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"adqiHMoIxsrF","outputId":"8dbbef37-d39f-45d7-92dc-f67c0199f6e8"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Libraries imported successfully.\n"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import json\n","import re\n","import time\n","from datetime import datetime\n","\n","# Text Preprocessing\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download necessary NLTK data (run once)\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')\n","try:\n","    nltk.data.find('corpora/wordnet')\n","except LookupError:\n","    nltk.download('wordnet')\n","\n","# Scikit-learn for baseline models and metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_recall_fscore_support,\n","    roc_auc_score,\n","    classification_report,\n","    confusion_matrix,\n","    ConfusionMatrixDisplay,\n","    roc_curve,\n","    auc\n",")\n","from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n","\n","# PyTorch and Hugging Face Transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW  # Correct import for AdamW\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    get_linear_schedule_with_warmup,\n","    DistilBertTokenizer,  # Example using DistilBERT\n","    DistilBertForSequenceClassification\n",")\n","\n","# Plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","\n","print(\"Libraries imported successfully.\")\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":125691,"status":"ok","timestamp":1744731315540,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"sDKxsGKPyC0d","outputId":"bd73626a-b3b7-4857-baf6-c94a0cecb999"},"outputs":[{"output_type":"stream","name":"stdout","text":["Libraries imported successfully.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive mounted.\n","Review data path: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_review.json\n","Business data path: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_business.json\n","\n","Loading review data from: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_review.json\n","Review data loaded in 101.03 seconds.\n","Review dataset shape: (6990280, 9)\n","\n","Review Data Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 6990280 entries, 0 to 6990279\n","Data columns (total 9 columns):\n"," #   Column       Dtype         \n","---  ------       -----         \n"," 0   review_id    object        \n"," 1   user_id      object        \n"," 2   business_id  object        \n"," 3   stars        int64         \n"," 4   useful       int64         \n"," 5   funny        int64         \n"," 6   cool         int64         \n"," 7   text         object        \n"," 8   date         datetime64[ns]\n","dtypes: datetime64[ns](1), int64(4), object(4)\n","memory usage: 480.0+ MB\n","\n","Review Data Head:\n","                review_id                 user_id             business_id  \\\n","0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n","1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n","2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n","3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n","4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n","\n","   stars  useful  funny  cool  \\\n","0      3       0      0     0   \n","1      5       1      0     1   \n","2      3       0      0     0   \n","3      5       1      0     1   \n","4      4       1      0     1   \n","\n","                                                text                date  \n","0  If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n","1  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n","2  Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n","3  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n","4  Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n","\n","'date' column converted to datetime.\n","\n","Loading business data from: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_business.json\n","Business data loaded in 6.24 seconds.\n","Business dataset shape: (150346, 14)\n","\n","Business Data Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 150346 entries, 0 to 150345\n","Data columns (total 14 columns):\n"," #   Column        Non-Null Count   Dtype  \n","---  ------        --------------   -----  \n"," 0   business_id   150346 non-null  object \n"," 1   name          150346 non-null  object \n"," 2   address       150346 non-null  object \n"," 3   city          150346 non-null  object \n"," 4   state         150346 non-null  object \n"," 5   postal_code   150346 non-null  object \n"," 6   latitude      150346 non-null  float64\n"," 7   longitude     150346 non-null  float64\n"," 8   stars         150346 non-null  float64\n"," 9   review_count  150346 non-null  int64  \n"," 10  is_open       150346 non-null  int64  \n"," 11  attributes    136602 non-null  object \n"," 12  categories    150243 non-null  object \n"," 13  hours         127123 non-null  object \n","dtypes: float64(3), int64(2), object(9)\n","memory usage: 16.1+ MB\n","\n","Business Data Head:\n","              business_id                      name  \\\n","0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n","1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n","2  tUFrWirKiKi_TAnsVWINQQ                    Target   \n","3  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n","4  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n","\n","                           address           city state postal_code  \\\n","0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n","1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n","2             5255 E Broadway Blvd         Tucson    AZ       85711   \n","3                      935 Race St   Philadelphia    PA       19107   \n","4                    101 Walnut St     Green Lane    PA       18054   \n","\n","    latitude   longitude  stars  review_count  is_open  \\\n","0  34.426679 -119.711197    5.0             7        0   \n","1  38.551126  -90.335695    3.0            15        1   \n","2  32.223236 -110.880452    3.5            22        0   \n","3  39.955505  -75.155564    4.0            80        1   \n","4  40.338183  -75.471659    4.5            13        1   \n","\n","                                          attributes  \\\n","0                      {'ByAppointmentOnly': 'True'}   \n","1             {'BusinessAcceptsCreditCards': 'True'}   \n","2  {'BikeParking': 'True', 'BusinessAcceptsCredit...   \n","3  {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n","4  {'BusinessAcceptsCreditCards': 'True', 'Wheelc...   \n","\n","                                          categories  \\\n","0  Doctors, Traditional Chinese Medicine, Naturop...   \n","1  Shipping Centers, Local Services, Notaries, Ma...   \n","2  Department Stores, Shopping, Fashion, Home & G...   \n","3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n","4                          Brewpubs, Breweries, Food   \n","\n","                                               hours  \n","0                                               None  \n","1  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n","2  {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...  \n","3  {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...  \n","4  {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  \n","\n","Example Business Categories: Doctors, Traditional Chinese Medicine, Naturopathic/Holistic, Acupuncture, Health & Medical, Nutritionists\n"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import json\n","import re\n","import time\n","from datetime import datetime\n","\n","# Text Preprocessing\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download necessary NLTK data (run once)\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')\n","try:\n","    nltk.data.find('corpora/wordnet')\n","except LookupError:\n","    nltk.download('wordnet')\n","\n","# Scikit-learn for baseline models and metrics\n","# ... (rest of the imports) ...\n","\n","# Set random seed for reproducibility\n","# ... (seed setting) ...\n","\n","print(\"Libraries imported successfully.\")\n","\n","# --- File Paths ---\n","# Mount Google Drive\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    # Adjust the file paths accordingly\n","    review_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_review.json'\n","    business_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_business.json'\n","    print(\"Google Drive mounted.\")\n","    print(f\"Review data path: {review_file_path}\")\n","    print(f\"Business data path: {business_file_path}\")\n","except ModuleNotFoundError:\n","    # If not in Colab, use the local path directly (adjust if needed)\n","    review_file_path = r\"G:\\My Drive\\CMPS 6730 - NLP\\FinalProject\\yelp_academic_dataset_review.json\" # Use raw string\n","    business_file_path = r\"G:\\My Drive\\CMPS 6730 - NLP\\FinalProject\\yelp_academic_dataset_business.json\" # Use raw string\n","    print(\"Running locally. Ensure the file paths are correct.\")\n","    print(f\"Review data path: {review_file_path}\")\n","    print(f\"Business data path: {business_file_path}\")\n","\n","# --- Load Review Data in Chunks ---\n","print(f\"\\nLoading review data from: {review_file_path}\")\n","start_time = time.time()\n","\n","all_data = []\n","chunksize = 100000  # Adjust as needed\n","\n","for chunk in pd.read_json(review_file_path, lines=True, chunksize=chunksize):\n","    all_data.append(chunk)\n","\n","df_reviews = pd.concat(all_data, ignore_index=True)\n","\n","loading_time = time.time() - start_time\n","print(f\"Review data loaded in {loading_time:.2f} seconds.\")\n","print(f\"Review dataset shape: {df_reviews.shape}\")\n","print(\"\\nReview Data Info:\")\n","df_reviews.info()\n","print(\"\\nReview Data Head:\")\n","print(df_reviews.head())\n","\n","# Convert 'date' column to datetime objects\n","df_reviews['date'] = pd.to_datetime(df_reviews['date'])\n","print(\"\\n'date' column converted to datetime.\")\n","\n","# --- Load Business Data ---\n","print(f\"\\nLoading business data from: {business_file_path}\")\n","start_time = time.time()\n","try:\n","    df_business = pd.read_json(business_file_path, lines=True)\n","except ValueError as e:\n","    print(f\"Error reading business JSON with pandas: {e}. Trying line-by-line.\")\n","    business_data = []\n","    with open(business_file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            try:\n","                business_data.append(json.loads(line))\n","            except json.JSONDecodeError:\n","                print(f\"Skipping malformed line in businesses\")\n","                continue\n","    df_business = pd.DataFrame(business_data)\n","\n","loading_time = time.time() - start_time\n","print(f\"Business data loaded in {loading_time:.2f} seconds.\")\n","print(f\"Business dataset shape: {df_business.shape}\")\n","print(\"\\nBusiness Data Info:\")\n","df_business.info()\n","print(\"\\nBusiness Data Head:\")\n","print(df_business.head())\n","print(\"\\nExample Business Categories:\", df_business['categories'].iloc[0])"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3472,"status":"ok","timestamp":1744731319013,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"YNowSd2VyJLF","outputId":"12170448-9ba3-42fe-e684-6aad0832dfd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique target restaurant business IDs: 64629\n","\n","Reviews filtered by date (2019-01-01 to 2024-12-31). Shape: (2111695, 9)\n","Reviews filtered by target business IDs. Shape: (1525330, 9)\n","\n","Final shape for processing (filtered by date and location/category): (1525330, 5)\n","\n","Selected relevant columns ('review_id', 'text', 'stars', 'date', 'business_id').\n","                     review_id  \\\n","194087  F6VdYuJiefNBfn3HNELv0A   \n","194089  nAMDCKElSKxOhzm9Lpt6Eg   \n","194093  3CmdoGKBZUX3Nb5IfbztMg   \n","203214  -4Nv_JAolCM0gzKM4DZpmQ   \n","204271  uObbDRxP_cwJVADckZJDZw   \n","\n","                                                     text  stars  \\\n","194087  The food is INCREDIBLE! We didn't have time to...      5   \n","194089  We had a great time, and excellent service. Al...      5   \n","194093  My favorite coffee shop in New Orleans for sur...      5   \n","203214  Old school circa 1979, the cozy, intimate banq...      4   \n","204271  Delicious! Tried this place on Saturday and it...      5   \n","\n","                      date             business_id  \n","194087 2019-01-04 02:18:09  z7em5co2qckbAXoDGXynsA  \n","194089 2019-01-06 11:48:21  M0r9lUn2gLFYgIwIfG8-bQ  \n","194093 2019-01-27 15:08:14  itAhmbhHOyQQparfwicjDQ  \n","203214 2019-02-17 20:28:26  -3AooxIkg38UyUdlz5oXdw  \n","204271 2019-02-11 14:59:40  WLiqfxv_GhFFA5sm878a2w  \n"]}],"source":["# Filter data based on your criteria (date range AND category)\n","\n","# 1. Filter Businesses by Category\n","\n","\n","# Filter by category (must contain 'Restaurants' or 'Food') - ALL locations\n","df_business['categories'] = df_business['categories'].fillna('')\n","all_restaurants = df_business[\n","    df_business['categories'].str.contains('Restaurant|Food', case=False, regex=True)\n","].copy()\n","\n","target_business_ids = set(all_restaurants['business_id'])\n","print(f\"Number of unique target restaurant business IDs: {len(target_business_ids)}\")\n","\n","if not target_business_ids:\n","    print(\"\\nWarning: No restaurant business IDs found for the specified location. Check city names and categories.\")\n","    # Handle this case - perhaps stop execution or proceed with only date filtering?\n","    # For now, we'll let it proceed, resulting in an empty review dataframe later.\n","\n","# 2. Filter Reviews by Date\n","start_date = datetime(2019, 1, 1)\n","end_date = datetime(2024, 12, 31) # Use end of 2024 for completeness\n","\n","df_reviews_filtered_date = df_reviews[(df_reviews['date'] >= start_date) & (df_reviews['date'] <= end_date)].copy()\n","print(f\"\\nReviews filtered by date ({start_date.date()} to {end_date.date()}). Shape: {df_reviews_filtered_date.shape}\")\n","\n","# 3. Filter Date-Filtered Reviews by Target Business IDs\n","if target_business_ids:\n","    df_filtered = df_reviews_filtered_date[\n","        df_reviews_filtered_date['business_id'].isin(target_business_ids)\n","    ].copy()\n","    print(f\"Reviews filtered by target business IDs. Shape: {df_filtered.shape}\")\n","else:\n","    print(\"\\nSkipping business ID filtering as no target IDs were found.\")\n","    df_filtered = pd.DataFrame(columns=df_reviews_filtered_date.columns) # Create empty DataFrame matching columns\n","\n","\n","# Select relevant columns for final processing\n","# Keep 'business_id' if you might need it later, otherwise drop it\n","df_processed = df_filtered[['review_id', 'text', 'stars', 'date', 'business_id']].copy()\n","\n","print(f\"\\nFinal shape for processing (filtered by date and location/category): {df_processed.shape}\")\n","\n","if df_processed.empty:\n","     print(\"\\nWARNING: No reviews match the filtering criteria (Date + Location/Category). Subsequent steps will fail.\")\n","     # Consider stopping execution here if the dataframe is empty.\n","else:\n","    print(\"\\nSelected relevant columns ('review_id', 'text', 'stars', 'date', 'business_id').\")\n","    print(df_processed.head())\n","\n","# --- Cleanup (Optional: remove intermediate dataframes to save memory) ---\n","# del df_reviews, df_business, df_reviews_filtered_date, businesses_in_location, restaurants_in_location\n","# import gc\n","# gc.collect()\n","# print(\"\\nIntermediate dataframes cleaned up.\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dzRnsH9WoR5Z","executionInfo":{"status":"ok","timestamp":1744731319016,"user_tz":300,"elapsed":3,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2871375,"status":"ok","timestamp":1744734190448,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"cjQ70p3A84vL","outputId":"0591431a-3a49-407f-a1d5-9fe5e0d66cba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Downloading NLTK 'vader_lexicon'...\n","Downloading NLTK 'punkt_tab'...\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Output file with sentiment scores will be saved to: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv\n","\n","Starting sentiment analysis process on 1525330 filtered reviews.\n","Analyzing review sentiments (this may take a while)...\n","Sentiment analysis complete.\n","\n","Successfully exported 1525330 reviews with sentiment scores to:\n","/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv\n","\n","--- NEXT STEPS ---\n","1. Explore the calculated sentiment scores (e.g., correlations with 'stars').\n","2. Refine the aspect keyword lists for better classification accuracy.\n","3. Consider alternative sentiment lexicons or models if needed.\n","4. Use these sentiment scores as features for further analysis or modeling.\n","\n","Preview of DataFrame with Sentiment Scores:\n","                review_id             business_id                date  stars  \\\n","0  F6VdYuJiefNBfn3HNELv0A  z7em5co2qckbAXoDGXynsA 2019-01-04 02:18:09      5   \n","1  nAMDCKElSKxOhzm9Lpt6Eg  M0r9lUn2gLFYgIwIfG8-bQ 2019-01-06 11:48:21      5   \n","2  3CmdoGKBZUX3Nb5IfbztMg  itAhmbhHOyQQparfwicjDQ 2019-01-27 15:08:14      5   \n","3  -4Nv_JAolCM0gzKM4DZpmQ  -3AooxIkg38UyUdlz5oXdw 2019-02-17 20:28:26      4   \n","4  uObbDRxP_cwJVADckZJDZw  WLiqfxv_GhFFA5sm878a2w 2019-02-11 14:59:40      5   \n","\n","                                                text  food_sentiment  \\\n","0  The food is INCREDIBLE! We didn't have time to...         2.15690   \n","1  We had a great time, and excellent service. Al...         0.07955   \n","2  My favorite coffee shop in New Orleans for sur...         1.60730   \n","3  Old school circa 1979, the cozy, intimate banq...         0.00000   \n","4  Delicious! Tried this place on Saturday and it...         0.30570   \n","\n","   service_sentiment  ambiance_sentiment  price_sentiment  context_sentiment  \\\n","0           0.026411                 0.0              0.0             0.0000   \n","1           0.803550                 0.0              0.0             0.0000   \n","2           0.000000                 0.0              0.0             0.0000   \n","3           0.000000                 0.0              0.0             0.2202   \n","4           0.000000                 0.0              0.0             0.0000   \n","\n","   other_sentiment  \n","0         0.423467  \n","1         0.123175  \n","2         0.000000  \n","3         0.000000  \n","4         0.447250  \n"]}],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import re # For basic text cleaning\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","\n","# --- Download NLTK data (only needs to be done once) ---\n","!pip install nltk\n","import nltk\n","\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError: # Changed to catch LookupError\n","    print(\"Downloading NLTK 'punkt' tokenizer...\")\n","    nltk.download('punkt', quiet=True)\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError: # Changed to catch LookupError\n","    print(\"Downloading NLTK 'stopwords'...\")\n","    nltk.download('stopwords', quiet=True)\n","try:\n","    nltk.data.find('sentiment/vader_lexicon.zip')\n","except LookupError: # Changed to catch LookupError\n","    print(\"Downloading NLTK 'vader_lexicon'...\")\n","    nltk.download('vader_lexicon', quiet=True)\n","\n","try:\n","    nltk.data.find('tokenizers/punkt_tab') # Check if the resource is already present\n","except LookupError:\n","    print(\"Downloading NLTK 'punkt_tab'...\")\n","    nltk.download('punkt_tab') # Download specifically 'punkt_tab'\n","\n","\n","# --- Configuration ---\n","# Define the path to your processed data (assuming df_processed is loaded elsewhere)\n","# If df_processed is not loaded, you'll need to load it first, e.g.:\n","# input_data_file = 'path/to/your/processed_reviews.csv'\n","# df_processed = pd.read_csv(input_data_file)\n","\n","# Define where to save the file with sentiment scores\n","try:\n","    # Assume Google Drive is mounted if in Colab\n","    output_sentiment_file = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n","    print(f\"Output file with sentiment scores will be saved to: {output_sentiment_file}\")\n","except NameError: # If 'drive' object doesn't exist (running locally)\n","    # Adjust local path if necessary\n","    output_sentiment_file = r\"G:\\My Drive\\CMPS 6730 - NLP\\FinalProject\\reviews_with_sentiment_scores.csv\"\n","    print(f\"Output file with sentiment scores will be saved to: {output_sentiment_file}\")\n","\n","# --- Aspect Keyword Lists (Illustrative Examples - EXPAND THESE SIGNIFICANTLY) ---\n","# Based on the paper, these lists should be comprehensive.\n","# Consider synonyms, related terms, common phrases.\n","\n","# --- Aspect Keyword Lists (EXPANDED - CONTINUE ADDING MORE!) ---\n","aspect_keywords = {\n","    'food': [\n","        # General Food\n","        'food', 'dish', 'meal', 'plate', 'menu', 'cuisine', 'recipe', 'ingredients', 'eat', 'ate', 'dining',\n","        # Taste & Quality\n","        'taste', 'flavor', 'delicious', 'tasty', 'yummy', 'savory', 'sweet', 'sour', 'bitter', 'spicy', 'hot',\n","        'fresh', 'quality', 'authentic', 'homemade', 'cooked', 'preparation', 'appetizer', 'entree', 'dessert',\n","        'portion', 'serving', 'burnt', 'undercooked', 'overcooked', 'bland', 'seasoning',\n","        # Specific Items (Examples - Add many more common ones)\n","        'chicken', 'beef', 'pork', 'fish', 'seafood', 'shrimp', 'crab', 'lobster', 'steak', 'burger', 'sandwich',\n","        'pizza', 'pasta', 'sushi', 'taco', 'salad', 'soup', 'bread', 'fries', 'rice', 'noodles', 'vegetables',\n","        'cake', 'pie', 'ice cream', 'coffee', 'tea', 'drink', 'beverage', 'wine', 'beer', 'cocktail'\n","    ],\n","    'service': [\n","        # Staff General\n","        'service', 'staff', 'server', 'waiter', 'waitress', 'waitstaff', 'host', 'hostess', 'bartender', 'manager', 'employee',\n","        # Staff Behavior\n","        'friendly', 'attentive', 'helpful', 'professional', 'polite', 'courteous', 'welcoming', 'accommodating',\n","        'rude', 'unfriendly', 'inattentive', 'slow', 'ignored', 'forgetful', 'unprofessional', 'arrogant', 'bad', 'terrible'\n","        # Process\n","        'wait', 'waiting', 'order', 'refill', 'check', 'bill', 'reservation', 'seated', 'prompt', 'quick', 'efficient',\n","        'mistake', 'error', 'issue', 'problem', 'complaint', 'request'\n","    ],\n","    'ambiance': [\n","        # General Feel\n","        'ambiance', 'atmosphere', 'vibe', 'decor', 'setting', 'environment', 'interior', 'design', 'layout',\n","        # Sensory\n","        'music', 'lighting', 'loud', 'noisy', 'quiet', 'sound', 'smell',\n","        # Comfort & Cleanliness\n","        'comfortable', 'cozy', 'relaxing', 'upscale', 'casual', 'romantic', 'view',\n","        'clean', 'dirty', 'tidy', 'messy', 'hygiene', 'restroom', 'bathroom', 'tables',\n","        # Space\n","        'crowded', 'spacious', 'seating', 'booth', 'patio', 'outdoor'\n","    ],\n","    'price': [\n","        # General Cost\n","        'price', 'cost', 'value', 'money', 'budget', 'bill', 'charge', 'worth', 'pay', 'paid', 'tab',\n","        # Affordability\n","        'cheap', 'expensive', 'affordable', 'overpriced', 'reasonable', 'deal', 'bargain', 'pricey', 'costly',\n","        'value for money', 'rip-off', 'discount', 'coupon', 'special'\n","    ],\n","    'context': [\n","        # Occasion\n","        'occasion', 'birthday', 'anniversary', 'celebration', 'date', 'romantic', 'special', 'holiday',\n","        # Company\n","        'friends', 'family', 'kids', 'children', 'group', 'party', 'business', 'work', 'solo', 'couple',\n","        # Time/Meal Type\n","        'lunch', 'dinner', 'brunch', 'breakfast', 'late night', 'happy hour',\n","        # Location Related (can overlap with ambiance)\n","        'location', 'neighborhood', 'parking', 'visit', 'trip', 'tourist'\n","     ]\n","}\n","# --- Initialize Sentiment Analyzer ---\n","analyzer = SentimentIntensityAnalyzer()\n","stop_words = set(stopwords.words('english'))\n","\n","# --- Helper Functions ---\n","\n","def clean_text(text):\n","    \"\"\"Basic text cleaning.\"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    text = text.lower() # Lowercase\n","    text = re.sub(r'\\d+', '', text) # Remove numbers\n","    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n","    text = text.strip() # Remove leading/trailing whitespace\n","    return text\n","\n","def classify_sentence_aspect(sentence, aspect_kw):\n","    \"\"\"\n","    Classifies a sentence into one aspect based on keyword counts.\n","    Follows the paper's method: assign to aspect with most keywords.\n","    \"\"\"\n","    # Clean and tokenize the sentence\n","    cleaned_sentence = clean_text(sentence)\n","    words = word_tokenize(cleaned_sentence)\n","    # Remove stop words for better keyword matching\n","    words = [word for word in words if word not in stop_words]\n","\n","    scores = {aspect: 0 for aspect in aspect_kw}\n","    max_score = 0\n","    best_aspect = 'other' # Default if no keywords match\n","\n","    # Count keywords for each aspect\n","    for aspect, keywords in aspect_kw.items():\n","        # Use a set for faster checking\n","        keyword_set = set(keywords)\n","        # Count occurrences of keywords in the sentence words\n","        score = sum(1 for word in words if word in keyword_set)\n","        scores[aspect] = score\n","        if score > max_score:\n","            max_score = score\n","            best_aspect = aspect\n","        # Handle ties (optional, could assign to 'multiple' or prioritize)\n","        # elif score == max_score and max_score > 0:\n","        #     best_aspect = 'multiple' # Or handle differently\n","\n","    # Ensure we only classify if at least one keyword was found\n","    if max_score == 0:\n","        best_aspect = 'other'\n","\n","    return best_aspect\n","\n","def get_sentence_sentiment(sentence):\n","    \"\"\"\n","    Calculates the VADER compound sentiment score for a sentence.\n","    Ranges from -1 (most negative) to +1 (most positive).\n","    The paper used AFINN (-5 to +5), VADER is a common alternative.\n","    \"\"\"\n","    # VADER's polarity_scores returns dict: {'neg': %, 'neu': %, 'pos': %, 'compound': score}\n","    vs = analyzer.polarity_scores(sentence)\n","    return vs['compound'] # Use the compound score as the overall sentiment\n","\n","def analyze_review_sentiment(review_text, aspect_kw):\n","    \"\"\"\n","    Analyzes a full review text to get weighted sentiment scores per aspect.\n","    Implements the methodology from the paper.\n","    \"\"\"\n","    if not isinstance(review_text, str) or not review_text.strip():\n","        # Return default scores for empty or invalid reviews\n","        return {f'{aspect}_sentiment': 0.0 for aspect in aspect_kw} | {'other_sentiment': 0.0}\n","\n","    # 1. Tokenize into sentences\n","    sentences = sent_tokenize(review_text)\n","    total_sentences = len(sentences)\n","\n","    if total_sentences == 0:\n","        # Handle reviews that couldn't be sentence-tokenized\n","         return {f'{aspect}_sentiment': 0.0 for aspect in aspect_kw} | {'other_sentiment': 0.0}\n","\n","    # 2. Classify aspect and get sentiment for each sentence\n","    sentence_results = []\n","    for sentence in sentences:\n","        aspect = classify_sentence_aspect(sentence, aspect_kw)\n","        sentiment = get_sentence_sentiment(sentence)\n","        sentence_results.append({'aspect': aspect, 'sentiment': sentiment})\n","\n","    # 3. Aggregate sentiment scores and count sentences per aspect\n","    aspect_sentiments_sum = defaultdict(float)\n","    aspect_sentence_counts = defaultdict(int)\n","\n","    for result in sentence_results:\n","        aspect = result['aspect']\n","        sentiment = result['sentiment']\n","        aspect_sentiments_sum[aspect] += sentiment\n","        aspect_sentence_counts[aspect] += 1\n","\n","    # 4. Calculate weighted sentiment scores (as per paper's formula)\n","    weighted_scores = {}\n","    all_aspects = list(aspect_kw.keys()) + ['other'] # Include 'other' category\n","\n","    for aspect in all_aspects:\n","        sum_score = aspect_sentiments_sum[aspect]\n","        count = aspect_sentence_counts[aspect]\n","\n","        # Calculate weight (proportion of sentences for this aspect)\n","        weight = count / total_sentences if total_sentences > 0 else 0\n","\n","        # Weighted score = Sum * Weight (or proportion)\n","        # Note: The paper's formula description is slightly ambiguous.\n","        # Sentiment_ij = Sentiment Score_ij * (# sentences Attribute j / # sentences Attribute_ij)\n","        # This implies weighting the *average* score per attribute.\n","        # Let's calculate both average and weighted sum for clarity.\n","        # average_score = sum_score / count if count > 0 else 0.0\n","        # weighted_score_avg_based = average_score * weight # Avg score weighted by proportion\n","\n","        # Alternative interpretation: Weight the *total* sentiment sum for the aspect\n","        # This seems more aligned with capturing overall impact.\n","        weighted_score_sum_based = sum_score * weight\n","\n","        # Store the weighted score based on the sum interpretation\n","        weighted_scores[f'{aspect}_sentiment'] = weighted_score_sum_based\n","\n","    return weighted_scores\n","\n","def get_overall_sentiment(review_text):  # <-- Add this function here\n","    \"\"\"Calculates the overall sentiment score for the entire review.\"\"\"\n","    if not isinstance(review_text, str) or not review_text.strip():\n","        return 0.0  # Return 0 for empty or invalid reviews\n","\n","    # Use VADER to get the compound sentiment score\n","    scores = analyzer.polarity_scores(review_text)\n","    return scores['compound']\n","\n","# --- Main Processing Logic ---\n","\n","# Check if df_processed exists and is populated\n","# IMPORTANT: Ensure df_processed is loaded before this point if not already in memory.\n","# Example:\n","# try:\n","#     df_processed = pd.read_csv('path/to/your/filtered_reviews.csv')\n","#     print(f\"Loaded {len(df_processed)} reviews.\")\n","# except FileNotFoundError:\n","#     print(\"Error: Processed review file not found.\")\n","#     df_processed = pd.DataFrame() # Create empty df to avoid error\n","\n","if 'df_processed' not in locals() or not isinstance(df_processed, pd.DataFrame) or df_processed.empty:\n","    print(\"\\nError: The 'df_processed' DataFrame is empty or does not exist.\")\n","    print(\"Please ensure your data loading step ran successfully.\")\n","    # Optionally, raise an error:\n","    # raise ValueError(\"Cannot proceed without data in df_processed.\")\n","else:\n","    print(f\"\\nStarting sentiment analysis process on {len(df_processed)} filtered reviews.\")\n","\n","    # Make a copy to avoid modifying the original DataFrame directly\n","    df_analysis = df_processed.copy()\n","\n","    # --- Apply Sentiment Analysis ---\n","    print(\"Analyzing review sentiments (this may take a while)...\")\n","\n","    # Use .apply() to process the 'text' column of each review\n","    # The result of apply will be a Series of dictionaries\n","    sentiment_results = df_analysis['text'].apply(lambda text: analyze_review_sentiment(text, aspect_keywords))\n","\n","    # Convert the Series of dictionaries into separate columns in the DataFrame\n","    # pd.json_normalize is efficient for this\n","    sentiment_df = pd.json_normalize(sentiment_results)\n","\n","    # Add these new sentiment score columns to the analysis DataFrame\n","    df_analysis = pd.concat([df_analysis.reset_index(drop=True), sentiment_df.reset_index(drop=True)], axis=1)\n","\n","    print(\"Sentiment analysis complete.\")\n","\n","    # --- Select and Reorder Columns for Output ---\n","    # Keep original identifiers and add the new sentiment scores\n","    output_columns = ['review_id', 'business_id', 'date', 'stars', 'text'] + \\\n","                     [f'{aspect}_sentiment' for aspect in aspect_keywords] + \\\n","                     ['other_sentiment'] # Include 'other' if calculated\n","\n","    # Ensure all expected columns exist before selecting\n","    final_columns = [col for col in output_columns if col in df_analysis.columns]\n","    df_final_output = df_analysis[final_columns]\n","\n","    # --- Export to CSV ---\n","    try:\n","        # Use index=False to avoid writing the DataFrame index as a column\n","        df_final_output.to_csv(output_sentiment_file, index=False, encoding='utf-8')\n","        print(f\"\\nSuccessfully exported {len(df_final_output)} reviews with sentiment scores to:\")\n","        print(output_sentiment_file)\n","        print(\"\\n--- NEXT STEPS ---\")\n","        print(\"1. Explore the calculated sentiment scores (e.g., correlations with 'stars').\")\n","        print(\"2. Refine the aspect keyword lists for better classification accuracy.\")\n","        print(\"3. Consider alternative sentiment lexicons or models if needed.\")\n","        print(\"4. Use these sentiment scores as features for further analysis or modeling.\")\n","\n","    except Exception as e:\n","        print(f\"\\nError exporting file: {e}\")\n","        print(\"Please check the output path and ensure you have write permissions.\")\n","\n","# Optional: Display the first few rows with the new scores\n","if 'df_final_output' in locals():\n","    print(\"\\nPreview of DataFrame with Sentiment Scores:\")\n","    print(df_final_output.head())"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":710775,"status":"ok","timestamp":1744734901222,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"HSAHlEOvyNlH","outputId":"35e33a2e-3f41-46f3-c336-149b4c4704e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting text preprocessing...\n","Text preprocessing completed in 710.21 seconds.\n","\n","Original vs Processed Text Examples:\n","--- Example 1 ---\n","Original: The food is INCREDIBLE! We didn't have time to get any of the cocktails, but they had some pretty creative and classy ones on the menu. Will be going back for that alone. But anyways they have this wa...\n","Processed: food incredible didnt time get cocktail pretty creative classy one menu going back alone anyways way taking unique twist thing absolutely killing oh god im going talk cole slaw minute dont know whole world suck cant cole slaw perfect like somehow straight red cabbage think also creamy tangy light checked box didnt even know possible also need mention peppermint cheesecake desert chocolately browniey crust wonderful world caught cheese cake world thing make sense course also mention fried chicken supper came cole slaw deliberately mentioning absolutely phenomenal breading flavorful there nice spicy honey drizzle really finish also came corn bread par famous daves without sugary youre corn bread know easy comparison sum think best example great place pickled green tomato came fried chicken know like time think nice plate fried chicken itd garnished nice pickle well thats fine dandy blue duck throw something little unique turn fantastic pair everything together perfectly create unbelievable dining experience waiter nice\n","--------------------\n","--- Example 2 ---\n","Original: We had a great time, and excellent service. All food items were fresh, unique, and high quality. Nothing appeared to be pre-made,\n","Except for the ice cream which was exceptionally good. The waitress wa...\n","Processed: great time excellent service food item fresh unique high quality nothing appeared premade except ice cream exceptionally good waitress sweet friendly\n","--------------------\n","--- Example 3 ---\n","Original: My favorite coffee shop in New Orleans for sure! Perfect to study (quiet environment), super nice and hospital staff, such a bright space, and the coffee is full of flavor!...\n","Processed: favorite coffee shop new orleans sure perfect study quiet environment super nice hospital staff bright space coffee full flavor\n","--------------------\n","\n","Shape after removing empty processed texts: (1525313, 6)\n"]}],"source":["# Initialize lemmatizer and stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import time\n","\n","# Download the necessary NLTK data if it's not already present\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    nltk.download('punkt_tab')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text(text):\n","    \"\"\"Cleans and preprocesses text data.\"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    # Lowercase\n","    text = text.lower()\n","    # Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    # Remove user @ references and '#' from tweet\n","    text = re.sub(r'\\@\\w+|\\#','', text)\n","    # Remove punctuation\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    # Tokenize\n","    tokens = word_tokenize(text)\n","    # Remove stopwords and lemmatize\n","    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\n","    return ' '.join(processed_tokens)\n","\n","# Apply preprocessing (can take time on large datasets)\n","print(\"\\nStarting text preprocessing...\")\n","start_time = time.time()\n","df_processed['processed_text'] = df_processed['text'].apply(preprocess_text)\n","processing_time = time.time() - start_time\n","print(f\"Text preprocessing completed in {processing_time:.2f} seconds.\")\n","\n","# Display some processed text examples\n","print(\"\\nOriginal vs Processed Text Examples:\")\n","for i in range(3):\n","    print(f\"--- Example {i+1} ---\")\n","    print(\"Original:\", df_processed['text'].iloc[i][:200] + \"...\") # Show first 200 chars\n","    print(\"Processed:\", df_processed['processed_text'].iloc[i])\n","    print(\"-\" * 20)\n","\n","# Drop rows where processed text is empty\n","df_processed = df_processed[df_processed['processed_text'] != '']\n","print(f\"\\nShape after removing empty processed texts: {df_processed.shape}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1173528,"status":"ok","timestamp":1744736074751,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"IsTW9F96yahm","outputId":"c63efebc-a87a-4257-c7aa-9b7e385f86ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentiment score columns not found in df_processed. Calculating or loading them...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using fixed bin edges: [-inf, -0.05, 0.05, inf]\n","Assigning integer labels: [0, 1, 2] (negative, neutral, positive)\n","\n","Sentiment scores binned using fixed thresholds:\n","   food_sentiment  service_sentiment  ambiance_sentiment  price_sentiment  \\\n","0               2                  1                   1                1   \n","1               2                  2                   1                1   \n","2               2                  1                   1                1   \n","3               1                  1                   1                1   \n","4               2                  1                   1                1   \n","\n","   context_sentiment  overall_sentiment  \n","0                  1                  2  \n","1                  1                  2  \n","2                  1                  2  \n","3                  2                  2  \n","4                  1                  2  \n","\n","Value counts for 'food_sentiment' (example):\n","food_sentiment\n","2    1019023\n","1     385113\n","0     121177\n","Name: count, dtype: int64\n","Training set size: 1220250\n","Testing set size: 305063\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer  # Import SentimentIntensityAnalyzer\n","\n","\n","# Initialize SentimentIntensityAnalyzer (if not already initialized)\n","analyzer = SentimentIntensityAnalyzer()\n","\n","def get_overall_sentiment(review_text):  # <-- Add this function here\n","    \"\"\"Calculates the overall sentiment score for the entire review.\"\"\"\n","    if not isinstance(review_text, str) or not review_text.strip():\n","        return 0.0  # Return 0 for empty or invalid reviews\n","\n","    # Use VADER to get the compound sentiment score\n","    scores = analyzer.polarity_scores(review_text)\n","    return scores['compound']\n","# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n","\n","# 1. Load or calculate sentiment scores if not in df_processed\n","try:\n","    # If sentiment scores are already in df_processed, use them\n","    sentiment_columns = ['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']\n","    Y = df_processed[sentiment_columns].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","except KeyError:\n","    print(\"Sentiment score columns not found in df_processed. Calculating or loading them...\")\n","    # Calculate overall sentiment for each review\n","    df_processed['overall_sentiment'] = df_processed['text'].apply(get_overall_sentiment)\n","    # Option 2: Load from a separate file (if you saved them earlier)\n","    from google.colab import drive\n","    drive.mount('/content/drive')  # Mount Google Drive\n","    sentiment_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n","\n","    sentiment_df = pd.read_csv(sentiment_file_path)\n","    # Merge sentiment scores with the original dataframe based on a common column (e.g., 'review_id')\n","    df_processed = pd.merge(\n","        df_processed,\n","        sentiment_df[['review_id', 'food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']],\n","        on='review_id',\n","        how='left'\n","    )\n","    Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']].copy()\n","\n","# --- 2. Correct sentiment binning using fixed thresholds ---\n","\n","# Define fixed VADER thresholds and corresponding integer labels\n","# Bins: (-inf, -0.05], (-0.05, 0.05), [0.05, +inf)\n","# We add +/- infinity to ensure all values are covered.\n","bin_edges = [-float('inf'), -0.05, 0.05, float('inf')]\n","# Assign integer labels: 0 for Negative, 1 for Neutral, 2 for Positive\n","bin_labels = [0, 1, 2]\n","label_names = ['negative', 'neutral', 'positive'] # For potential use in reports if needed\n","\n","print(f\"Using fixed bin edges: {bin_edges}\")\n","print(f\"Assigning integer labels: {bin_labels} ({', '.join(label_names)})\")\n","\n","# Ensure Y is a DataFrame (it should be from previous steps)\n","if not isinstance(Y, pd.DataFrame):\n","     raise TypeError(\"Y should be a pandas DataFrame at this stage.\")\n","\n","# Create a new DataFrame for binned labels to avoid modifying Y inplace initially\n","Y_binned = pd.DataFrame(index=Y.index)\n","\n","for column in Y.columns:\n","    # Cast column values to float64 explicitly to avoid dtype issues\n","    # Use .loc to avoid SettingWithCopyWarning if Y is a slice\n","    Y_column_float = Y.loc[:, column].astype(float)\n","\n","    # Use pd.cut for binning with fixed edges\n","    # include_lowest=True: includes the lowest value (-inf) in the first bin\n","    # right=True (default): bins are (edge1, edge2], except first which is [edge1, edge2] due to include_lowest\n","    # If you want bins like [edge1, edge2), use right=False\n","    binned_data = pd.cut(\n","        Y_column_float,\n","        bins=bin_edges,\n","        labels=bin_labels,\n","        include_lowest=True,\n","        right=True # Standard VADER thresholds often use >= 0.05 for positive, <= -0.05 for negative\n","    )\n","\n","    # Check for NaNs introduced by binning (shouldn't happen with inf edges, but good practice)\n","    if binned_data.isnull().any():\n","        print(f\"Warning: NaNs found in '{column}' after binning. Check original data.\")\n","        # Handle NaNs if necessary, e.g., fill with neutral=1 or drop rows\n","        # binned_data = binned_data.fillna(1) # Example: Fill NaN with Neutral\n","\n","    # Assign the binned data (as integers) to the new DataFrame\n","    Y_binned[column] = binned_data.astype(int)\n","\n","# Replace the original Y with the binned version\n","Y = Y_binned\n","print(\"\\nSentiment scores binned using fixed thresholds:\")\n","print(Y.head())\n","print(\"\\nValue counts for 'food_sentiment' (example):\")\n","print(Y['food_sentiment'].value_counts())\n","\n","# --- End of binning modification ---\n","\n","# 3. Prepare data for training and testing\n","# Assuming 'processed_text' column contains preprocessed text data\n","X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n","\n","# Get the first sentiment column for stratification\n","stratify_column = Y.iloc[:, 0]  # Assuming Y is a DataFrame\n","\n","# Split data into training and testing sets\n","X_train, X_test, Y_train, Y_test = train_test_split(\n","    X, Y, test_size=0.2, random_state=42, stratify=stratify_column\n",")\n","\n","# Print dataset sizes\n","print(f\"Training set size: {len(X_train)}\")\n","print(f\"Testing set size: {len(X_test)}\")\n","\n","# ... (rest of the code, including TF-IDF, tokenization, and model training) ..."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":423075,"status":"ok","timestamp":1744736497837,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"7PoVnJaXV0XT","outputId":"5ede7606-3c84-4d34-fbbf-f5831da55e02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using fixed bin edges: [-inf, -0.05, 0.05, inf]\n","Assigning integer labels: [0, 1, 2] (negative, neutral, positive)\n","\n","Sentiment scores binned using fixed thresholds:\n","   food_sentiment  service_sentiment  ambiance_sentiment  price_sentiment  \\\n","0               2                  1                   1                1   \n","1               2                  2                   1                1   \n","2               2                  1                   1                1   \n","3               1                  1                   1                1   \n","4               2                  1                   1                1   \n","\n","   context_sentiment  overall_sentiment  \n","0                  1                  2  \n","1                  1                  2  \n","2                  1                  2  \n","3                  2                  2  \n","4                  1                  2  \n","\n","Value counts for 'food_sentiment' (example):\n","food_sentiment\n","2    1019023\n","1     385113\n","0     121177\n","Name: count, dtype: int64\n","Training set size: 1220250\n","Testing set size: 305063\n","\n","Training Baseline Model (Logistic Regression with OneVsRest)...\n","\n","--- Baseline (Logistic Regression) Classification Report (Per Aspect) ---\n","\n","--- Aspect: food ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.64      0.29      0.40     24235\n","     neutral       0.72      0.68      0.70     77023\n","    positive       0.85      0.92      0.89    203805\n","\n","    accuracy                           0.81    305063\n","   macro avg       0.74      0.63      0.66    305063\n","weighted avg       0.80      0.81      0.80    305063\n","\n","\n","--- Aspect: service ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.71      0.52      0.60     25208\n","     neutral       0.81      0.90      0.85    187376\n","    positive       0.76      0.64      0.70     92479\n","\n","    accuracy                           0.79    305063\n","   macro avg       0.76      0.69      0.72    305063\n","weighted avg       0.79      0.79      0.79    305063\n","\n","\n","--- Aspect: ambiance ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.64      0.19      0.29      3174\n","     neutral       0.94      0.98      0.96    270947\n","    positive       0.70      0.49      0.58     30942\n","\n","    accuracy                           0.92    305063\n","   macro avg       0.76      0.55      0.61    305063\n","weighted avg       0.91      0.92      0.91    305063\n","\n","\n","--- Aspect: price ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.42      0.06      0.11      3099\n","     neutral       0.95      0.99      0.97    286154\n","    positive       0.56      0.28      0.37     15810\n","\n","    accuracy                           0.94    305063\n","   macro avg       0.65      0.44      0.48    305063\n","weighted avg       0.93      0.94      0.93    305063\n","\n","\n","--- Aspect: context ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.51      0.04      0.08      5564\n","     neutral       0.89      0.96      0.93    253527\n","    positive       0.66      0.45      0.53     45972\n","\n","    accuracy                           0.87    305063\n","   macro avg       0.69      0.48      0.51    305063\n","weighted avg       0.85      0.87      0.85    305063\n","\n","\n","--- Aspect: overall_sentiment ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.79      0.75      0.77     42897\n","     neutral       0.72      0.03      0.05      4096\n","    positive       0.95      0.97      0.96    258070\n","\n","    accuracy                           0.93    305063\n","   macro avg       0.82      0.58      0.59    305063\n","weighted avg       0.93      0.93      0.92    305063\n","\n","\n","--- Baseline Overall Micro/Macro Averages ---\n","Micro Average: Precision=0.8774, Recall=0.8774, F1-Score=0.8774\n","Macro Average: Precision=0.8329, Recall=0.7613, F1-Score=0.7898\n"]}],"source":["# --- Import necessary libraries ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n","\n","# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n","\n","# 1. Load or calculate sentiment scores if not in df_processed\n","try:\n","    # If sentiment scores are already in df_processed, use them\n","    sentiment_columns = ['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']\n","    Y = df_processed[sentiment_columns].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","except KeyError:\n","    print(\"Sentiment score columns not found in df_processed. Calculating or loading them...\")\n","    # Option 2: Load from a separate file (if you saved them earlier)\n","    from google.colab import drive\n","    drive.mount('/content/drive')  # Mount Google Drive\n","    sentiment_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n","\n","    sentiment_df = pd.read_csv(sentiment_file_path)\n","    # Merge sentiment scores with the original dataframe based on a common column (e.g., 'review_id')\n","    df_processed = pd.merge(\n","        df_processed,\n","        sentiment_df[['review_id', 'food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']],\n","        on='review_id',\n","        how='left'\n","    )\n","    Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']].copy()\n","\n","# --- 2. Correct sentiment binning using fixed thresholds ---\n","\n","# Define fixed VADER thresholds and corresponding integer labels\n","# Bins: (-inf, -0.05], (-0.05, 0.05), [0.05, +inf)\n","# We add +/- infinity to ensure all values are covered.\n","bin_edges = [-float('inf'), -0.05, 0.05, float('inf')]\n","# Assign integer labels: 0 for Negative, 1 for Neutral, 2 for Positive\n","bin_labels = [0, 1, 2]\n","label_names = ['negative', 'neutral', 'positive'] # For potential use in reports if needed\n","\n","print(f\"Using fixed bin edges: {bin_edges}\")\n","print(f\"Assigning integer labels: {bin_labels} ({', '.join(label_names)})\")\n","\n","# Ensure Y is a DataFrame (it should be from previous steps)\n","if not isinstance(Y, pd.DataFrame):\n","     raise TypeError(\"Y should be a pandas DataFrame at this stage.\")\n","\n","# Create a new DataFrame for binned labels to avoid modifying Y inplace initially\n","Y_binned = pd.DataFrame(index=Y.index)\n","\n","for column in Y.columns:\n","    # Cast column values to float64 explicitly to avoid dtype issues\n","    # Use .loc to avoid SettingWithCopyWarning if Y is a slice\n","    Y_column_float = Y.loc[:, column].astype(float)\n","\n","    # Use pd.cut for binning with fixed edges\n","    # include_lowest=True: includes the lowest value (-inf) in the first bin\n","    # right=True (default): bins are (edge1, edge2], except first which is [edge1, edge2] due to include_lowest\n","    # If you want bins like [edge1, edge2), use right=False\n","    binned_data = pd.cut(\n","        Y_column_float,\n","        bins=bin_edges,\n","        labels=bin_labels,\n","        include_lowest=True,\n","        right=True # Standard VADER thresholds often use >= 0.05 for positive, <= -0.05 for negative\n","    )\n","\n","    # Check for NaNs introduced by binning (shouldn't happen with inf edges, but good practice)\n","    if binned_data.isnull().any():\n","        print(f\"Warning: NaNs found in '{column}' after binning. Check original data.\")\n","        # Handle NaNs if necessary, e.g., fill with neutral=1 or drop rows\n","        # binned_data = binned_data.fillna(1) # Example: Fill NaN with Neutral\n","\n","    # Assign the binned data (as integers) to the new DataFrame\n","    Y_binned[column] = binned_data.astype(int)\n","\n","# Replace the original Y with the binned version\n","Y = Y_binned\n","print(\"\\nSentiment scores binned using fixed thresholds:\")\n","print(Y.head())\n","print(\"\\nValue counts for 'food_sentiment' (example):\")\n","print(Y['food_sentiment'].value_counts())\n","\n","# --- End of binning modification ---\n","\n","# 3. Prepare data for training and testing\n","# Assuming 'processed_text' column contains preprocessed text data\n","X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n","\n","# Get the first sentiment column for stratification\n","stratify_column = Y.iloc[:, 0]  # Assuming Y is a DataFrame\n","\n","# Split data into training and testing sets\n","X_train, X_test, Y_train, Y_test = train_test_split(\n","    X, Y, test_size=0.2, random_state=42, stratify=stratify_column\n",")\n","\n","# Print dataset sizes\n","print(f\"Training set size: {len(X_train)}\")\n","print(f\"Testing set size: {len(X_test)}\")\n","\n","\n","# --- TF-IDF Vectorization ---\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n","\n","# Fit the vectorizer to the training data and transform it\n","X_train_tfidf = vectorizer.fit_transform(X_train)\n","\n","# Transform the testing data using the fitted vectorizer\n","X_test_tfidf = vectorizer.transform(X_test)\n","\n","# --- Baseline Model Training and Evaluation ---\n","# Using OneVsRestClassifier: Trains one classifier per class (or per output)\n","# Here, we wrap Logistic Regression. One LR model will be trained for each aspect.\n","print(\"\\nTraining Baseline Model (Logistic Regression with OneVsRest)...\")\n","\n","# Create a base estimator\n","log_reg = LogisticRegression(solver='liblinear', random_state=42) # Liblinear often good for high-dim sparse data\n","\n","# Wrap it with OneVsRestClassifier for multi-label/multi-output scenario\n","# It trains one classifier per column in Y\n","baseline_model = MultiOutputClassifier(log_reg) # Simpler API for multi-output integer targets\n","\n","# Train the model\n","baseline_model.fit(X_train_tfidf, Y_train) # Use the numeric labels DataFrame/Array\n","\n","# Make predictions on the test set\n","Y_pred_baseline = baseline_model.predict(X_test_tfidf)\n","\n","# --- Baseline Evaluation ---\n","# Assuming 'aspects' and 'sentiment_classes' are defined (from your Transformer code)\n","\n","baseline_report = {} # To store results in a dictionary\n","print(\"\\n--- Baseline (Logistic Regression) Classification Report (Per Aspect) ---\")\n","\n","baseline_f1_scores = {} # Store F1-scores per aspect\n","all_true_flat_baseline = []  # To store flattened true labels for overall metrics\n","all_pred_flat_baseline = []  # To store flattened predicted labels for overall metrics\n","\n","# Add these lines before the loop in your code:\n","aspects = ['food', 'service', 'ambiance', 'price', 'context', 'overall_sentiment']\n","sentiment_classes = ['negative', 'neutral', 'positive']\n","\n","for i, aspect_name in enumerate(aspects):\n","    print(f\"\\n--- Aspect: {aspect_name} ---\")\n","    true_labels = Y_test.iloc[:, i].values # Get true labels for this aspect\n","    pred_labels = Y_pred_baseline[:, i] # Get predicted labels for this aspect\n","\n","    # Get unique labels in pred_labels and true_labels\n","    unique_labels = np.unique(np.concatenate((pred_labels, true_labels)))\n","\n","    # Filter target_names to include only the present labels\n","    present_target_names = [name for idx, name in enumerate(sentiment_classes) if idx in unique_labels]\n","\n","\n","    report = classification_report(true_labels, pred_labels, target_names=present_target_names, zero_division=0)\n","    print(report)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted', zero_division=0)\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    baseline_report[aspect_name] = {'precision': precision, 'recall': recall, 'f1-score': f1, 'accuracy': accuracy}\n","    baseline_f1_scores[aspect_name] = f1  # Store F1-score for this aspect\n","\n","    all_true_flat_baseline.extend(true_labels)  # Extend for overall metrics\n","    all_pred_flat_baseline.extend(pred_labels)  # Extend for overall metrics\n","\n","# Overall Micro and Macro Averages for Baseline\n","print(\"\\n--- Baseline Overall Micro/Macro Averages ---\")\n","precision_micro_b, recall_micro_b, f1_micro_b, _ = precision_recall_fscore_support(all_true_flat_baseline, all_pred_flat_baseline, average='micro', zero_division=0)\n","precision_macro_b, recall_macro_b, f1_macro_b, _ = precision_recall_fscore_support(all_true_flat_baseline, all_pred_flat_baseline, average='macro', zero_division=0)\n","print(f\"Micro Average: Precision={precision_micro_b:.4f}, Recall={recall_micro_b:.4f}, F1-Score={f1_micro_b:.4f}\")\n","print(f\"Macro Average: Precision={precision_macro_b:.4f}, Recall={recall_macro_b:.4f}, F1-Score={f1_macro_b:.4f}\")\n","\n","baseline_report['overall_micro'] = {'precision': precision_micro_b, 'recall': recall_micro_b, 'f1-score': f1_micro_b}\n","baseline_report['overall_macro'] = {'precision': precision_macro_b, 'recall': recall_macro_b, 'f1-score': f1_macro_b}"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3e367864dd2f4b10bc3ff3b151b46734","2c149b5adec34d498adde80342fc99f5","375cce2e62ae4f359db5ac21b569d5ff","cd8cf040dcdc45be89554a9eea7c8684","963413e6a30749648b268cb980f93fbc","b098d2b670364d1fae3e9c2260c8aaba","48ed2b8ddb894e40a431a6cf9085d8d4","28e7b7ed9d3949118d0b8ca32fbb8008","284b1c9b827a4431b58ed2f6129e15b7","43867c91be264328be33e5f42342518f","ad551431d55443349bbecba4ae3caf5f","de0447f1bef6401abd9ddab23700ee90","1d2112bfc66c4475b142279a9dcefb42","a932e1b596684aa497ab7893d38e12ab","7d8dc588f8124031b45e48fe894ad151","36f30a28109347d4a20de30a046fab9f","6b9d40832f574c2c98cc74950d7222a9","42b3d3042f2e426ca0e68a84f2f511d9","1b489cb3528940028b736e6e87221b3b","2bf6c3867f8a4562881d0465fec97bbd","cd5b6ace7f3744d1b99b28f18c19df94","5a9a105f32594f9191222f0e783537be","425e4bcc72af422ca761363c62bff12c","197e1f7cdb5a4fd396dd7486e10d0512","85eb0280d0fd4c7780fdcef9b7bd3414","c2e7cc1ea7ac490e9c9fed249bdb64e7","a84e5f25be0a482da94839e419f98740","26b5cb29f4a64a1598393d6773988ee5","e1cb42df5a7745ba88f3f883d96d13b2","1055df58bb96480a8bf28ef6b3bddbf4","52198debe48747d9b87a3e1323dd293d","ad420e9062fe4078a263969b3e5d1ebd","dec6132366164e23b77cbb9fcc85b39d","13ce28b63b61434e81df5263825f001e","07e1ffac08b94a579799db298af4d555","7d393bff604d4c1c8f41903ff7dbd005","ff7646b32afb46829a7bfa34026d07f7","2abed77c7481455db1099516f214bc7c","b74de697ad7740f5b94077d210596b5e","082a0a018d754c1b8e6ed9f3b416034c","6e8951f5fe9f41618cf4414d02aa4e7b","928e11a476fa410eaef36d89bf19cdd1","2b266cf63b2048b4b1dd31e699b60f88","1d94c4a5d5f941c3be377c0ae79aebf4","cc2913302a344120a9a4d67b33dfdf6f","13dd0d574aac40b286fc99e79ba44124","843ec795fe564f79b06dd90256788d77","d9d650e7feae43808e14d2d2214b40b4","2019498d22cc40b6a5eac9000351a819","5c31b165833e4dcfa7fe8d77053f69c7","7288e44b4b794c08a721fff516fef60a","fce7dda606384a51add2608221a2a2fe","b62c1515a6284d07b54b1b6bcf9057b5","8dafe3be6f4f493588b4f06efa87f367","2369d2b98c3446d59d40df73c55adb78"]},"id":"U1W_x4T5ylgS","outputId":"e6564847-a439-4fff-dcd2-9f55bf6e48ee","executionInfo":{"status":"ok","timestamp":1744773799883,"user_tz":300,"elapsed":37301994,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using fixed bin edges: [-inf, -0.05, 0.05, inf]\n","Assigning integer labels: [0, 1, 2] (negative, neutral, positive)\n","\n","Sentiment scores binned using fixed thresholds:\n","   food_sentiment  service_sentiment  ambiance_sentiment  price_sentiment  \\\n","0               2                  1                   1                1   \n","1               2                  2                   1                1   \n","2               2                  1                   1                1   \n","3               1                  1                   1                1   \n","4               2                  1                   1                1   \n","\n","   context_sentiment  overall_sentiment  \n","0                  1                  2  \n","1                  1                  2  \n","2                  1                  2  \n","3                  2                  2  \n","4                  1                  2  \n","\n","Value counts for 'food_sentiment' (example):\n","food_sentiment\n","2    1019023\n","1     385113\n","0     121177\n","Name: count, dtype: int64\n","Training set size: 1220250\n","Testing set size: 305063\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e367864dd2f4b10bc3ff3b151b46734"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de0447f1bef6401abd9ddab23700ee90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"425e4bcc72af422ca761363c62bff12c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ce28b63b61434e81df5263825f001e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2913302a344120a9a4d67b33dfdf6f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Customized DistilBERT model loaded. Output layer size: 18\n","Using device: cuda\n","\n","Starting Transformer Model Training...\n","--- Epoch 1/3 ---\n","  Batch 100/76266, Loss: 0.4605\n","  Batch 200/76266, Loss: 0.5363\n","  Batch 300/76266, Loss: 0.3642\n","  Batch 400/76266, Loss: 0.3450\n","  Batch 500/76266, Loss: 0.4664\n","  Batch 600/76266, Loss: 0.4256\n","  Batch 700/76266, Loss: 0.3760\n","  Batch 800/76266, Loss: 0.3525\n","  Batch 900/76266, Loss: 0.3631\n","  Batch 1000/76266, Loss: 0.3212\n","  Batch 1100/76266, Loss: 0.3153\n","  Batch 1200/76266, Loss: 0.4185\n","  Batch 1300/76266, Loss: 0.3455\n","  Batch 1400/76266, Loss: 0.3565\n","  Batch 1500/76266, Loss: 0.2568\n","  Batch 1600/76266, Loss: 0.3890\n","  Batch 1700/76266, Loss: 0.2809\n","  Batch 1800/76266, Loss: 0.2546\n","  Batch 1900/76266, Loss: 0.2207\n","  Batch 2000/76266, Loss: 0.2464\n","  Batch 2100/76266, Loss: 0.2484\n","  Batch 2200/76266, Loss: 0.3675\n","  Batch 2300/76266, Loss: 0.2434\n","  Batch 2400/76266, Loss: 0.2172\n","  Batch 2500/76266, Loss: 0.3450\n","  Batch 2600/76266, Loss: 0.2391\n","  Batch 2700/76266, Loss: 0.3490\n","  Batch 2800/76266, Loss: 0.2605\n","  Batch 2900/76266, Loss: 0.2697\n","  Batch 3000/76266, Loss: 0.2977\n","  Batch 3100/76266, Loss: 0.2739\n","  Batch 3200/76266, Loss: 0.3967\n","  Batch 3300/76266, Loss: 0.2183\n","  Batch 3400/76266, Loss: 0.1852\n","  Batch 3500/76266, Loss: 0.3476\n","  Batch 3600/76266, Loss: 0.2157\n","  Batch 3700/76266, Loss: 0.3720\n","  Batch 3800/76266, Loss: 0.2292\n","  Batch 3900/76266, Loss: 0.2398\n","  Batch 4000/76266, Loss: 0.2939\n","  Batch 4100/76266, Loss: 0.2868\n","  Batch 4200/76266, Loss: 0.3422\n","  Batch 4300/76266, Loss: 0.2106\n","  Batch 4400/76266, Loss: 0.2370\n","  Batch 4500/76266, Loss: 0.2753\n","  Batch 4600/76266, Loss: 0.2297\n","  Batch 4700/76266, Loss: 0.2656\n","  Batch 4800/76266, Loss: 0.2935\n","  Batch 4900/76266, Loss: 0.2822\n","  Batch 5000/76266, Loss: 0.3437\n","  Batch 5100/76266, Loss: 0.2469\n","  Batch 5200/76266, Loss: 0.2218\n","  Batch 5300/76266, Loss: 0.2201\n","  Batch 5400/76266, Loss: 0.3032\n","  Batch 5500/76266, Loss: 0.2901\n","  Batch 5600/76266, Loss: 0.2854\n","  Batch 5700/76266, Loss: 0.2879\n","  Batch 5800/76266, Loss: 0.2604\n","  Batch 5900/76266, Loss: 0.2035\n","  Batch 6000/76266, Loss: 0.2731\n","  Batch 6100/76266, Loss: 0.3019\n","  Batch 6200/76266, Loss: 0.2162\n","  Batch 6300/76266, Loss: 0.1488\n","  Batch 6400/76266, Loss: 0.2188\n","  Batch 6500/76266, Loss: 0.2475\n","  Batch 6600/76266, Loss: 0.3056\n","  Batch 6700/76266, Loss: 0.1772\n","  Batch 6800/76266, Loss: 0.2584\n","  Batch 6900/76266, Loss: 0.2266\n","  Batch 7000/76266, Loss: 0.2180\n","  Batch 7100/76266, Loss: 0.4651\n","  Batch 7200/76266, Loss: 0.3116\n","  Batch 7300/76266, Loss: 0.3559\n","  Batch 7400/76266, Loss: 0.2649\n","  Batch 7500/76266, Loss: 0.2257\n","  Batch 7600/76266, Loss: 0.2698\n","  Batch 7700/76266, Loss: 0.1621\n","  Batch 7800/76266, Loss: 0.2283\n","  Batch 7900/76266, Loss: 0.1570\n","  Batch 8000/76266, Loss: 0.1728\n","  Batch 8100/76266, Loss: 0.2094\n","  Batch 8200/76266, Loss: 0.2318\n","  Batch 8300/76266, Loss: 0.2221\n","  Batch 8400/76266, Loss: 0.2925\n","  Batch 8500/76266, Loss: 0.1719\n","  Batch 8600/76266, Loss: 0.1988\n","  Batch 8700/76266, Loss: 0.2134\n","  Batch 8800/76266, Loss: 0.2269\n","  Batch 8900/76266, Loss: 0.2403\n","  Batch 9000/76266, Loss: 0.2572\n","  Batch 9100/76266, Loss: 0.2907\n","  Batch 9200/76266, Loss: 0.1678\n","  Batch 9300/76266, Loss: 0.2315\n","  Batch 9400/76266, Loss: 0.1683\n","  Batch 9500/76266, Loss: 0.2563\n","  Batch 9600/76266, Loss: 0.1721\n","  Batch 9700/76266, Loss: 0.1617\n","  Batch 9800/76266, Loss: 0.2170\n","  Batch 9900/76266, Loss: 0.2363\n","  Batch 10000/76266, Loss: 0.3079\n","  Batch 10100/76266, Loss: 0.2818\n","  Batch 10200/76266, Loss: 0.2236\n","  Batch 10300/76266, Loss: 0.2009\n","  Batch 10400/76266, Loss: 0.3059\n","  Batch 10500/76266, Loss: 0.3217\n","  Batch 10600/76266, Loss: 0.2569\n","  Batch 10700/76266, Loss: 0.2708\n","  Batch 10800/76266, Loss: 0.3675\n","  Batch 10900/76266, Loss: 0.2785\n","  Batch 11000/76266, Loss: 0.1570\n","  Batch 11100/76266, Loss: 0.1962\n","  Batch 11200/76266, Loss: 0.2333\n","  Batch 11300/76266, Loss: 0.1956\n","  Batch 11400/76266, Loss: 0.3201\n","  Batch 11500/76266, Loss: 0.2251\n","  Batch 11600/76266, Loss: 0.1804\n","  Batch 11700/76266, Loss: 0.2265\n","  Batch 11800/76266, Loss: 0.2019\n","  Batch 11900/76266, Loss: 0.2126\n","  Batch 12000/76266, Loss: 0.3634\n","  Batch 12100/76266, Loss: 0.3363\n","  Batch 12200/76266, Loss: 0.2121\n","  Batch 12300/76266, Loss: 0.2112\n","  Batch 12400/76266, Loss: 0.2091\n","  Batch 12500/76266, Loss: 0.1804\n","  Batch 12600/76266, Loss: 0.2767\n","  Batch 12700/76266, Loss: 0.2527\n","  Batch 12800/76266, Loss: 0.3343\n","  Batch 12900/76266, Loss: 0.2867\n","  Batch 13000/76266, Loss: 0.1403\n","  Batch 13100/76266, Loss: 0.1730\n","  Batch 13200/76266, Loss: 0.1962\n","  Batch 13300/76266, Loss: 0.1862\n","  Batch 13400/76266, Loss: 0.2193\n","  Batch 13500/76266, Loss: 0.2382\n","  Batch 13600/76266, Loss: 0.2506\n","  Batch 13700/76266, Loss: 0.2608\n","  Batch 13800/76266, Loss: 0.3063\n","  Batch 13900/76266, Loss: 0.2544\n","  Batch 14000/76266, Loss: 0.2457\n","  Batch 14100/76266, Loss: 0.1820\n","  Batch 14200/76266, Loss: 0.2121\n","  Batch 14300/76266, Loss: 0.2892\n","  Batch 14400/76266, Loss: 0.2762\n","  Batch 14500/76266, Loss: 0.2478\n","  Batch 14600/76266, Loss: 0.3546\n","  Batch 14700/76266, Loss: 0.2678\n","  Batch 14800/76266, Loss: 0.1930\n","  Batch 14900/76266, Loss: 0.2312\n","  Batch 15000/76266, Loss: 0.2492\n","  Batch 15100/76266, Loss: 0.1468\n","  Batch 15200/76266, Loss: 0.2562\n","  Batch 15300/76266, Loss: 0.2423\n","  Batch 15400/76266, Loss: 0.2149\n","  Batch 15500/76266, Loss: 0.2538\n","  Batch 15600/76266, Loss: 0.2450\n","  Batch 15700/76266, Loss: 0.3074\n","  Batch 15800/76266, Loss: 0.2448\n","  Batch 15900/76266, Loss: 0.2932\n","  Batch 16000/76266, Loss: 0.3259\n","  Batch 16100/76266, Loss: 0.1280\n","  Batch 16200/76266, Loss: 0.2700\n","  Batch 16300/76266, Loss: 0.2143\n","  Batch 16400/76266, Loss: 0.2631\n","  Batch 16500/76266, Loss: 0.1910\n","  Batch 16600/76266, Loss: 0.2024\n","  Batch 16700/76266, Loss: 0.2409\n","  Batch 16800/76266, Loss: 0.2226\n","  Batch 16900/76266, Loss: 0.3664\n","  Batch 17000/76266, Loss: 0.3904\n","  Batch 17100/76266, Loss: 0.2879\n","  Batch 17200/76266, Loss: 0.1900\n","  Batch 17300/76266, Loss: 0.3143\n","  Batch 17400/76266, Loss: 0.3596\n","  Batch 17500/76266, Loss: 0.2060\n","  Batch 17600/76266, Loss: 0.3185\n","  Batch 17700/76266, Loss: 0.3433\n","  Batch 17800/76266, Loss: 0.1642\n","  Batch 17900/76266, Loss: 0.2780\n","  Batch 18000/76266, Loss: 0.2497\n","  Batch 18100/76266, Loss: 0.3029\n","  Batch 18200/76266, Loss: 0.2133\n","  Batch 18300/76266, Loss: 0.2538\n","  Batch 18400/76266, Loss: 0.1484\n","  Batch 18500/76266, Loss: 0.2488\n","  Batch 18600/76266, Loss: 0.2301\n","  Batch 18700/76266, Loss: 0.2111\n","  Batch 18800/76266, Loss: 0.2983\n","  Batch 18900/76266, Loss: 0.2572\n","  Batch 19000/76266, Loss: 0.1671\n","  Batch 19100/76266, Loss: 0.2175\n","  Batch 19200/76266, Loss: 0.2349\n","  Batch 19300/76266, Loss: 0.3254\n","  Batch 19400/76266, Loss: 0.3029\n","  Batch 19500/76266, Loss: 0.2364\n","  Batch 19600/76266, Loss: 0.2604\n","  Batch 19700/76266, Loss: 0.1362\n","  Batch 19800/76266, Loss: 0.3143\n","  Batch 19900/76266, Loss: 0.3204\n","  Batch 20000/76266, Loss: 0.2638\n","  Batch 20100/76266, Loss: 0.2057\n","  Batch 20200/76266, Loss: 0.2447\n","  Batch 20300/76266, Loss: 0.1646\n","  Batch 20400/76266, Loss: 0.2188\n","  Batch 20500/76266, Loss: 0.1546\n","  Batch 20600/76266, Loss: 0.3031\n","  Batch 20700/76266, Loss: 0.2330\n","  Batch 20800/76266, Loss: 0.2741\n","  Batch 20900/76266, Loss: 0.2615\n","  Batch 21000/76266, Loss: 0.2846\n","  Batch 21100/76266, Loss: 0.1039\n","  Batch 21200/76266, Loss: 0.1959\n","  Batch 21300/76266, Loss: 0.2193\n","  Batch 21400/76266, Loss: 0.2199\n","  Batch 21500/76266, Loss: 0.2410\n","  Batch 21600/76266, Loss: 0.1617\n","  Batch 21700/76266, Loss: 0.2123\n","  Batch 21800/76266, Loss: 0.2898\n","  Batch 21900/76266, Loss: 0.2996\n","  Batch 22000/76266, Loss: 0.1674\n","  Batch 22100/76266, Loss: 0.1422\n","  Batch 22200/76266, Loss: 0.1731\n","  Batch 22300/76266, Loss: 0.2551\n","  Batch 22400/76266, Loss: 0.3240\n","  Batch 22500/76266, Loss: 0.2284\n","  Batch 22600/76266, Loss: 0.3538\n","  Batch 22700/76266, Loss: 0.1725\n","  Batch 22800/76266, Loss: 0.1338\n","  Batch 22900/76266, Loss: 0.1635\n","  Batch 23000/76266, Loss: 0.3660\n","  Batch 23100/76266, Loss: 0.3493\n","  Batch 23200/76266, Loss: 0.2593\n","  Batch 23300/76266, Loss: 0.2686\n","  Batch 23400/76266, Loss: 0.3398\n","  Batch 23500/76266, Loss: 0.1725\n","  Batch 23600/76266, Loss: 0.2390\n","  Batch 23700/76266, Loss: 0.2645\n","  Batch 23800/76266, Loss: 0.1634\n","  Batch 23900/76266, Loss: 0.2515\n","  Batch 24000/76266, Loss: 0.1863\n","  Batch 24100/76266, Loss: 0.2096\n","  Batch 24200/76266, Loss: 0.2190\n","  Batch 24300/76266, Loss: 0.2342\n","  Batch 24400/76266, Loss: 0.1903\n","  Batch 24500/76266, Loss: 0.2107\n","  Batch 24600/76266, Loss: 0.2453\n","  Batch 24700/76266, Loss: 0.3048\n","  Batch 24800/76266, Loss: 0.1693\n","  Batch 24900/76266, Loss: 0.1019\n","  Batch 25000/76266, Loss: 0.2731\n","  Batch 25100/76266, Loss: 0.2476\n","  Batch 25200/76266, Loss: 0.2740\n","  Batch 25300/76266, Loss: 0.2234\n","  Batch 25400/76266, Loss: 0.2749\n","  Batch 25500/76266, Loss: 0.3239\n","  Batch 25600/76266, Loss: 0.1759\n","  Batch 25700/76266, Loss: 0.1550\n","  Batch 25800/76266, Loss: 0.3333\n","  Batch 25900/76266, Loss: 0.2930\n","  Batch 26000/76266, Loss: 0.2031\n","  Batch 26100/76266, Loss: 0.2615\n","  Batch 26200/76266, Loss: 0.1789\n","  Batch 26300/76266, Loss: 0.1641\n","  Batch 26400/76266, Loss: 0.2188\n","  Batch 26500/76266, Loss: 0.2428\n","  Batch 26600/76266, Loss: 0.2549\n","  Batch 26700/76266, Loss: 0.2244\n","  Batch 26800/76266, Loss: 0.2534\n","  Batch 26900/76266, Loss: 0.1456\n","  Batch 27000/76266, Loss: 0.1602\n","  Batch 27100/76266, Loss: 0.2574\n","  Batch 27200/76266, Loss: 0.2382\n","  Batch 27300/76266, Loss: 0.2180\n","  Batch 27400/76266, Loss: 0.2230\n","  Batch 27500/76266, Loss: 0.2046\n","  Batch 27600/76266, Loss: 0.3032\n","  Batch 27700/76266, Loss: 0.1903\n","  Batch 27800/76266, Loss: 0.2648\n","  Batch 27900/76266, Loss: 0.1562\n","  Batch 28000/76266, Loss: 0.2593\n","  Batch 28100/76266, Loss: 0.2646\n","  Batch 28200/76266, Loss: 0.1697\n","  Batch 28300/76266, Loss: 0.2197\n","  Batch 28400/76266, Loss: 0.2383\n","  Batch 28500/76266, Loss: 0.2706\n","  Batch 28600/76266, Loss: 0.2225\n","  Batch 28700/76266, Loss: 0.2846\n","  Batch 28800/76266, Loss: 0.2620\n","  Batch 28900/76266, Loss: 0.2307\n","  Batch 29000/76266, Loss: 0.1964\n","  Batch 29100/76266, Loss: 0.1291\n","  Batch 29200/76266, Loss: 0.2664\n","  Batch 29300/76266, Loss: 0.2709\n","  Batch 29400/76266, Loss: 0.2695\n","  Batch 29500/76266, Loss: 0.1597\n","  Batch 29600/76266, Loss: 0.3364\n","  Batch 29700/76266, Loss: 0.2313\n","  Batch 29800/76266, Loss: 0.1808\n","  Batch 29900/76266, Loss: 0.3994\n","  Batch 30000/76266, Loss: 0.1997\n","  Batch 30100/76266, Loss: 0.1902\n","  Batch 30200/76266, Loss: 0.1853\n","  Batch 30300/76266, Loss: 0.1439\n","  Batch 30400/76266, Loss: 0.2629\n","  Batch 30500/76266, Loss: 0.1843\n","  Batch 30600/76266, Loss: 0.1899\n","  Batch 30700/76266, Loss: 0.1812\n","  Batch 30800/76266, Loss: 0.2181\n","  Batch 30900/76266, Loss: 0.1917\n","  Batch 31000/76266, Loss: 0.1754\n","  Batch 31100/76266, Loss: 0.2329\n","  Batch 31200/76266, Loss: 0.1662\n","  Batch 31300/76266, Loss: 0.2347\n","  Batch 31400/76266, Loss: 0.2663\n","  Batch 31500/76266, Loss: 0.1844\n","  Batch 31600/76266, Loss: 0.1455\n","  Batch 31700/76266, Loss: 0.1703\n","  Batch 31800/76266, Loss: 0.1697\n","  Batch 31900/76266, Loss: 0.1565\n","  Batch 32000/76266, Loss: 0.2556\n","  Batch 32100/76266, Loss: 0.3518\n","  Batch 32200/76266, Loss: 0.3054\n","  Batch 32300/76266, Loss: 0.1779\n","  Batch 32400/76266, Loss: 0.2076\n","  Batch 32500/76266, Loss: 0.2088\n","  Batch 32600/76266, Loss: 0.2078\n","  Batch 32700/76266, Loss: 0.2368\n","  Batch 32800/76266, Loss: 0.1786\n","  Batch 32900/76266, Loss: 0.2044\n","  Batch 33000/76266, Loss: 0.1760\n","  Batch 33100/76266, Loss: 0.2892\n","  Batch 33200/76266, Loss: 0.2601\n","  Batch 33300/76266, Loss: 0.1980\n","  Batch 33400/76266, Loss: 0.1340\n","  Batch 33500/76266, Loss: 0.3303\n","  Batch 33600/76266, Loss: 0.2630\n","  Batch 33700/76266, Loss: 0.2218\n","  Batch 33800/76266, Loss: 0.2351\n","  Batch 33900/76266, Loss: 0.2205\n","  Batch 34000/76266, Loss: 0.2216\n","  Batch 34100/76266, Loss: 0.2219\n","  Batch 34200/76266, Loss: 0.1180\n","  Batch 34300/76266, Loss: 0.2250\n","  Batch 34400/76266, Loss: 0.1894\n","  Batch 34500/76266, Loss: 0.2536\n","  Batch 34600/76266, Loss: 0.3357\n","  Batch 34700/76266, Loss: 0.2658\n","  Batch 34800/76266, Loss: 0.1544\n","  Batch 34900/76266, Loss: 0.2647\n","  Batch 35000/76266, Loss: 0.1970\n","  Batch 35100/76266, Loss: 0.1673\n","  Batch 35200/76266, Loss: 0.2063\n","  Batch 35300/76266, Loss: 0.2289\n","  Batch 35400/76266, Loss: 0.2131\n","  Batch 35500/76266, Loss: 0.2119\n","  Batch 35600/76266, Loss: 0.3056\n","  Batch 35700/76266, Loss: 0.1717\n","  Batch 35800/76266, Loss: 0.2051\n","  Batch 35900/76266, Loss: 0.2968\n","  Batch 36000/76266, Loss: 0.2155\n","  Batch 36100/76266, Loss: 0.1978\n","  Batch 36200/76266, Loss: 0.2441\n","  Batch 36300/76266, Loss: 0.1694\n","  Batch 36400/76266, Loss: 0.2874\n","  Batch 36500/76266, Loss: 0.3214\n","  Batch 36600/76266, Loss: 0.1707\n","  Batch 36700/76266, Loss: 0.2490\n","  Batch 36800/76266, Loss: 0.2530\n","  Batch 36900/76266, Loss: 0.2173\n","  Batch 37000/76266, Loss: 0.1928\n","  Batch 37100/76266, Loss: 0.1913\n","  Batch 37200/76266, Loss: 0.1842\n","  Batch 37300/76266, Loss: 0.2607\n","  Batch 37400/76266, Loss: 0.2394\n","  Batch 37500/76266, Loss: 0.2119\n","  Batch 37600/76266, Loss: 0.1837\n","  Batch 37700/76266, Loss: 0.1526\n","  Batch 37800/76266, Loss: 0.2440\n","  Batch 37900/76266, Loss: 0.2323\n","  Batch 38000/76266, Loss: 0.3739\n","  Batch 38100/76266, Loss: 0.1430\n","  Batch 38200/76266, Loss: 0.1720\n","  Batch 38300/76266, Loss: 0.2033\n","  Batch 38400/76266, Loss: 0.1741\n","  Batch 38500/76266, Loss: 0.1850\n","  Batch 38600/76266, Loss: 0.3159\n","  Batch 38700/76266, Loss: 0.2704\n","  Batch 38800/76266, Loss: 0.2097\n","  Batch 38900/76266, Loss: 0.1351\n","  Batch 39000/76266, Loss: 0.2041\n","  Batch 39100/76266, Loss: 0.1652\n","  Batch 39200/76266, Loss: 0.3000\n","  Batch 39300/76266, Loss: 0.2107\n","  Batch 39400/76266, Loss: 0.2053\n","  Batch 39500/76266, Loss: 0.1944\n","  Batch 39600/76266, Loss: 0.1800\n","  Batch 39700/76266, Loss: 0.2093\n","  Batch 39800/76266, Loss: 0.2186\n","  Batch 39900/76266, Loss: 0.3000\n","  Batch 40000/76266, Loss: 0.2496\n","  Batch 40100/76266, Loss: 0.2581\n","  Batch 40200/76266, Loss: 0.2052\n","  Batch 40300/76266, Loss: 0.2578\n","  Batch 40400/76266, Loss: 0.1791\n","  Batch 40500/76266, Loss: 0.2315\n","  Batch 40600/76266, Loss: 0.2474\n","  Batch 40700/76266, Loss: 0.2373\n","  Batch 40800/76266, Loss: 0.3760\n","  Batch 40900/76266, Loss: 0.2230\n","  Batch 41000/76266, Loss: 0.2254\n","  Batch 41100/76266, Loss: 0.1642\n","  Batch 41200/76266, Loss: 0.1571\n","  Batch 41300/76266, Loss: 0.2433\n","  Batch 41400/76266, Loss: 0.1987\n","  Batch 41500/76266, Loss: 0.3521\n","  Batch 41600/76266, Loss: 0.1458\n","  Batch 41700/76266, Loss: 0.1839\n","  Batch 41800/76266, Loss: 0.2057\n","  Batch 41900/76266, Loss: 0.1532\n","  Batch 42000/76266, Loss: 0.2379\n","  Batch 42100/76266, Loss: 0.2749\n","  Batch 42200/76266, Loss: 0.3214\n","  Batch 42300/76266, Loss: 0.2278\n","  Batch 42400/76266, Loss: 0.1708\n","  Batch 42500/76266, Loss: 0.3077\n","  Batch 42600/76266, Loss: 0.1744\n","  Batch 42700/76266, Loss: 0.1982\n","  Batch 42800/76266, Loss: 0.2547\n","  Batch 42900/76266, Loss: 0.1171\n","  Batch 43000/76266, Loss: 0.2412\n","  Batch 43100/76266, Loss: 0.2161\n","  Batch 43200/76266, Loss: 0.2359\n","  Batch 43300/76266, Loss: 0.2553\n","  Batch 43400/76266, Loss: 0.3019\n","  Batch 43500/76266, Loss: 0.3436\n","  Batch 43600/76266, Loss: 0.2771\n","  Batch 43700/76266, Loss: 0.3974\n","  Batch 43800/76266, Loss: 0.2414\n","  Batch 43900/76266, Loss: 0.2838\n","  Batch 44000/76266, Loss: 0.2686\n","  Batch 44100/76266, Loss: 0.3237\n","  Batch 44200/76266, Loss: 0.1895\n","  Batch 44300/76266, Loss: 0.1920\n","  Batch 44400/76266, Loss: 0.2363\n","  Batch 44500/76266, Loss: 0.2555\n","  Batch 44600/76266, Loss: 0.2397\n","  Batch 44700/76266, Loss: 0.3245\n","  Batch 44800/76266, Loss: 0.2605\n","  Batch 44900/76266, Loss: 0.3423\n","  Batch 45000/76266, Loss: 0.1884\n","  Batch 45100/76266, Loss: 0.2218\n","  Batch 45200/76266, Loss: 0.1843\n","  Batch 45300/76266, Loss: 0.4388\n","  Batch 45400/76266, Loss: 0.2182\n","  Batch 45500/76266, Loss: 0.1500\n","  Batch 45600/76266, Loss: 0.1256\n","  Batch 45700/76266, Loss: 0.2064\n","  Batch 45800/76266, Loss: 0.2118\n","  Batch 45900/76266, Loss: 0.1950\n","  Batch 46000/76266, Loss: 0.1432\n","  Batch 46100/76266, Loss: 0.1841\n","  Batch 46200/76266, Loss: 0.2502\n","  Batch 46300/76266, Loss: 0.2584\n","  Batch 46400/76266, Loss: 0.2765\n","  Batch 46500/76266, Loss: 0.2676\n","  Batch 46600/76266, Loss: 0.2361\n","  Batch 46700/76266, Loss: 0.1274\n","  Batch 46800/76266, Loss: 0.2227\n","  Batch 46900/76266, Loss: 0.1791\n","  Batch 47000/76266, Loss: 0.2441\n","  Batch 47100/76266, Loss: 0.3046\n","  Batch 47200/76266, Loss: 0.2591\n","  Batch 47300/76266, Loss: 0.2721\n","  Batch 47400/76266, Loss: 0.2244\n","  Batch 47500/76266, Loss: 0.1832\n","  Batch 47600/76266, Loss: 0.1480\n","  Batch 47700/76266, Loss: 0.1917\n","  Batch 47800/76266, Loss: 0.1436\n","  Batch 47900/76266, Loss: 0.1629\n","  Batch 48000/76266, Loss: 0.1779\n","  Batch 48100/76266, Loss: 0.1897\n","  Batch 48200/76266, Loss: 0.2579\n","  Batch 48300/76266, Loss: 0.2090\n","  Batch 48400/76266, Loss: 0.1793\n","  Batch 48500/76266, Loss: 0.2478\n","  Batch 48600/76266, Loss: 0.2246\n","  Batch 48700/76266, Loss: 0.2450\n","  Batch 48800/76266, Loss: 0.2844\n","  Batch 48900/76266, Loss: 0.2988\n","  Batch 49000/76266, Loss: 0.2079\n","  Batch 49100/76266, Loss: 0.1269\n","  Batch 49200/76266, Loss: 0.3211\n","  Batch 49300/76266, Loss: 0.1556\n","  Batch 49400/76266, Loss: 0.2183\n","  Batch 49500/76266, Loss: 0.2133\n","  Batch 49600/76266, Loss: 0.2655\n","  Batch 49700/76266, Loss: 0.1822\n","  Batch 49800/76266, Loss: 0.1756\n","  Batch 49900/76266, Loss: 0.1956\n","  Batch 50000/76266, Loss: 0.1960\n","  Batch 50100/76266, Loss: 0.1705\n","  Batch 50200/76266, Loss: 0.1545\n","  Batch 50300/76266, Loss: 0.2453\n","  Batch 50400/76266, Loss: 0.2726\n","  Batch 50500/76266, Loss: 0.2563\n","  Batch 50600/76266, Loss: 0.3628\n","  Batch 50700/76266, Loss: 0.1704\n","  Batch 50800/76266, Loss: 0.2076\n","  Batch 50900/76266, Loss: 0.2933\n","  Batch 51000/76266, Loss: 0.2966\n","  Batch 51100/76266, Loss: 0.2126\n","  Batch 51200/76266, Loss: 0.1901\n","  Batch 51300/76266, Loss: 0.2749\n","  Batch 51400/76266, Loss: 0.1696\n","  Batch 51500/76266, Loss: 0.2667\n","  Batch 51600/76266, Loss: 0.2024\n","  Batch 51700/76266, Loss: 0.1844\n","  Batch 51800/76266, Loss: 0.2493\n","  Batch 51900/76266, Loss: 0.2170\n","  Batch 52000/76266, Loss: 0.1698\n","  Batch 52100/76266, Loss: 0.1564\n","  Batch 52200/76266, Loss: 0.2628\n","  Batch 52300/76266, Loss: 0.1513\n","  Batch 52400/76266, Loss: 0.1694\n","  Batch 52500/76266, Loss: 0.1645\n","  Batch 52600/76266, Loss: 0.2374\n","  Batch 52700/76266, Loss: 0.2077\n","  Batch 52800/76266, Loss: 0.1593\n","  Batch 52900/76266, Loss: 0.2849\n","  Batch 53000/76266, Loss: 0.2284\n","  Batch 53100/76266, Loss: 0.2887\n","  Batch 53200/76266, Loss: 0.1646\n","  Batch 53300/76266, Loss: 0.2694\n","  Batch 53400/76266, Loss: 0.1619\n","  Batch 53500/76266, Loss: 0.2009\n","  Batch 53600/76266, Loss: 0.3400\n","  Batch 53700/76266, Loss: 0.1866\n","  Batch 53800/76266, Loss: 0.1925\n","  Batch 53900/76266, Loss: 0.2021\n","  Batch 54000/76266, Loss: 0.2504\n","  Batch 54100/76266, Loss: 0.2272\n","  Batch 54200/76266, Loss: 0.2211\n","  Batch 54300/76266, Loss: 0.2401\n","  Batch 54400/76266, Loss: 0.1966\n","  Batch 54500/76266, Loss: 0.2236\n","  Batch 54600/76266, Loss: 0.2507\n","  Batch 54700/76266, Loss: 0.1529\n","  Batch 54800/76266, Loss: 0.2310\n","  Batch 54900/76266, Loss: 0.1411\n","  Batch 55000/76266, Loss: 0.1300\n","  Batch 55100/76266, Loss: 0.1752\n","  Batch 55200/76266, Loss: 0.2332\n","  Batch 55300/76266, Loss: 0.2599\n","  Batch 55400/76266, Loss: 0.2622\n","  Batch 55500/76266, Loss: 0.1708\n","  Batch 55600/76266, Loss: 0.2637\n","  Batch 55700/76266, Loss: 0.1459\n","  Batch 55800/76266, Loss: 0.1663\n","  Batch 55900/76266, Loss: 0.2879\n","  Batch 56000/76266, Loss: 0.1696\n","  Batch 56100/76266, Loss: 0.1583\n","  Batch 56200/76266, Loss: 0.2916\n","  Batch 56300/76266, Loss: 0.3343\n","  Batch 56400/76266, Loss: 0.2248\n","  Batch 56500/76266, Loss: 0.2924\n","  Batch 56600/76266, Loss: 0.2609\n","  Batch 56700/76266, Loss: 0.2299\n","  Batch 56800/76266, Loss: 0.1292\n","  Batch 56900/76266, Loss: 0.3522\n","  Batch 57000/76266, Loss: 0.1667\n","  Batch 57100/76266, Loss: 0.1437\n","  Batch 57200/76266, Loss: 0.1917\n","  Batch 57300/76266, Loss: 0.3051\n","  Batch 57400/76266, Loss: 0.2060\n","  Batch 57500/76266, Loss: 0.2232\n","  Batch 57600/76266, Loss: 0.1349\n","  Batch 57700/76266, Loss: 0.1616\n","  Batch 57800/76266, Loss: 0.2380\n","  Batch 57900/76266, Loss: 0.1418\n","  Batch 58000/76266, Loss: 0.2970\n","  Batch 58100/76266, Loss: 0.1640\n","  Batch 58200/76266, Loss: 0.2101\n","  Batch 58300/76266, Loss: 0.1510\n","  Batch 58400/76266, Loss: 0.1852\n","  Batch 58500/76266, Loss: 0.2241\n","  Batch 58600/76266, Loss: 0.2003\n","  Batch 58700/76266, Loss: 0.0806\n","  Batch 58800/76266, Loss: 0.1848\n","  Batch 58900/76266, Loss: 0.1699\n","  Batch 59000/76266, Loss: 0.1754\n","  Batch 59100/76266, Loss: 0.1238\n","  Batch 59200/76266, Loss: 0.1460\n","  Batch 59300/76266, Loss: 0.2125\n","  Batch 59400/76266, Loss: 0.3222\n","  Batch 59500/76266, Loss: 0.2316\n","  Batch 59600/76266, Loss: 0.1590\n","  Batch 59700/76266, Loss: 0.2831\n","  Batch 59800/76266, Loss: 0.1840\n","  Batch 59900/76266, Loss: 0.2020\n","  Batch 60000/76266, Loss: 0.1827\n","  Batch 60100/76266, Loss: 0.2069\n","  Batch 60200/76266, Loss: 0.1930\n","  Batch 60300/76266, Loss: 0.1921\n","  Batch 60400/76266, Loss: 0.2156\n","  Batch 60500/76266, Loss: 0.1187\n","  Batch 60600/76266, Loss: 0.1712\n","  Batch 60700/76266, Loss: 0.3552\n","  Batch 60800/76266, Loss: 0.2710\n","  Batch 60900/76266, Loss: 0.2756\n","  Batch 61000/76266, Loss: 0.1536\n","  Batch 61100/76266, Loss: 0.1521\n","  Batch 61200/76266, Loss: 0.2935\n","  Batch 61300/76266, Loss: 0.1818\n","  Batch 61400/76266, Loss: 0.2181\n","  Batch 61500/76266, Loss: 0.2282\n","  Batch 61600/76266, Loss: 0.1758\n","  Batch 61700/76266, Loss: 0.1810\n","  Batch 61800/76266, Loss: 0.1330\n","  Batch 61900/76266, Loss: 0.2060\n","  Batch 62000/76266, Loss: 0.1277\n","  Batch 62100/76266, Loss: 0.1955\n","  Batch 62200/76266, Loss: 0.1409\n","  Batch 62300/76266, Loss: 0.2589\n","  Batch 62400/76266, Loss: 0.2233\n","  Batch 62500/76266, Loss: 0.1626\n","  Batch 62600/76266, Loss: 0.1778\n","  Batch 62700/76266, Loss: 0.2759\n","  Batch 62800/76266, Loss: 0.2163\n","  Batch 62900/76266, Loss: 0.2587\n","  Batch 63000/76266, Loss: 0.1605\n","  Batch 63100/76266, Loss: 0.3236\n","  Batch 63200/76266, Loss: 0.1869\n","  Batch 63300/76266, Loss: 0.1877\n","  Batch 63400/76266, Loss: 0.1517\n","  Batch 63500/76266, Loss: 0.1382\n","  Batch 63600/76266, Loss: 0.1479\n","  Batch 63700/76266, Loss: 0.2410\n","  Batch 63800/76266, Loss: 0.3592\n","  Batch 63900/76266, Loss: 0.2405\n","  Batch 64000/76266, Loss: 0.2050\n","  Batch 64100/76266, Loss: 0.2510\n","  Batch 64200/76266, Loss: 0.2070\n","  Batch 64300/76266, Loss: 0.2429\n","  Batch 64400/76266, Loss: 0.2329\n","  Batch 64500/76266, Loss: 0.1103\n","  Batch 64600/76266, Loss: 0.3622\n","  Batch 64700/76266, Loss: 0.2543\n","  Batch 64800/76266, Loss: 0.2452\n","  Batch 64900/76266, Loss: 0.1409\n","  Batch 65000/76266, Loss: 0.1544\n","  Batch 65100/76266, Loss: 0.1918\n","  Batch 65200/76266, Loss: 0.2413\n","  Batch 65300/76266, Loss: 0.2059\n","  Batch 65400/76266, Loss: 0.2515\n","  Batch 65500/76266, Loss: 0.3064\n","  Batch 65600/76266, Loss: 0.2646\n","  Batch 65700/76266, Loss: 0.1764\n","  Batch 65800/76266, Loss: 0.2055\n","  Batch 65900/76266, Loss: 0.1454\n","  Batch 66000/76266, Loss: 0.1959\n","  Batch 66100/76266, Loss: 0.2153\n","  Batch 66200/76266, Loss: 0.1484\n","  Batch 66300/76266, Loss: 0.2316\n","  Batch 66400/76266, Loss: 0.2763\n","  Batch 66500/76266, Loss: 0.3087\n","  Batch 66600/76266, Loss: 0.3112\n","  Batch 66700/76266, Loss: 0.2038\n","  Batch 66800/76266, Loss: 0.1166\n","  Batch 66900/76266, Loss: 0.0851\n","  Batch 67000/76266, Loss: 0.2649\n","  Batch 67100/76266, Loss: 0.1900\n","  Batch 67200/76266, Loss: 0.1883\n","  Batch 67300/76266, Loss: 0.1823\n","  Batch 67400/76266, Loss: 0.1665\n","  Batch 67500/76266, Loss: 0.3330\n","  Batch 67600/76266, Loss: 0.1630\n","  Batch 67700/76266, Loss: 0.1917\n","  Batch 67800/76266, Loss: 0.2246\n","  Batch 67900/76266, Loss: 0.1984\n","  Batch 68000/76266, Loss: 0.1862\n","  Batch 68100/76266, Loss: 0.2166\n","  Batch 68200/76266, Loss: 0.2538\n","  Batch 68300/76266, Loss: 0.1941\n","  Batch 68400/76266, Loss: 0.2561\n","  Batch 68500/76266, Loss: 0.3024\n","  Batch 68600/76266, Loss: 0.1831\n","  Batch 68700/76266, Loss: 0.3283\n","  Batch 68800/76266, Loss: 0.3222\n","  Batch 68900/76266, Loss: 0.2339\n","  Batch 69000/76266, Loss: 0.1715\n","  Batch 69100/76266, Loss: 0.2545\n","  Batch 69200/76266, Loss: 0.2288\n","  Batch 69300/76266, Loss: 0.2864\n","  Batch 69400/76266, Loss: 0.1956\n","  Batch 69500/76266, Loss: 0.1601\n","  Batch 69600/76266, Loss: 0.2863\n","  Batch 69700/76266, Loss: 0.2075\n","  Batch 69800/76266, Loss: 0.1826\n","  Batch 69900/76266, Loss: 0.2274\n","  Batch 70000/76266, Loss: 0.2315\n","  Batch 70100/76266, Loss: 0.1926\n","  Batch 70200/76266, Loss: 0.1881\n","  Batch 70300/76266, Loss: 0.1406\n","  Batch 70400/76266, Loss: 0.2825\n","  Batch 70500/76266, Loss: 0.3090\n","  Batch 70600/76266, Loss: 0.1973\n","  Batch 70700/76266, Loss: 0.2346\n","  Batch 70800/76266, Loss: 0.1468\n","  Batch 70900/76266, Loss: 0.2899\n","  Batch 71000/76266, Loss: 0.1943\n","  Batch 71100/76266, Loss: 0.3047\n","  Batch 71200/76266, Loss: 0.2230\n","  Batch 71300/76266, Loss: 0.3505\n","  Batch 71400/76266, Loss: 0.1546\n","  Batch 71500/76266, Loss: 0.1388\n","  Batch 71600/76266, Loss: 0.3901\n","  Batch 71700/76266, Loss: 0.3550\n","  Batch 71800/76266, Loss: 0.2159\n","  Batch 71900/76266, Loss: 0.2480\n","  Batch 72000/76266, Loss: 0.1412\n","  Batch 72100/76266, Loss: 0.2043\n","  Batch 72200/76266, Loss: 0.2433\n","  Batch 72300/76266, Loss: 0.2576\n","  Batch 72400/76266, Loss: 0.2723\n","  Batch 72500/76266, Loss: 0.2625\n","  Batch 72600/76266, Loss: 0.1977\n","  Batch 72700/76266, Loss: 0.2607\n","  Batch 72800/76266, Loss: 0.1929\n","  Batch 72900/76266, Loss: 0.1850\n","  Batch 73000/76266, Loss: 0.2521\n","  Batch 73100/76266, Loss: 0.2744\n","  Batch 73200/76266, Loss: 0.2139\n","  Batch 73300/76266, Loss: 0.1946\n","  Batch 73400/76266, Loss: 0.2176\n","  Batch 73500/76266, Loss: 0.1423\n","  Batch 73600/76266, Loss: 0.2555\n","  Batch 73700/76266, Loss: 0.2724\n","  Batch 73800/76266, Loss: 0.2304\n","  Batch 73900/76266, Loss: 0.1860\n","  Batch 74000/76266, Loss: 0.2595\n","  Batch 74100/76266, Loss: 0.4019\n","  Batch 74200/76266, Loss: 0.2285\n","  Batch 74300/76266, Loss: 0.2496\n","  Batch 74400/76266, Loss: 0.2865\n","  Batch 74500/76266, Loss: 0.2436\n","  Batch 74600/76266, Loss: 0.1338\n","  Batch 74700/76266, Loss: 0.2227\n","  Batch 74800/76266, Loss: 0.2339\n","  Batch 74900/76266, Loss: 0.1909\n","  Batch 75000/76266, Loss: 0.2297\n","  Batch 75100/76266, Loss: 0.1944\n","  Batch 75200/76266, Loss: 0.4000\n","  Batch 75300/76266, Loss: 0.2231\n","  Batch 75400/76266, Loss: 0.2479\n","  Batch 75500/76266, Loss: 0.2480\n","  Batch 75600/76266, Loss: 0.2889\n","  Batch 75700/76266, Loss: 0.0870\n","  Batch 75800/76266, Loss: 0.2655\n","  Batch 75900/76266, Loss: 0.2343\n","  Batch 76000/76266, Loss: 0.2004\n","  Batch 76100/76266, Loss: 0.2114\n","  Batch 76200/76266, Loss: 0.2609\n","Epoch 1 completed in 11584.55s. Average Training Loss: 0.2328\n","--- Epoch 2/3 ---\n","  Batch 100/76266, Loss: 0.2105\n","  Batch 200/76266, Loss: 0.1638\n","  Batch 300/76266, Loss: 0.1383\n","  Batch 400/76266, Loss: 0.2674\n","  Batch 500/76266, Loss: 0.2163\n","  Batch 600/76266, Loss: 0.1804\n","  Batch 700/76266, Loss: 0.1603\n","  Batch 800/76266, Loss: 0.2189\n","  Batch 900/76266, Loss: 0.1098\n","  Batch 1000/76266, Loss: 0.1440\n","  Batch 1100/76266, Loss: 0.1603\n","  Batch 1200/76266, Loss: 0.1470\n","  Batch 1300/76266, Loss: 0.1641\n","  Batch 1400/76266, Loss: 0.1391\n","  Batch 1500/76266, Loss: 0.2448\n","  Batch 1600/76266, Loss: 0.1885\n","  Batch 1700/76266, Loss: 0.1828\n","  Batch 1800/76266, Loss: 0.2410\n","  Batch 1900/76266, Loss: 0.1717\n","  Batch 2000/76266, Loss: 0.3174\n","  Batch 2100/76266, Loss: 0.2821\n","  Batch 2200/76266, Loss: 0.1580\n","  Batch 2300/76266, Loss: 0.2170\n","  Batch 2400/76266, Loss: 0.2001\n","  Batch 2500/76266, Loss: 0.1504\n","  Batch 2600/76266, Loss: 0.2113\n","  Batch 2700/76266, Loss: 0.2114\n","  Batch 2800/76266, Loss: 0.2728\n","  Batch 2900/76266, Loss: 0.2267\n","  Batch 3000/76266, Loss: 0.1386\n","  Batch 3100/76266, Loss: 0.2207\n","  Batch 3200/76266, Loss: 0.1674\n","  Batch 3300/76266, Loss: 0.1242\n","  Batch 3400/76266, Loss: 0.1989\n","  Batch 3500/76266, Loss: 0.1775\n","  Batch 3600/76266, Loss: 0.1760\n","  Batch 3700/76266, Loss: 0.1793\n","  Batch 3800/76266, Loss: 0.1873\n","  Batch 3900/76266, Loss: 0.1157\n","  Batch 4000/76266, Loss: 0.1409\n","  Batch 4100/76266, Loss: 0.1648\n","  Batch 4200/76266, Loss: 0.2395\n","  Batch 4300/76266, Loss: 0.0860\n","  Batch 4400/76266, Loss: 0.1292\n","  Batch 4500/76266, Loss: 0.1498\n","  Batch 4600/76266, Loss: 0.2661\n","  Batch 4700/76266, Loss: 0.1968\n","  Batch 4800/76266, Loss: 0.1737\n","  Batch 4900/76266, Loss: 0.2460\n","  Batch 5000/76266, Loss: 0.3175\n","  Batch 5100/76266, Loss: 0.2355\n","  Batch 5200/76266, Loss: 0.2513\n","  Batch 5300/76266, Loss: 0.2773\n","  Batch 5400/76266, Loss: 0.1058\n","  Batch 5500/76266, Loss: 0.1701\n","  Batch 5600/76266, Loss: 0.2258\n","  Batch 5700/76266, Loss: 0.1250\n","  Batch 5800/76266, Loss: 0.1784\n","  Batch 5900/76266, Loss: 0.2783\n","  Batch 6000/76266, Loss: 0.1783\n","  Batch 6100/76266, Loss: 0.1183\n","  Batch 6200/76266, Loss: 0.2509\n","  Batch 6300/76266, Loss: 0.1793\n","  Batch 6400/76266, Loss: 0.3047\n","  Batch 6500/76266, Loss: 0.2407\n","  Batch 6600/76266, Loss: 0.1991\n","  Batch 6700/76266, Loss: 0.3328\n","  Batch 6800/76266, Loss: 0.2172\n","  Batch 6900/76266, Loss: 0.1514\n","  Batch 7000/76266, Loss: 0.1852\n","  Batch 7100/76266, Loss: 0.2463\n","  Batch 7200/76266, Loss: 0.1524\n","  Batch 7300/76266, Loss: 0.1532\n","  Batch 7400/76266, Loss: 0.2208\n","  Batch 7500/76266, Loss: 0.1678\n","  Batch 7600/76266, Loss: 0.1563\n","  Batch 7700/76266, Loss: 0.3154\n","  Batch 7800/76266, Loss: 0.1801\n","  Batch 7900/76266, Loss: 0.2704\n","  Batch 8000/76266, Loss: 0.2629\n","  Batch 8100/76266, Loss: 0.1520\n","  Batch 8200/76266, Loss: 0.2579\n","  Batch 8300/76266, Loss: 0.2550\n","  Batch 8400/76266, Loss: 0.1405\n","  Batch 8500/76266, Loss: 0.2177\n","  Batch 8600/76266, Loss: 0.2155\n","  Batch 8700/76266, Loss: 0.1693\n","  Batch 8800/76266, Loss: 0.1919\n","  Batch 8900/76266, Loss: 0.1969\n","  Batch 9000/76266, Loss: 0.1651\n","  Batch 9100/76266, Loss: 0.1979\n","  Batch 9200/76266, Loss: 0.1173\n","  Batch 9300/76266, Loss: 0.1816\n","  Batch 9400/76266, Loss: 0.1899\n","  Batch 9500/76266, Loss: 0.1890\n","  Batch 9600/76266, Loss: 0.1568\n","  Batch 9700/76266, Loss: 0.1428\n","  Batch 9800/76266, Loss: 0.2692\n","  Batch 9900/76266, Loss: 0.1716\n","  Batch 10000/76266, Loss: 0.3178\n","  Batch 10100/76266, Loss: 0.2166\n","  Batch 10200/76266, Loss: 0.3076\n","  Batch 10300/76266, Loss: 0.2212\n","  Batch 10400/76266, Loss: 0.2583\n","  Batch 10500/76266, Loss: 0.2664\n","  Batch 10600/76266, Loss: 0.2343\n","  Batch 10700/76266, Loss: 0.1795\n","  Batch 10800/76266, Loss: 0.1917\n","  Batch 10900/76266, Loss: 0.1582\n","  Batch 11000/76266, Loss: 0.1316\n","  Batch 11100/76266, Loss: 0.2354\n","  Batch 11200/76266, Loss: 0.2117\n","  Batch 11300/76266, Loss: 0.3327\n","  Batch 11400/76266, Loss: 0.1552\n","  Batch 11500/76266, Loss: 0.1102\n","  Batch 11600/76266, Loss: 0.2219\n","  Batch 11700/76266, Loss: 0.1993\n","  Batch 11800/76266, Loss: 0.1182\n","  Batch 11900/76266, Loss: 0.2281\n","  Batch 12000/76266, Loss: 0.2000\n","  Batch 12100/76266, Loss: 0.1779\n","  Batch 12200/76266, Loss: 0.3171\n","  Batch 12300/76266, Loss: 0.1878\n","  Batch 12400/76266, Loss: 0.2206\n","  Batch 12500/76266, Loss: 0.2907\n","  Batch 12600/76266, Loss: 0.1576\n","  Batch 12700/76266, Loss: 0.1692\n","  Batch 12800/76266, Loss: 0.1554\n","  Batch 12900/76266, Loss: 0.2148\n","  Batch 13000/76266, Loss: 0.1851\n","  Batch 13100/76266, Loss: 0.2216\n","  Batch 13200/76266, Loss: 0.1697\n","  Batch 13300/76266, Loss: 0.2610\n","  Batch 13400/76266, Loss: 0.1804\n","  Batch 13500/76266, Loss: 0.1653\n","  Batch 13600/76266, Loss: 0.2483\n","  Batch 13700/76266, Loss: 0.1947\n","  Batch 13800/76266, Loss: 0.2559\n","  Batch 13900/76266, Loss: 0.2465\n","  Batch 14000/76266, Loss: 0.1104\n","  Batch 14100/76266, Loss: 0.1971\n","  Batch 14200/76266, Loss: 0.1670\n","  Batch 14300/76266, Loss: 0.2024\n","  Batch 14400/76266, Loss: 0.3280\n","  Batch 14500/76266, Loss: 0.1854\n","  Batch 14600/76266, Loss: 0.2117\n","  Batch 14700/76266, Loss: 0.1607\n","  Batch 14800/76266, Loss: 0.1986\n","  Batch 14900/76266, Loss: 0.2530\n","  Batch 15000/76266, Loss: 0.2417\n","  Batch 15100/76266, Loss: 0.2722\n","  Batch 15200/76266, Loss: 0.2610\n","  Batch 15300/76266, Loss: 0.2392\n","  Batch 15400/76266, Loss: 0.2615\n","  Batch 15500/76266, Loss: 0.2020\n","  Batch 15600/76266, Loss: 0.1852\n","  Batch 15700/76266, Loss: 0.1875\n","  Batch 15800/76266, Loss: 0.1880\n","  Batch 15900/76266, Loss: 0.1763\n","  Batch 16000/76266, Loss: 0.1617\n","  Batch 16100/76266, Loss: 0.2646\n","  Batch 16200/76266, Loss: 0.1399\n","  Batch 16300/76266, Loss: 0.2942\n","  Batch 16400/76266, Loss: 0.2131\n","  Batch 16500/76266, Loss: 0.2222\n","  Batch 16600/76266, Loss: 0.1747\n","  Batch 16700/76266, Loss: 0.2372\n","  Batch 16800/76266, Loss: 0.2440\n","  Batch 16900/76266, Loss: 0.1493\n","  Batch 17000/76266, Loss: 0.1526\n","  Batch 17100/76266, Loss: 0.1760\n","  Batch 17200/76266, Loss: 0.0980\n","  Batch 17300/76266, Loss: 0.1767\n","  Batch 17400/76266, Loss: 0.1968\n","  Batch 17500/76266, Loss: 0.2328\n","  Batch 17600/76266, Loss: 0.2267\n","  Batch 17700/76266, Loss: 0.1671\n","  Batch 17800/76266, Loss: 0.1415\n","  Batch 17900/76266, Loss: 0.2599\n","  Batch 18000/76266, Loss: 0.2005\n","  Batch 18100/76266, Loss: 0.1937\n","  Batch 18200/76266, Loss: 0.1842\n","  Batch 18300/76266, Loss: 0.1513\n","  Batch 18400/76266, Loss: 0.1465\n","  Batch 18500/76266, Loss: 0.1867\n","  Batch 18600/76266, Loss: 0.1580\n","  Batch 18700/76266, Loss: 0.3194\n","  Batch 18800/76266, Loss: 0.0921\n","  Batch 18900/76266, Loss: 0.1671\n","  Batch 19000/76266, Loss: 0.2391\n","  Batch 19100/76266, Loss: 0.2810\n","  Batch 19200/76266, Loss: 0.1976\n","  Batch 19300/76266, Loss: 0.3150\n","  Batch 19400/76266, Loss: 0.1588\n","  Batch 19500/76266, Loss: 0.3187\n","  Batch 19600/76266, Loss: 0.3104\n","  Batch 19700/76266, Loss: 0.2022\n","  Batch 19800/76266, Loss: 0.1752\n","  Batch 19900/76266, Loss: 0.1337\n","  Batch 20000/76266, Loss: 0.1492\n","  Batch 20100/76266, Loss: 0.2456\n","  Batch 20200/76266, Loss: 0.2323\n","  Batch 20300/76266, Loss: 0.2066\n","  Batch 20400/76266, Loss: 0.1270\n","  Batch 20500/76266, Loss: 0.1792\n","  Batch 20600/76266, Loss: 0.2529\n","  Batch 20700/76266, Loss: 0.2289\n","  Batch 20800/76266, Loss: 0.2736\n","  Batch 20900/76266, Loss: 0.1669\n","  Batch 21000/76266, Loss: 0.3918\n","  Batch 21100/76266, Loss: 0.2041\n","  Batch 21200/76266, Loss: 0.1903\n","  Batch 21300/76266, Loss: 0.1397\n","  Batch 21400/76266, Loss: 0.2438\n","  Batch 21500/76266, Loss: 0.2307\n","  Batch 21600/76266, Loss: 0.1549\n","  Batch 21700/76266, Loss: 0.2869\n","  Batch 21800/76266, Loss: 0.1730\n","  Batch 21900/76266, Loss: 0.2314\n","  Batch 22000/76266, Loss: 0.1369\n","  Batch 22100/76266, Loss: 0.1676\n","  Batch 22200/76266, Loss: 0.2102\n","  Batch 22300/76266, Loss: 0.2055\n","  Batch 22400/76266, Loss: 0.2151\n","  Batch 22500/76266, Loss: 0.1745\n","  Batch 22600/76266, Loss: 0.1930\n","  Batch 22700/76266, Loss: 0.1723\n","  Batch 22800/76266, Loss: 0.3254\n","  Batch 22900/76266, Loss: 0.1989\n","  Batch 23000/76266, Loss: 0.2045\n","  Batch 23100/76266, Loss: 0.1704\n","  Batch 23200/76266, Loss: 0.1806\n","  Batch 23300/76266, Loss: 0.1930\n","  Batch 23400/76266, Loss: 0.1806\n","  Batch 23500/76266, Loss: 0.2298\n","  Batch 23600/76266, Loss: 0.1739\n","  Batch 23700/76266, Loss: 0.2080\n","  Batch 23800/76266, Loss: 0.2170\n","  Batch 23900/76266, Loss: 0.2193\n","  Batch 24000/76266, Loss: 0.1349\n","  Batch 24100/76266, Loss: 0.2066\n","  Batch 24200/76266, Loss: 0.2540\n","  Batch 24300/76266, Loss: 0.1704\n","  Batch 24400/76266, Loss: 0.2967\n","  Batch 24500/76266, Loss: 0.2006\n","  Batch 24600/76266, Loss: 0.1934\n","  Batch 24700/76266, Loss: 0.2371\n","  Batch 24800/76266, Loss: 0.1396\n","  Batch 24900/76266, Loss: 0.2543\n","  Batch 25000/76266, Loss: 0.1813\n","  Batch 25100/76266, Loss: 0.2067\n","  Batch 25200/76266, Loss: 0.1732\n","  Batch 25300/76266, Loss: 0.1568\n","  Batch 25400/76266, Loss: 0.1723\n","  Batch 25500/76266, Loss: 0.1554\n","  Batch 25600/76266, Loss: 0.2196\n","  Batch 25700/76266, Loss: 0.2096\n","  Batch 25800/76266, Loss: 0.1643\n","  Batch 25900/76266, Loss: 0.2193\n","  Batch 26000/76266, Loss: 0.2765\n","  Batch 26100/76266, Loss: 0.2162\n","  Batch 26200/76266, Loss: 0.0834\n","  Batch 26300/76266, Loss: 0.1960\n","  Batch 26400/76266, Loss: 0.1398\n","  Batch 26500/76266, Loss: 0.1106\n","  Batch 26600/76266, Loss: 0.1789\n","  Batch 26700/76266, Loss: 0.2744\n","  Batch 26800/76266, Loss: 0.2532\n","  Batch 26900/76266, Loss: 0.2172\n","  Batch 27000/76266, Loss: 0.2036\n","  Batch 27100/76266, Loss: 0.1895\n","  Batch 27200/76266, Loss: 0.2987\n","  Batch 27300/76266, Loss: 0.2571\n","  Batch 27400/76266, Loss: 0.1842\n","  Batch 27500/76266, Loss: 0.2692\n","  Batch 27600/76266, Loss: 0.1749\n","  Batch 27700/76266, Loss: 0.2327\n","  Batch 27800/76266, Loss: 0.2725\n","  Batch 27900/76266, Loss: 0.2709\n","  Batch 28000/76266, Loss: 0.2527\n","  Batch 28100/76266, Loss: 0.1481\n","  Batch 28200/76266, Loss: 0.1578\n","  Batch 28300/76266, Loss: 0.3645\n","  Batch 28400/76266, Loss: 0.2198\n","  Batch 28500/76266, Loss: 0.1824\n","  Batch 28600/76266, Loss: 0.1929\n","  Batch 28700/76266, Loss: 0.1950\n","  Batch 28800/76266, Loss: 0.2070\n","  Batch 28900/76266, Loss: 0.2077\n","  Batch 29000/76266, Loss: 0.2643\n","  Batch 29100/76266, Loss: 0.2158\n","  Batch 29200/76266, Loss: 0.2541\n","  Batch 29300/76266, Loss: 0.2106\n","  Batch 29400/76266, Loss: 0.1833\n","  Batch 29500/76266, Loss: 0.2310\n","  Batch 29600/76266, Loss: 0.1643\n","  Batch 29700/76266, Loss: 0.1536\n","  Batch 29800/76266, Loss: 0.2345\n","  Batch 29900/76266, Loss: 0.1045\n","  Batch 30000/76266, Loss: 0.2234\n","  Batch 30100/76266, Loss: 0.1911\n","  Batch 30200/76266, Loss: 0.2631\n","  Batch 30300/76266, Loss: 0.1572\n","  Batch 30400/76266, Loss: 0.2111\n","  Batch 30500/76266, Loss: 0.2817\n","  Batch 30600/76266, Loss: 0.1207\n","  Batch 30700/76266, Loss: 0.2325\n","  Batch 30800/76266, Loss: 0.2379\n","  Batch 30900/76266, Loss: 0.2166\n","  Batch 31000/76266, Loss: 0.1365\n","  Batch 31100/76266, Loss: 0.1467\n","  Batch 31200/76266, Loss: 0.1419\n","  Batch 31300/76266, Loss: 0.1371\n","  Batch 31400/76266, Loss: 0.1268\n","  Batch 31500/76266, Loss: 0.1423\n","  Batch 31600/76266, Loss: 0.3128\n","  Batch 31700/76266, Loss: 0.2576\n","  Batch 31800/76266, Loss: 0.1639\n","  Batch 31900/76266, Loss: 0.1621\n","  Batch 32000/76266, Loss: 0.1860\n","  Batch 32100/76266, Loss: 0.1957\n","  Batch 32200/76266, Loss: 0.1861\n","  Batch 32300/76266, Loss: 0.2048\n","  Batch 32400/76266, Loss: 0.1566\n","  Batch 32500/76266, Loss: 0.2467\n","  Batch 32600/76266, Loss: 0.1027\n","  Batch 32700/76266, Loss: 0.1094\n","  Batch 32800/76266, Loss: 0.2385\n","  Batch 32900/76266, Loss: 0.2050\n","  Batch 33000/76266, Loss: 0.3051\n","  Batch 33100/76266, Loss: 0.1325\n","  Batch 33200/76266, Loss: 0.1830\n","  Batch 33300/76266, Loss: 0.1870\n","  Batch 33400/76266, Loss: 0.1133\n","  Batch 33500/76266, Loss: 0.2089\n","  Batch 33600/76266, Loss: 0.2441\n","  Batch 33700/76266, Loss: 0.2200\n","  Batch 33800/76266, Loss: 0.1319\n","  Batch 33900/76266, Loss: 0.1659\n","  Batch 34000/76266, Loss: 0.1422\n","  Batch 34100/76266, Loss: 0.1668\n","  Batch 34200/76266, Loss: 0.1015\n","  Batch 34300/76266, Loss: 0.1389\n","  Batch 34400/76266, Loss: 0.2347\n","  Batch 34500/76266, Loss: 0.2282\n","  Batch 34600/76266, Loss: 0.2658\n","  Batch 34700/76266, Loss: 0.1560\n","  Batch 34800/76266, Loss: 0.1281\n","  Batch 34900/76266, Loss: 0.3291\n","  Batch 35000/76266, Loss: 0.1821\n","  Batch 35100/76266, Loss: 0.2044\n","  Batch 35200/76266, Loss: 0.1373\n","  Batch 35300/76266, Loss: 0.1702\n","  Batch 35400/76266, Loss: 0.2323\n","  Batch 35500/76266, Loss: 0.1478\n","  Batch 35600/76266, Loss: 0.1552\n","  Batch 35700/76266, Loss: 0.1755\n","  Batch 35800/76266, Loss: 0.1759\n","  Batch 35900/76266, Loss: 0.2119\n","  Batch 36000/76266, Loss: 0.2380\n","  Batch 36100/76266, Loss: 0.1594\n","  Batch 36200/76266, Loss: 0.1564\n","  Batch 36300/76266, Loss: 0.1533\n","  Batch 36400/76266, Loss: 0.1870\n","  Batch 36500/76266, Loss: 0.1580\n","  Batch 36600/76266, Loss: 0.1864\n","  Batch 36700/76266, Loss: 0.1268\n","  Batch 36800/76266, Loss: 0.2029\n","  Batch 36900/76266, Loss: 0.1901\n","  Batch 37000/76266, Loss: 0.2068\n","  Batch 37100/76266, Loss: 0.3116\n","  Batch 37200/76266, Loss: 0.1610\n","  Batch 37300/76266, Loss: 0.2646\n","  Batch 37400/76266, Loss: 0.2343\n","  Batch 37500/76266, Loss: 0.2166\n","  Batch 37600/76266, Loss: 0.2145\n","  Batch 37700/76266, Loss: 0.1937\n","  Batch 37800/76266, Loss: 0.2107\n","  Batch 37900/76266, Loss: 0.3091\n","  Batch 38000/76266, Loss: 0.1821\n","  Batch 38100/76266, Loss: 0.0974\n","  Batch 38200/76266, Loss: 0.2048\n","  Batch 38300/76266, Loss: 0.1544\n","  Batch 38400/76266, Loss: 0.1765\n","  Batch 38500/76266, Loss: 0.2508\n","  Batch 38600/76266, Loss: 0.2036\n","  Batch 38700/76266, Loss: 0.1695\n","  Batch 38800/76266, Loss: 0.1414\n","  Batch 38900/76266, Loss: 0.2138\n","  Batch 39000/76266, Loss: 0.2773\n","  Batch 39100/76266, Loss: 0.1572\n","  Batch 39200/76266, Loss: 0.2302\n","  Batch 39300/76266, Loss: 0.1468\n","  Batch 39400/76266, Loss: 0.2423\n","  Batch 39500/76266, Loss: 0.2049\n","  Batch 39600/76266, Loss: 0.2150\n","  Batch 39700/76266, Loss: 0.2176\n","  Batch 39800/76266, Loss: 0.2172\n","  Batch 39900/76266, Loss: 0.1160\n","  Batch 40000/76266, Loss: 0.2247\n","  Batch 40100/76266, Loss: 0.1858\n","  Batch 40200/76266, Loss: 0.1654\n","  Batch 40300/76266, Loss: 0.1742\n","  Batch 40400/76266, Loss: 0.2464\n","  Batch 40500/76266, Loss: 0.2041\n","  Batch 40600/76266, Loss: 0.1845\n","  Batch 40700/76266, Loss: 0.2099\n","  Batch 40800/76266, Loss: 0.2025\n","  Batch 40900/76266, Loss: 0.2317\n","  Batch 41000/76266, Loss: 0.1976\n","  Batch 41100/76266, Loss: 0.1893\n","  Batch 41200/76266, Loss: 0.1837\n","  Batch 41300/76266, Loss: 0.1883\n","  Batch 41400/76266, Loss: 0.2328\n","  Batch 41500/76266, Loss: 0.1244\n","  Batch 41600/76266, Loss: 0.2116\n","  Batch 41700/76266, Loss: 0.1041\n","  Batch 41800/76266, Loss: 0.1994\n","  Batch 41900/76266, Loss: 0.1975\n","  Batch 42000/76266, Loss: 0.1891\n","  Batch 42100/76266, Loss: 0.1603\n","  Batch 42200/76266, Loss: 0.2482\n","  Batch 42300/76266, Loss: 0.1729\n","  Batch 42400/76266, Loss: 0.1563\n","  Batch 42500/76266, Loss: 0.2529\n","  Batch 42600/76266, Loss: 0.2582\n","  Batch 42700/76266, Loss: 0.2247\n","  Batch 42800/76266, Loss: 0.2879\n","  Batch 42900/76266, Loss: 0.1841\n","  Batch 43000/76266, Loss: 0.2111\n","  Batch 43100/76266, Loss: 0.1793\n","  Batch 43200/76266, Loss: 0.1094\n","  Batch 43300/76266, Loss: 0.1470\n","  Batch 43400/76266, Loss: 0.2450\n","  Batch 43500/76266, Loss: 0.1595\n","  Batch 43600/76266, Loss: 0.1510\n","  Batch 43700/76266, Loss: 0.1521\n","  Batch 43800/76266, Loss: 0.1497\n","  Batch 43900/76266, Loss: 0.2627\n","  Batch 44000/76266, Loss: 0.2826\n","  Batch 44100/76266, Loss: 0.1525\n","  Batch 44200/76266, Loss: 0.2206\n","  Batch 44300/76266, Loss: 0.4608\n","  Batch 44400/76266, Loss: 0.1515\n","  Batch 44500/76266, Loss: 0.2191\n","  Batch 44600/76266, Loss: 0.1493\n","  Batch 44700/76266, Loss: 0.3278\n","  Batch 44800/76266, Loss: 0.1590\n","  Batch 44900/76266, Loss: 0.1605\n","  Batch 45000/76266, Loss: 0.1858\n","  Batch 45100/76266, Loss: 0.1789\n","  Batch 45200/76266, Loss: 0.1410\n","  Batch 45300/76266, Loss: 0.2489\n","  Batch 45400/76266, Loss: 0.1788\n","  Batch 45500/76266, Loss: 0.1830\n","  Batch 45600/76266, Loss: 0.2198\n","  Batch 45700/76266, Loss: 0.1033\n","  Batch 45800/76266, Loss: 0.2400\n","  Batch 45900/76266, Loss: 0.1904\n","  Batch 46000/76266, Loss: 0.0719\n","  Batch 46100/76266, Loss: 0.1952\n","  Batch 46200/76266, Loss: 0.1501\n","  Batch 46300/76266, Loss: 0.2341\n","  Batch 46400/76266, Loss: 0.1325\n","  Batch 46500/76266, Loss: 0.1399\n","  Batch 46600/76266, Loss: 0.1825\n","  Batch 46700/76266, Loss: 0.1323\n","  Batch 46800/76266, Loss: 0.1054\n","  Batch 46900/76266, Loss: 0.1450\n","  Batch 47000/76266, Loss: 0.2339\n","  Batch 47100/76266, Loss: 0.1293\n","  Batch 47200/76266, Loss: 0.2351\n","  Batch 47300/76266, Loss: 0.1948\n","  Batch 47400/76266, Loss: 0.2359\n","  Batch 47500/76266, Loss: 0.1942\n","  Batch 47600/76266, Loss: 0.1251\n","  Batch 47700/76266, Loss: 0.1592\n","  Batch 47800/76266, Loss: 0.1504\n","  Batch 47900/76266, Loss: 0.1820\n","  Batch 48000/76266, Loss: 0.1788\n","  Batch 48100/76266, Loss: 0.2676\n","  Batch 48200/76266, Loss: 0.1717\n","  Batch 48300/76266, Loss: 0.1743\n","  Batch 48400/76266, Loss: 0.2337\n","  Batch 48500/76266, Loss: 0.2691\n","  Batch 48600/76266, Loss: 0.2195\n","  Batch 48700/76266, Loss: 0.1962\n","  Batch 48800/76266, Loss: 0.2601\n","  Batch 48900/76266, Loss: 0.1479\n","  Batch 49000/76266, Loss: 0.1300\n","  Batch 49100/76266, Loss: 0.2421\n","  Batch 49200/76266, Loss: 0.1610\n","  Batch 49300/76266, Loss: 0.1644\n","  Batch 49400/76266, Loss: 0.1643\n","  Batch 49500/76266, Loss: 0.2049\n","  Batch 49600/76266, Loss: 0.1892\n","  Batch 49700/76266, Loss: 0.2110\n","  Batch 49800/76266, Loss: 0.2457\n","  Batch 49900/76266, Loss: 0.2552\n","  Batch 50000/76266, Loss: 0.2504\n","  Batch 50100/76266, Loss: 0.2234\n","  Batch 50200/76266, Loss: 0.1702\n","  Batch 50300/76266, Loss: 0.2774\n","  Batch 50400/76266, Loss: 0.2562\n","  Batch 50500/76266, Loss: 0.1243\n","  Batch 50600/76266, Loss: 0.1512\n","  Batch 50700/76266, Loss: 0.0935\n","  Batch 50800/76266, Loss: 0.1785\n","  Batch 50900/76266, Loss: 0.1612\n","  Batch 51000/76266, Loss: 0.3212\n","  Batch 51100/76266, Loss: 0.2323\n","  Batch 51200/76266, Loss: 0.2011\n","  Batch 51300/76266, Loss: 0.1508\n","  Batch 51400/76266, Loss: 0.1881\n","  Batch 51500/76266, Loss: 0.1988\n","  Batch 51600/76266, Loss: 0.1585\n","  Batch 51700/76266, Loss: 0.2006\n","  Batch 51800/76266, Loss: 0.2060\n","  Batch 51900/76266, Loss: 0.2123\n","  Batch 52000/76266, Loss: 0.2968\n","  Batch 52100/76266, Loss: 0.2070\n","  Batch 52200/76266, Loss: 0.2128\n","  Batch 52300/76266, Loss: 0.1823\n","  Batch 52400/76266, Loss: 0.1607\n","  Batch 52500/76266, Loss: 0.2812\n","  Batch 52600/76266, Loss: 0.1427\n","  Batch 52700/76266, Loss: 0.1735\n","  Batch 52800/76266, Loss: 0.3883\n","  Batch 52900/76266, Loss: 0.1800\n","  Batch 53000/76266, Loss: 0.2503\n","  Batch 53100/76266, Loss: 0.1523\n","  Batch 53200/76266, Loss: 0.2232\n","  Batch 53300/76266, Loss: 0.3323\n","  Batch 53400/76266, Loss: 0.1775\n","  Batch 53500/76266, Loss: 0.2074\n","  Batch 53600/76266, Loss: 0.1658\n","  Batch 53700/76266, Loss: 0.1225\n","  Batch 53800/76266, Loss: 0.2228\n","  Batch 53900/76266, Loss: 0.2361\n","  Batch 54000/76266, Loss: 0.1485\n","  Batch 54100/76266, Loss: 0.0955\n","  Batch 54200/76266, Loss: 0.2784\n","  Batch 54300/76266, Loss: 0.1540\n","  Batch 54400/76266, Loss: 0.1736\n","  Batch 54500/76266, Loss: 0.1892\n","  Batch 54600/76266, Loss: 0.1811\n","  Batch 54700/76266, Loss: 0.1575\n","  Batch 54800/76266, Loss: 0.1530\n","  Batch 54900/76266, Loss: 0.1900\n","  Batch 55000/76266, Loss: 0.1584\n","  Batch 55100/76266, Loss: 0.1232\n","  Batch 55200/76266, Loss: 0.2686\n","  Batch 55300/76266, Loss: 0.1164\n","  Batch 55400/76266, Loss: 0.2373\n","  Batch 55500/76266, Loss: 0.1730\n","  Batch 55600/76266, Loss: 0.2013\n","  Batch 55700/76266, Loss: 0.2289\n","  Batch 55800/76266, Loss: 0.2042\n","  Batch 55900/76266, Loss: 0.1925\n","  Batch 56000/76266, Loss: 0.1446\n","  Batch 56100/76266, Loss: 0.2316\n","  Batch 56200/76266, Loss: 0.1721\n","  Batch 56300/76266, Loss: 0.1293\n","  Batch 56400/76266, Loss: 0.1417\n","  Batch 56500/76266, Loss: 0.1720\n","  Batch 56600/76266, Loss: 0.1427\n","  Batch 56700/76266, Loss: 0.1863\n","  Batch 56800/76266, Loss: 0.2259\n","  Batch 56900/76266, Loss: 0.1153\n","  Batch 57000/76266, Loss: 0.2235\n","  Batch 57100/76266, Loss: 0.1567\n","  Batch 57200/76266, Loss: 0.2002\n","  Batch 57300/76266, Loss: 0.2007\n","  Batch 57400/76266, Loss: 0.1970\n","  Batch 57500/76266, Loss: 0.3015\n","  Batch 57600/76266, Loss: 0.1976\n","  Batch 57700/76266, Loss: 0.1825\n","  Batch 57800/76266, Loss: 0.1602\n","  Batch 57900/76266, Loss: 0.2068\n","  Batch 58000/76266, Loss: 0.3111\n","  Batch 58100/76266, Loss: 0.1880\n","  Batch 58200/76266, Loss: 0.1333\n","  Batch 58300/76266, Loss: 0.1850\n","  Batch 58400/76266, Loss: 0.2252\n","  Batch 58500/76266, Loss: 0.2413\n","  Batch 58600/76266, Loss: 0.1239\n","  Batch 58700/76266, Loss: 0.2548\n","  Batch 58800/76266, Loss: 0.2110\n","  Batch 58900/76266, Loss: 0.1625\n","  Batch 59000/76266, Loss: 0.3206\n","  Batch 59100/76266, Loss: 0.2286\n","  Batch 59200/76266, Loss: 0.2365\n","  Batch 59300/76266, Loss: 0.2358\n","  Batch 59400/76266, Loss: 0.2226\n","  Batch 59500/76266, Loss: 0.2129\n","  Batch 59600/76266, Loss: 0.3003\n","  Batch 59700/76266, Loss: 0.1623\n","  Batch 59800/76266, Loss: 0.1712\n","  Batch 59900/76266, Loss: 0.2608\n","  Batch 60000/76266, Loss: 0.2422\n","  Batch 60100/76266, Loss: 0.1610\n","  Batch 60200/76266, Loss: 0.2266\n","  Batch 60300/76266, Loss: 0.3118\n","  Batch 60400/76266, Loss: 0.1085\n","  Batch 60500/76266, Loss: 0.2288\n","  Batch 60600/76266, Loss: 0.1520\n","  Batch 60700/76266, Loss: 0.1582\n","  Batch 60800/76266, Loss: 0.2867\n","  Batch 60900/76266, Loss: 0.1628\n","  Batch 61000/76266, Loss: 0.1794\n","  Batch 61100/76266, Loss: 0.1564\n","  Batch 61200/76266, Loss: 0.1875\n","  Batch 61300/76266, Loss: 0.2002\n","  Batch 61400/76266, Loss: 0.2238\n","  Batch 61500/76266, Loss: 0.1295\n","  Batch 61600/76266, Loss: 0.1636\n","  Batch 61700/76266, Loss: 0.2851\n","  Batch 61800/76266, Loss: 0.3298\n","  Batch 61900/76266, Loss: 0.3094\n","  Batch 62000/76266, Loss: 0.2696\n","  Batch 62100/76266, Loss: 0.1581\n","  Batch 62200/76266, Loss: 0.1908\n","  Batch 62300/76266, Loss: 0.2102\n","  Batch 62400/76266, Loss: 0.2078\n","  Batch 62500/76266, Loss: 0.2751\n","  Batch 62600/76266, Loss: 0.2638\n","  Batch 62700/76266, Loss: 0.1223\n","  Batch 62800/76266, Loss: 0.1855\n","  Batch 62900/76266, Loss: 0.2348\n","  Batch 63000/76266, Loss: 0.1229\n","  Batch 63100/76266, Loss: 0.2206\n","  Batch 63200/76266, Loss: 0.1171\n","  Batch 63300/76266, Loss: 0.2048\n","  Batch 63400/76266, Loss: 0.2628\n","  Batch 63500/76266, Loss: 0.2431\n","  Batch 63600/76266, Loss: 0.1578\n","  Batch 63700/76266, Loss: 0.1650\n","  Batch 63800/76266, Loss: 0.2641\n","  Batch 63900/76266, Loss: 0.1545\n","  Batch 64000/76266, Loss: 0.2187\n","  Batch 64100/76266, Loss: 0.3032\n","  Batch 64200/76266, Loss: 0.1935\n","  Batch 64300/76266, Loss: 0.1333\n","  Batch 64400/76266, Loss: 0.2425\n","  Batch 64500/76266, Loss: 0.2560\n","  Batch 64600/76266, Loss: 0.1558\n","  Batch 64700/76266, Loss: 0.2038\n","  Batch 64800/76266, Loss: 0.1204\n","  Batch 64900/76266, Loss: 0.0650\n","  Batch 65000/76266, Loss: 0.1162\n","  Batch 65100/76266, Loss: 0.3359\n","  Batch 65200/76266, Loss: 0.3680\n","  Batch 65300/76266, Loss: 0.2217\n","  Batch 65400/76266, Loss: 0.1681\n","  Batch 65500/76266, Loss: 0.2540\n","  Batch 65600/76266, Loss: 0.2484\n","  Batch 65700/76266, Loss: 0.2583\n","  Batch 65800/76266, Loss: 0.1621\n","  Batch 65900/76266, Loss: 0.1115\n","  Batch 66000/76266, Loss: 0.2790\n","  Batch 66100/76266, Loss: 0.1974\n","  Batch 66200/76266, Loss: 0.3531\n","  Batch 66300/76266, Loss: 0.1371\n","  Batch 66400/76266, Loss: 0.3072\n","  Batch 66500/76266, Loss: 0.1466\n","  Batch 66600/76266, Loss: 0.1399\n","  Batch 66700/76266, Loss: 0.2378\n","  Batch 66800/76266, Loss: 0.2150\n","  Batch 66900/76266, Loss: 0.2196\n","  Batch 67000/76266, Loss: 0.2396\n","  Batch 67100/76266, Loss: 0.1815\n","  Batch 67200/76266, Loss: 0.1190\n","  Batch 67300/76266, Loss: 0.1916\n","  Batch 67400/76266, Loss: 0.2017\n","  Batch 67500/76266, Loss: 0.1218\n","  Batch 67600/76266, Loss: 0.1471\n","  Batch 67700/76266, Loss: 0.2201\n","  Batch 67800/76266, Loss: 0.2741\n","  Batch 67900/76266, Loss: 0.1468\n","  Batch 68000/76266, Loss: 0.2631\n","  Batch 68100/76266, Loss: 0.2291\n","  Batch 68200/76266, Loss: 0.2040\n","  Batch 68300/76266, Loss: 0.1624\n","  Batch 68400/76266, Loss: 0.2113\n","  Batch 68500/76266, Loss: 0.2675\n","  Batch 68600/76266, Loss: 0.1420\n","  Batch 68700/76266, Loss: 0.1066\n","  Batch 68800/76266, Loss: 0.2612\n","  Batch 68900/76266, Loss: 0.2464\n","  Batch 69000/76266, Loss: 0.1354\n","  Batch 69100/76266, Loss: 0.1768\n","  Batch 69200/76266, Loss: 0.3263\n","  Batch 69300/76266, Loss: 0.1345\n","  Batch 69400/76266, Loss: 0.1291\n","  Batch 69500/76266, Loss: 0.1810\n","  Batch 69600/76266, Loss: 0.1641\n","  Batch 69700/76266, Loss: 0.2637\n","  Batch 69800/76266, Loss: 0.2867\n","  Batch 69900/76266, Loss: 0.1829\n","  Batch 70000/76266, Loss: 0.2391\n","  Batch 70100/76266, Loss: 0.1725\n","  Batch 70200/76266, Loss: 0.1446\n","  Batch 70300/76266, Loss: 0.2972\n","  Batch 70400/76266, Loss: 0.1746\n","  Batch 70500/76266, Loss: 0.1661\n","  Batch 70600/76266, Loss: 0.2459\n","  Batch 70700/76266, Loss: 0.2377\n","  Batch 70800/76266, Loss: 0.1397\n","  Batch 70900/76266, Loss: 0.2857\n","  Batch 71000/76266, Loss: 0.2010\n","  Batch 71100/76266, Loss: 0.2043\n","  Batch 71200/76266, Loss: 0.1699\n","  Batch 71300/76266, Loss: 0.2112\n","  Batch 71400/76266, Loss: 0.2498\n","  Batch 71500/76266, Loss: 0.2223\n","  Batch 71600/76266, Loss: 0.2399\n","  Batch 71700/76266, Loss: 0.1318\n","  Batch 71800/76266, Loss: 0.2654\n","  Batch 71900/76266, Loss: 0.2251\n","  Batch 72000/76266, Loss: 0.1949\n","  Batch 72100/76266, Loss: 0.1900\n","  Batch 72200/76266, Loss: 0.2093\n","  Batch 72300/76266, Loss: 0.2615\n","  Batch 72400/76266, Loss: 0.2174\n","  Batch 72500/76266, Loss: 0.2776\n","  Batch 72600/76266, Loss: 0.1061\n","  Batch 72700/76266, Loss: 0.1561\n","  Batch 72800/76266, Loss: 0.1214\n","  Batch 72900/76266, Loss: 0.1538\n","  Batch 73000/76266, Loss: 0.1539\n","  Batch 73100/76266, Loss: 0.1732\n","  Batch 73200/76266, Loss: 0.3395\n","  Batch 73300/76266, Loss: 0.1320\n","  Batch 73400/76266, Loss: 0.2431\n","  Batch 73500/76266, Loss: 0.1835\n","  Batch 73600/76266, Loss: 0.1275\n","  Batch 73700/76266, Loss: 0.1961\n","  Batch 73800/76266, Loss: 0.1406\n","  Batch 73900/76266, Loss: 0.1540\n","  Batch 74000/76266, Loss: 0.2782\n","  Batch 74100/76266, Loss: 0.1416\n","  Batch 74200/76266, Loss: 0.2105\n","  Batch 74300/76266, Loss: 0.1659\n","  Batch 74400/76266, Loss: 0.2720\n","  Batch 74500/76266, Loss: 0.1487\n","  Batch 74600/76266, Loss: 0.1369\n","  Batch 74700/76266, Loss: 0.2797\n","  Batch 74800/76266, Loss: 0.1629\n","  Batch 74900/76266, Loss: 0.2112\n","  Batch 75000/76266, Loss: 0.1013\n","  Batch 75100/76266, Loss: 0.2105\n","  Batch 75200/76266, Loss: 0.2648\n","  Batch 75300/76266, Loss: 0.1598\n","  Batch 75400/76266, Loss: 0.1933\n","  Batch 75500/76266, Loss: 0.2037\n","  Batch 75600/76266, Loss: 0.0774\n","  Batch 75700/76266, Loss: 0.1475\n","  Batch 75800/76266, Loss: 0.2377\n","  Batch 75900/76266, Loss: 0.1595\n","  Batch 76000/76266, Loss: 0.1273\n","  Batch 76100/76266, Loss: 0.1977\n","  Batch 76200/76266, Loss: 0.1651\n","Epoch 2 completed in 11581.31s. Average Training Loss: 0.2025\n","--- Epoch 3/3 ---\n","  Batch 100/76266, Loss: 0.1581\n","  Batch 200/76266, Loss: 0.1791\n","  Batch 300/76266, Loss: 0.1786\n","  Batch 400/76266, Loss: 0.2439\n","  Batch 500/76266, Loss: 0.1951\n","  Batch 600/76266, Loss: 0.2263\n","  Batch 700/76266, Loss: 0.1488\n","  Batch 800/76266, Loss: 0.0846\n","  Batch 900/76266, Loss: 0.1654\n","  Batch 1000/76266, Loss: 0.1629\n","  Batch 1100/76266, Loss: 0.2740\n","  Batch 1200/76266, Loss: 0.1396\n","  Batch 1300/76266, Loss: 0.2975\n","  Batch 1400/76266, Loss: 0.2110\n","  Batch 1500/76266, Loss: 0.0947\n","  Batch 1600/76266, Loss: 0.0677\n","  Batch 1700/76266, Loss: 0.2375\n","  Batch 1800/76266, Loss: 0.2209\n","  Batch 1900/76266, Loss: 0.2338\n","  Batch 2000/76266, Loss: 0.0951\n","  Batch 2100/76266, Loss: 0.2041\n","  Batch 2200/76266, Loss: 0.2131\n","  Batch 2300/76266, Loss: 0.2192\n","  Batch 2400/76266, Loss: 0.1516\n","  Batch 2500/76266, Loss: 0.2237\n","  Batch 2600/76266, Loss: 0.1795\n","  Batch 2700/76266, Loss: 0.2098\n","  Batch 2800/76266, Loss: 0.1985\n","  Batch 2900/76266, Loss: 0.1628\n","  Batch 3000/76266, Loss: 0.1422\n","  Batch 3100/76266, Loss: 0.1531\n","  Batch 3200/76266, Loss: 0.1347\n","  Batch 3300/76266, Loss: 0.2064\n","  Batch 3400/76266, Loss: 0.1842\n","  Batch 3500/76266, Loss: 0.1506\n","  Batch 3600/76266, Loss: 0.2122\n","  Batch 3700/76266, Loss: 0.1374\n","  Batch 3800/76266, Loss: 0.1611\n","  Batch 3900/76266, Loss: 0.2446\n","  Batch 4000/76266, Loss: 0.1855\n","  Batch 4100/76266, Loss: 0.1359\n","  Batch 4200/76266, Loss: 0.1747\n","  Batch 4300/76266, Loss: 0.1375\n","  Batch 4400/76266, Loss: 0.2465\n","  Batch 4500/76266, Loss: 0.1958\n","  Batch 4600/76266, Loss: 0.1785\n","  Batch 4700/76266, Loss: 0.3493\n","  Batch 4800/76266, Loss: 0.1655\n","  Batch 4900/76266, Loss: 0.1167\n","  Batch 5000/76266, Loss: 0.2245\n","  Batch 5100/76266, Loss: 0.1526\n","  Batch 5200/76266, Loss: 0.1718\n","  Batch 5300/76266, Loss: 0.2206\n","  Batch 5400/76266, Loss: 0.2017\n","  Batch 5500/76266, Loss: 0.1409\n","  Batch 5600/76266, Loss: 0.1853\n","  Batch 5700/76266, Loss: 0.1557\n","  Batch 5800/76266, Loss: 0.1173\n","  Batch 5900/76266, Loss: 0.0767\n","  Batch 6000/76266, Loss: 0.2415\n","  Batch 6100/76266, Loss: 0.1893\n","  Batch 6200/76266, Loss: 0.1193\n","  Batch 6300/76266, Loss: 0.1577\n","  Batch 6400/76266, Loss: 0.1626\n","  Batch 6500/76266, Loss: 0.0796\n","  Batch 6600/76266, Loss: 0.1856\n","  Batch 6700/76266, Loss: 0.2947\n","  Batch 6800/76266, Loss: 0.1652\n","  Batch 6900/76266, Loss: 0.1819\n","  Batch 7000/76266, Loss: 0.1986\n","  Batch 7100/76266, Loss: 0.1830\n","  Batch 7200/76266, Loss: 0.0828\n","  Batch 7300/76266, Loss: 0.2284\n","  Batch 7400/76266, Loss: 0.2144\n","  Batch 7500/76266, Loss: 0.2210\n","  Batch 7600/76266, Loss: 0.2809\n","  Batch 7700/76266, Loss: 0.1586\n","  Batch 7800/76266, Loss: 0.1357\n","  Batch 7900/76266, Loss: 0.1394\n","  Batch 8000/76266, Loss: 0.1984\n","  Batch 8100/76266, Loss: 0.1724\n","  Batch 8200/76266, Loss: 0.1500\n","  Batch 8300/76266, Loss: 0.1405\n","  Batch 8400/76266, Loss: 0.1387\n","  Batch 8500/76266, Loss: 0.0649\n","  Batch 8600/76266, Loss: 0.2856\n","  Batch 8700/76266, Loss: 0.1854\n","  Batch 8800/76266, Loss: 0.2356\n","  Batch 8900/76266, Loss: 0.1878\n","  Batch 9000/76266, Loss: 0.1261\n","  Batch 9100/76266, Loss: 0.1773\n","  Batch 9200/76266, Loss: 0.2432\n","  Batch 9300/76266, Loss: 0.0786\n","  Batch 9400/76266, Loss: 0.2285\n","  Batch 9500/76266, Loss: 0.1129\n","  Batch 9600/76266, Loss: 0.2016\n","  Batch 9700/76266, Loss: 0.2026\n","  Batch 9800/76266, Loss: 0.0923\n","  Batch 9900/76266, Loss: 0.3019\n","  Batch 10000/76266, Loss: 0.2178\n","  Batch 10100/76266, Loss: 0.1692\n","  Batch 10200/76266, Loss: 0.1216\n","  Batch 10300/76266, Loss: 0.3508\n","  Batch 10400/76266, Loss: 0.1884\n","  Batch 10500/76266, Loss: 0.1272\n","  Batch 10600/76266, Loss: 0.1699\n","  Batch 10700/76266, Loss: 0.2636\n","  Batch 10800/76266, Loss: 0.1875\n","  Batch 10900/76266, Loss: 0.1464\n","  Batch 11000/76266, Loss: 0.1026\n","  Batch 11100/76266, Loss: 0.1775\n","  Batch 11200/76266, Loss: 0.1364\n","  Batch 11300/76266, Loss: 0.2127\n","  Batch 11400/76266, Loss: 0.1443\n","  Batch 11500/76266, Loss: 0.1345\n","  Batch 11600/76266, Loss: 0.2193\n","  Batch 11700/76266, Loss: 0.1877\n","  Batch 11800/76266, Loss: 0.2652\n","  Batch 11900/76266, Loss: 0.1645\n","  Batch 12000/76266, Loss: 0.1680\n","  Batch 12100/76266, Loss: 0.1260\n","  Batch 12200/76266, Loss: 0.1203\n","  Batch 12300/76266, Loss: 0.1548\n","  Batch 12400/76266, Loss: 0.1169\n","  Batch 12500/76266, Loss: 0.1734\n","  Batch 12600/76266, Loss: 0.2050\n","  Batch 12700/76266, Loss: 0.1184\n","  Batch 12800/76266, Loss: 0.1856\n","  Batch 12900/76266, Loss: 0.2727\n","  Batch 13000/76266, Loss: 0.1386\n","  Batch 13100/76266, Loss: 0.1717\n","  Batch 13200/76266, Loss: 0.1484\n","  Batch 13300/76266, Loss: 0.1874\n","  Batch 13400/76266, Loss: 0.1470\n","  Batch 13500/76266, Loss: 0.1175\n","  Batch 13600/76266, Loss: 0.1090\n","  Batch 13700/76266, Loss: 0.2962\n","  Batch 13800/76266, Loss: 0.1302\n","  Batch 13900/76266, Loss: 0.2048\n","  Batch 14000/76266, Loss: 0.1433\n","  Batch 14100/76266, Loss: 0.1732\n","  Batch 14200/76266, Loss: 0.1446\n","  Batch 14300/76266, Loss: 0.1838\n","  Batch 14400/76266, Loss: 0.1290\n","  Batch 14500/76266, Loss: 0.1095\n","  Batch 14600/76266, Loss: 0.2538\n","  Batch 14700/76266, Loss: 0.1637\n","  Batch 14800/76266, Loss: 0.1752\n","  Batch 14900/76266, Loss: 0.1958\n","  Batch 15000/76266, Loss: 0.1487\n","  Batch 15100/76266, Loss: 0.1466\n","  Batch 15200/76266, Loss: 0.2118\n","  Batch 15300/76266, Loss: 0.2186\n","  Batch 15400/76266, Loss: 0.1903\n","  Batch 15500/76266, Loss: 0.1643\n","  Batch 15600/76266, Loss: 0.1612\n","  Batch 15700/76266, Loss: 0.2397\n","  Batch 15800/76266, Loss: 0.2024\n","  Batch 15900/76266, Loss: 0.0955\n","  Batch 16000/76266, Loss: 0.1974\n","  Batch 16100/76266, Loss: 0.1941\n","  Batch 16200/76266, Loss: 0.1691\n","  Batch 16300/76266, Loss: 0.2059\n","  Batch 16400/76266, Loss: 0.1133\n","  Batch 16500/76266, Loss: 0.0716\n","  Batch 16600/76266, Loss: 0.1284\n","  Batch 16700/76266, Loss: 0.2831\n","  Batch 16800/76266, Loss: 0.2005\n","  Batch 16900/76266, Loss: 0.1505\n","  Batch 17000/76266, Loss: 0.1664\n","  Batch 17100/76266, Loss: 0.2478\n","  Batch 17200/76266, Loss: 0.2913\n","  Batch 17300/76266, Loss: 0.0807\n","  Batch 17400/76266, Loss: 0.1836\n","  Batch 17500/76266, Loss: 0.1720\n","  Batch 17600/76266, Loss: 0.1578\n","  Batch 17700/76266, Loss: 0.1883\n","  Batch 17800/76266, Loss: 0.1679\n","  Batch 17900/76266, Loss: 0.1404\n","  Batch 18000/76266, Loss: 0.2073\n","  Batch 18100/76266, Loss: 0.2329\n","  Batch 18200/76266, Loss: 0.2477\n","  Batch 18300/76266, Loss: 0.0795\n","  Batch 18400/76266, Loss: 0.2354\n","  Batch 18500/76266, Loss: 0.1566\n","  Batch 18600/76266, Loss: 0.1700\n","  Batch 18700/76266, Loss: 0.1390\n","  Batch 18800/76266, Loss: 0.2924\n","  Batch 18900/76266, Loss: 0.1884\n","  Batch 19000/76266, Loss: 0.2771\n","  Batch 19100/76266, Loss: 0.1450\n","  Batch 19200/76266, Loss: 0.1902\n","  Batch 19300/76266, Loss: 0.1708\n","  Batch 19400/76266, Loss: 0.1145\n","  Batch 19500/76266, Loss: 0.1395\n","  Batch 19600/76266, Loss: 0.1215\n","  Batch 19700/76266, Loss: 0.1681\n","  Batch 19800/76266, Loss: 0.1331\n","  Batch 19900/76266, Loss: 0.1430\n","  Batch 20000/76266, Loss: 0.1714\n","  Batch 20100/76266, Loss: 0.2711\n","  Batch 20200/76266, Loss: 0.1643\n","  Batch 20300/76266, Loss: 0.1828\n","  Batch 20400/76266, Loss: 0.1796\n","  Batch 20500/76266, Loss: 0.1485\n","  Batch 20600/76266, Loss: 0.2072\n","  Batch 20700/76266, Loss: 0.1629\n","  Batch 20800/76266, Loss: 0.1367\n","  Batch 20900/76266, Loss: 0.1467\n","  Batch 21000/76266, Loss: 0.1493\n","  Batch 21100/76266, Loss: 0.1354\n","  Batch 21200/76266, Loss: 0.2494\n","  Batch 21300/76266, Loss: 0.2266\n","  Batch 21400/76266, Loss: 0.1950\n","  Batch 21500/76266, Loss: 0.2310\n","  Batch 21600/76266, Loss: 0.1841\n","  Batch 21700/76266, Loss: 0.3002\n","  Batch 21800/76266, Loss: 0.1478\n","  Batch 21900/76266, Loss: 0.1552\n","  Batch 22000/76266, Loss: 0.1516\n","  Batch 22100/76266, Loss: 0.2016\n","  Batch 22200/76266, Loss: 0.1427\n","  Batch 22300/76266, Loss: 0.2902\n","  Batch 22400/76266, Loss: 0.0887\n","  Batch 22500/76266, Loss: 0.2228\n","  Batch 22600/76266, Loss: 0.1765\n","  Batch 22700/76266, Loss: 0.1939\n","  Batch 22800/76266, Loss: 0.1075\n","  Batch 22900/76266, Loss: 0.2396\n","  Batch 23000/76266, Loss: 0.1144\n","  Batch 23100/76266, Loss: 0.2000\n","  Batch 23200/76266, Loss: 0.1142\n","  Batch 23300/76266, Loss: 0.1235\n","  Batch 23400/76266, Loss: 0.2136\n","  Batch 23500/76266, Loss: 0.1726\n","  Batch 23600/76266, Loss: 0.1024\n","  Batch 23700/76266, Loss: 0.1384\n","  Batch 23800/76266, Loss: 0.1493\n","  Batch 23900/76266, Loss: 0.1560\n","  Batch 24000/76266, Loss: 0.1564\n","  Batch 24100/76266, Loss: 0.1001\n","  Batch 24200/76266, Loss: 0.1590\n","  Batch 24300/76266, Loss: 0.2751\n","  Batch 24400/76266, Loss: 0.2335\n","  Batch 24500/76266, Loss: 0.1656\n","  Batch 24600/76266, Loss: 0.1594\n","  Batch 24700/76266, Loss: 0.0690\n","  Batch 24800/76266, Loss: 0.1554\n","  Batch 24900/76266, Loss: 0.1694\n","  Batch 25000/76266, Loss: 0.1994\n","  Batch 25100/76266, Loss: 0.1418\n","  Batch 25200/76266, Loss: 0.3240\n","  Batch 25300/76266, Loss: 0.2336\n","  Batch 25400/76266, Loss: 0.3899\n","  Batch 25500/76266, Loss: 0.2423\n","  Batch 25600/76266, Loss: 0.1184\n","  Batch 25700/76266, Loss: 0.1858\n","  Batch 25800/76266, Loss: 0.1394\n","  Batch 25900/76266, Loss: 0.2004\n","  Batch 26000/76266, Loss: 0.1143\n","  Batch 26100/76266, Loss: 0.1479\n","  Batch 26200/76266, Loss: 0.2519\n","  Batch 26300/76266, Loss: 0.2261\n","  Batch 26400/76266, Loss: 0.2557\n","  Batch 26500/76266, Loss: 0.2641\n","  Batch 26600/76266, Loss: 0.1665\n","  Batch 26700/76266, Loss: 0.1873\n","  Batch 26800/76266, Loss: 0.1179\n","  Batch 26900/76266, Loss: 0.1460\n","  Batch 27000/76266, Loss: 0.1597\n","  Batch 27100/76266, Loss: 0.2074\n","  Batch 27200/76266, Loss: 0.2431\n","  Batch 27300/76266, Loss: 0.1395\n","  Batch 27400/76266, Loss: 0.2365\n","  Batch 27500/76266, Loss: 0.1801\n","  Batch 27600/76266, Loss: 0.1911\n","  Batch 27700/76266, Loss: 0.1964\n","  Batch 27800/76266, Loss: 0.1699\n","  Batch 27900/76266, Loss: 0.1303\n","  Batch 28000/76266, Loss: 0.1639\n","  Batch 28100/76266, Loss: 0.1943\n","  Batch 28200/76266, Loss: 0.1683\n","  Batch 28300/76266, Loss: 0.2270\n","  Batch 28400/76266, Loss: 0.2202\n","  Batch 28500/76266, Loss: 0.1605\n","  Batch 28600/76266, Loss: 0.2770\n","  Batch 28700/76266, Loss: 0.1436\n","  Batch 28800/76266, Loss: 0.1650\n","  Batch 28900/76266, Loss: 0.1715\n","  Batch 29000/76266, Loss: 0.1743\n","  Batch 29100/76266, Loss: 0.1267\n","  Batch 29200/76266, Loss: 0.1041\n","  Batch 29300/76266, Loss: 0.1363\n","  Batch 29400/76266, Loss: 0.1655\n","  Batch 29500/76266, Loss: 0.2527\n","  Batch 29600/76266, Loss: 0.1861\n","  Batch 29700/76266, Loss: 0.2889\n","  Batch 29800/76266, Loss: 0.1403\n","  Batch 29900/76266, Loss: 0.2383\n","  Batch 30000/76266, Loss: 0.2316\n","  Batch 30100/76266, Loss: 0.2950\n","  Batch 30200/76266, Loss: 0.1600\n","  Batch 30300/76266, Loss: 0.1600\n","  Batch 30400/76266, Loss: 0.2163\n","  Batch 30500/76266, Loss: 0.1205\n","  Batch 30600/76266, Loss: 0.1163\n","  Batch 30700/76266, Loss: 0.1848\n","  Batch 30800/76266, Loss: 0.2466\n","  Batch 30900/76266, Loss: 0.1692\n","  Batch 31000/76266, Loss: 0.2178\n","  Batch 31100/76266, Loss: 0.2126\n","  Batch 31200/76266, Loss: 0.2211\n","  Batch 31300/76266, Loss: 0.2566\n","  Batch 31400/76266, Loss: 0.1487\n","  Batch 31500/76266, Loss: 0.1353\n","  Batch 31600/76266, Loss: 0.1236\n","  Batch 31700/76266, Loss: 0.1355\n","  Batch 31800/76266, Loss: 0.1952\n","  Batch 31900/76266, Loss: 0.0917\n","  Batch 32000/76266, Loss: 0.1447\n","  Batch 32100/76266, Loss: 0.1361\n","  Batch 32200/76266, Loss: 0.0964\n","  Batch 32300/76266, Loss: 0.1499\n","  Batch 32400/76266, Loss: 0.1890\n","  Batch 32500/76266, Loss: 0.1284\n","  Batch 32600/76266, Loss: 0.1750\n","  Batch 32700/76266, Loss: 0.1470\n","  Batch 32800/76266, Loss: 0.1303\n","  Batch 32900/76266, Loss: 0.2102\n","  Batch 33000/76266, Loss: 0.2011\n","  Batch 33100/76266, Loss: 0.1740\n","  Batch 33200/76266, Loss: 0.1211\n","  Batch 33300/76266, Loss: 0.1981\n","  Batch 33400/76266, Loss: 0.1180\n","  Batch 33500/76266, Loss: 0.1861\n","  Batch 33600/76266, Loss: 0.2186\n","  Batch 33700/76266, Loss: 0.1421\n","  Batch 33800/76266, Loss: 0.2474\n","  Batch 33900/76266, Loss: 0.1757\n","  Batch 34000/76266, Loss: 0.2308\n","  Batch 34100/76266, Loss: 0.2029\n","  Batch 34200/76266, Loss: 0.2543\n","  Batch 34300/76266, Loss: 0.1993\n","  Batch 34400/76266, Loss: 0.1954\n","  Batch 34500/76266, Loss: 0.1425\n","  Batch 34600/76266, Loss: 0.2173\n","  Batch 34700/76266, Loss: 0.0877\n","  Batch 34800/76266, Loss: 0.1930\n","  Batch 34900/76266, Loss: 0.3169\n","  Batch 35000/76266, Loss: 0.1263\n","  Batch 35100/76266, Loss: 0.1919\n","  Batch 35200/76266, Loss: 0.1659\n","  Batch 35300/76266, Loss: 0.2136\n","  Batch 35400/76266, Loss: 0.1990\n","  Batch 35500/76266, Loss: 0.2053\n","  Batch 35600/76266, Loss: 0.2166\n","  Batch 35700/76266, Loss: 0.1207\n","  Batch 35800/76266, Loss: 0.1650\n","  Batch 35900/76266, Loss: 0.1164\n","  Batch 36000/76266, Loss: 0.1483\n","  Batch 36100/76266, Loss: 0.1688\n","  Batch 36200/76266, Loss: 0.1213\n","  Batch 36300/76266, Loss: 0.1765\n","  Batch 36400/76266, Loss: 0.1696\n","  Batch 36500/76266, Loss: 0.2185\n","  Batch 36600/76266, Loss: 0.2054\n","  Batch 36700/76266, Loss: 0.0982\n","  Batch 36800/76266, Loss: 0.2187\n","  Batch 36900/76266, Loss: 0.2417\n","  Batch 37000/76266, Loss: 0.1518\n","  Batch 37100/76266, Loss: 0.1491\n","  Batch 37200/76266, Loss: 0.1025\n","  Batch 37300/76266, Loss: 0.1788\n","  Batch 37400/76266, Loss: 0.1377\n","  Batch 37500/76266, Loss: 0.1143\n","  Batch 37600/76266, Loss: 0.1330\n","  Batch 37700/76266, Loss: 0.1795\n","  Batch 37800/76266, Loss: 0.1962\n","  Batch 37900/76266, Loss: 0.1230\n","  Batch 38000/76266, Loss: 0.1981\n","  Batch 38100/76266, Loss: 0.1498\n","  Batch 38200/76266, Loss: 0.1346\n","  Batch 38300/76266, Loss: 0.2046\n","  Batch 38400/76266, Loss: 0.2033\n","  Batch 38500/76266, Loss: 0.1635\n","  Batch 38600/76266, Loss: 0.2038\n","  Batch 38700/76266, Loss: 0.1693\n","  Batch 38800/76266, Loss: 0.1369\n","  Batch 38900/76266, Loss: 0.1648\n","  Batch 39000/76266, Loss: 0.2025\n","  Batch 39100/76266, Loss: 0.1439\n","  Batch 39200/76266, Loss: 0.2570\n","  Batch 39300/76266, Loss: 0.1755\n","  Batch 39400/76266, Loss: 0.3515\n","  Batch 39500/76266, Loss: 0.1906\n","  Batch 39600/76266, Loss: 0.1984\n","  Batch 39700/76266, Loss: 0.1755\n","  Batch 39800/76266, Loss: 0.1732\n","  Batch 39900/76266, Loss: 0.2419\n","  Batch 40000/76266, Loss: 0.2069\n","  Batch 40100/76266, Loss: 0.1721\n","  Batch 40200/76266, Loss: 0.1288\n","  Batch 40300/76266, Loss: 0.1867\n","  Batch 40400/76266, Loss: 0.1269\n","  Batch 40500/76266, Loss: 0.1695\n","  Batch 40600/76266, Loss: 0.1934\n","  Batch 40700/76266, Loss: 0.0797\n","  Batch 40800/76266, Loss: 0.1942\n","  Batch 40900/76266, Loss: 0.0908\n","  Batch 41000/76266, Loss: 0.1634\n","  Batch 41100/76266, Loss: 0.0987\n","  Batch 41200/76266, Loss: 0.2477\n","  Batch 41300/76266, Loss: 0.1732\n","  Batch 41400/76266, Loss: 0.1650\n","  Batch 41500/76266, Loss: 0.1687\n","  Batch 41600/76266, Loss: 0.2566\n","  Batch 41700/76266, Loss: 0.2025\n","  Batch 41800/76266, Loss: 0.3054\n","  Batch 41900/76266, Loss: 0.1240\n","  Batch 42000/76266, Loss: 0.1721\n","  Batch 42100/76266, Loss: 0.2391\n","  Batch 42200/76266, Loss: 0.2507\n","  Batch 42300/76266, Loss: 0.1906\n","  Batch 42400/76266, Loss: 0.2789\n","  Batch 42500/76266, Loss: 0.2084\n","  Batch 42600/76266, Loss: 0.1830\n","  Batch 42700/76266, Loss: 0.1617\n","  Batch 42800/76266, Loss: 0.1753\n","  Batch 42900/76266, Loss: 0.1376\n","  Batch 43000/76266, Loss: 0.1579\n","  Batch 43100/76266, Loss: 0.1763\n","  Batch 43200/76266, Loss: 0.2176\n","  Batch 43300/76266, Loss: 0.2086\n","  Batch 43400/76266, Loss: 0.2219\n","  Batch 43500/76266, Loss: 0.2248\n","  Batch 43600/76266, Loss: 0.1598\n","  Batch 43700/76266, Loss: 0.2190\n","  Batch 43800/76266, Loss: 0.1564\n","  Batch 43900/76266, Loss: 0.1107\n","  Batch 44000/76266, Loss: 0.0891\n","  Batch 44100/76266, Loss: 0.1491\n","  Batch 44200/76266, Loss: 0.1798\n","  Batch 44300/76266, Loss: 0.2464\n","  Batch 44400/76266, Loss: 0.1294\n","  Batch 44500/76266, Loss: 0.1797\n","  Batch 44600/76266, Loss: 0.1878\n","  Batch 44700/76266, Loss: 0.0936\n","  Batch 44800/76266, Loss: 0.2262\n","  Batch 44900/76266, Loss: 0.2106\n","  Batch 45000/76266, Loss: 0.1208\n","  Batch 45100/76266, Loss: 0.1266\n","  Batch 45200/76266, Loss: 0.1098\n","  Batch 45300/76266, Loss: 0.3071\n","  Batch 45400/76266, Loss: 0.2596\n","  Batch 45500/76266, Loss: 0.0988\n","  Batch 45600/76266, Loss: 0.2109\n","  Batch 45700/76266, Loss: 0.2610\n","  Batch 45800/76266, Loss: 0.0938\n","  Batch 45900/76266, Loss: 0.2271\n","  Batch 46000/76266, Loss: 0.2525\n","  Batch 46100/76266, Loss: 0.1830\n","  Batch 46200/76266, Loss: 0.1974\n","  Batch 46300/76266, Loss: 0.1135\n","  Batch 46400/76266, Loss: 0.3177\n","  Batch 46500/76266, Loss: 0.2655\n","  Batch 46600/76266, Loss: 0.2048\n","  Batch 46700/76266, Loss: 0.1050\n","  Batch 46800/76266, Loss: 0.1049\n","  Batch 46900/76266, Loss: 0.2550\n","  Batch 47000/76266, Loss: 0.2629\n","  Batch 47100/76266, Loss: 0.2220\n","  Batch 47200/76266, Loss: 0.2096\n","  Batch 47300/76266, Loss: 0.2435\n","  Batch 47400/76266, Loss: 0.1290\n","  Batch 47500/76266, Loss: 0.1859\n","  Batch 47600/76266, Loss: 0.1672\n","  Batch 47700/76266, Loss: 0.2010\n","  Batch 47800/76266, Loss: 0.0908\n","  Batch 47900/76266, Loss: 0.1307\n","  Batch 48000/76266, Loss: 0.1192\n","  Batch 48100/76266, Loss: 0.0547\n","  Batch 48200/76266, Loss: 0.2450\n","  Batch 48300/76266, Loss: 0.2296\n","  Batch 48400/76266, Loss: 0.1845\n","  Batch 48500/76266, Loss: 0.1757\n","  Batch 48600/76266, Loss: 0.1624\n","  Batch 48700/76266, Loss: 0.2234\n","  Batch 48800/76266, Loss: 0.2576\n","  Batch 48900/76266, Loss: 0.2340\n","  Batch 49000/76266, Loss: 0.1976\n","  Batch 49100/76266, Loss: 0.1866\n","  Batch 49200/76266, Loss: 0.1524\n","  Batch 49300/76266, Loss: 0.1592\n","  Batch 49400/76266, Loss: 0.0386\n","  Batch 49500/76266, Loss: 0.1918\n","  Batch 49600/76266, Loss: 0.1464\n","  Batch 49700/76266, Loss: 0.1852\n","  Batch 49800/76266, Loss: 0.1855\n","  Batch 49900/76266, Loss: 0.1708\n","  Batch 50000/76266, Loss: 0.1091\n","  Batch 50100/76266, Loss: 0.2107\n","  Batch 50200/76266, Loss: 0.1630\n","  Batch 50300/76266, Loss: 0.0998\n","  Batch 50400/76266, Loss: 0.2263\n","  Batch 50500/76266, Loss: 0.1195\n","  Batch 50600/76266, Loss: 0.1642\n","  Batch 50700/76266, Loss: 0.1264\n","  Batch 50800/76266, Loss: 0.2163\n","  Batch 50900/76266, Loss: 0.2478\n","  Batch 51000/76266, Loss: 0.1749\n","  Batch 51100/76266, Loss: 0.1487\n","  Batch 51200/76266, Loss: 0.1491\n","  Batch 51300/76266, Loss: 0.1690\n","  Batch 51400/76266, Loss: 0.1742\n","  Batch 51500/76266, Loss: 0.1755\n","  Batch 51600/76266, Loss: 0.2299\n","  Batch 51700/76266, Loss: 0.1567\n","  Batch 51800/76266, Loss: 0.1942\n","  Batch 51900/76266, Loss: 0.1198\n","  Batch 52000/76266, Loss: 0.2700\n","  Batch 52100/76266, Loss: 0.1869\n","  Batch 52200/76266, Loss: 0.1985\n","  Batch 52300/76266, Loss: 0.1785\n","  Batch 52400/76266, Loss: 0.1551\n","  Batch 52500/76266, Loss: 0.1803\n","  Batch 52600/76266, Loss: 0.1407\n","  Batch 52700/76266, Loss: 0.2934\n","  Batch 52800/76266, Loss: 0.1294\n","  Batch 52900/76266, Loss: 0.0826\n","  Batch 53000/76266, Loss: 0.2353\n","  Batch 53100/76266, Loss: 0.0595\n","  Batch 53200/76266, Loss: 0.1442\n","  Batch 53300/76266, Loss: 0.2424\n","  Batch 53400/76266, Loss: 0.2281\n","  Batch 53500/76266, Loss: 0.1577\n","  Batch 53600/76266, Loss: 0.1463\n","  Batch 53700/76266, Loss: 0.1866\n","  Batch 53800/76266, Loss: 0.1295\n","  Batch 53900/76266, Loss: 0.1661\n","  Batch 54000/76266, Loss: 0.1532\n","  Batch 54100/76266, Loss: 0.1524\n","  Batch 54200/76266, Loss: 0.1358\n","  Batch 54300/76266, Loss: 0.2067\n","  Batch 54400/76266, Loss: 0.1986\n","  Batch 54500/76266, Loss: 0.2594\n","  Batch 54600/76266, Loss: 0.2039\n","  Batch 54700/76266, Loss: 0.2112\n","  Batch 54800/76266, Loss: 0.2120\n","  Batch 54900/76266, Loss: 0.2314\n","  Batch 55000/76266, Loss: 0.2318\n","  Batch 55100/76266, Loss: 0.3429\n","  Batch 55200/76266, Loss: 0.1569\n","  Batch 55300/76266, Loss: 0.2572\n","  Batch 55400/76266, Loss: 0.2693\n","  Batch 55500/76266, Loss: 0.3405\n","  Batch 55600/76266, Loss: 0.1989\n","  Batch 55700/76266, Loss: 0.2576\n","  Batch 55800/76266, Loss: 0.2137\n","  Batch 55900/76266, Loss: 0.2578\n","  Batch 56000/76266, Loss: 0.1531\n","  Batch 56100/76266, Loss: 0.0867\n","  Batch 56200/76266, Loss: 0.1454\n","  Batch 56300/76266, Loss: 0.1949\n","  Batch 56400/76266, Loss: 0.1895\n","  Batch 56500/76266, Loss: 0.1686\n","  Batch 56600/76266, Loss: 0.1690\n","  Batch 56700/76266, Loss: 0.1353\n","  Batch 56800/76266, Loss: 0.1110\n","  Batch 56900/76266, Loss: 0.1419\n","  Batch 57000/76266, Loss: 0.2889\n","  Batch 57100/76266, Loss: 0.1163\n","  Batch 57200/76266, Loss: 0.1881\n","  Batch 57300/76266, Loss: 0.2382\n","  Batch 57400/76266, Loss: 0.2920\n","  Batch 57500/76266, Loss: 0.1641\n","  Batch 57600/76266, Loss: 0.2417\n","  Batch 57700/76266, Loss: 0.1422\n","  Batch 57800/76266, Loss: 0.1403\n","  Batch 57900/76266, Loss: 0.1785\n","  Batch 58000/76266, Loss: 0.1920\n","  Batch 58100/76266, Loss: 0.0901\n","  Batch 58200/76266, Loss: 0.2029\n","  Batch 58300/76266, Loss: 0.1257\n","  Batch 58400/76266, Loss: 0.2484\n","  Batch 58500/76266, Loss: 0.1791\n","  Batch 58600/76266, Loss: 0.2057\n","  Batch 58700/76266, Loss: 0.3234\n","  Batch 58800/76266, Loss: 0.1400\n","  Batch 58900/76266, Loss: 0.1229\n","  Batch 59000/76266, Loss: 0.1725\n","  Batch 59100/76266, Loss: 0.2344\n","  Batch 59200/76266, Loss: 0.1981\n","  Batch 59300/76266, Loss: 0.2712\n","  Batch 59400/76266, Loss: 0.2611\n","  Batch 59500/76266, Loss: 0.1922\n","  Batch 59600/76266, Loss: 0.1433\n","  Batch 59700/76266, Loss: 0.1363\n","  Batch 59800/76266, Loss: 0.1079\n","  Batch 59900/76266, Loss: 0.1667\n","  Batch 60000/76266, Loss: 0.1314\n","  Batch 60100/76266, Loss: 0.1151\n","  Batch 60200/76266, Loss: 0.1654\n","  Batch 60300/76266, Loss: 0.2288\n","  Batch 60400/76266, Loss: 0.1977\n","  Batch 60500/76266, Loss: 0.2026\n","  Batch 60600/76266, Loss: 0.1597\n","  Batch 60700/76266, Loss: 0.1973\n","  Batch 60800/76266, Loss: 0.2161\n","  Batch 60900/76266, Loss: 0.1985\n","  Batch 61000/76266, Loss: 0.1962\n","  Batch 61100/76266, Loss: 0.1911\n","  Batch 61200/76266, Loss: 0.1455\n","  Batch 61300/76266, Loss: 0.1559\n","  Batch 61400/76266, Loss: 0.1704\n","  Batch 61500/76266, Loss: 0.1050\n","  Batch 61600/76266, Loss: 0.1968\n","  Batch 61700/76266, Loss: 0.1184\n","  Batch 61800/76266, Loss: 0.1725\n","  Batch 61900/76266, Loss: 0.2429\n","  Batch 62000/76266, Loss: 0.1887\n","  Batch 62100/76266, Loss: 0.1769\n","  Batch 62200/76266, Loss: 0.1583\n","  Batch 62300/76266, Loss: 0.2133\n","  Batch 62400/76266, Loss: 0.2103\n","  Batch 62500/76266, Loss: 0.1944\n","  Batch 62600/76266, Loss: 0.1588\n","  Batch 62700/76266, Loss: 0.2187\n","  Batch 62800/76266, Loss: 0.1659\n","  Batch 62900/76266, Loss: 0.1499\n","  Batch 63000/76266, Loss: 0.2123\n","  Batch 63100/76266, Loss: 0.1439\n","  Batch 63200/76266, Loss: 0.1400\n","  Batch 63300/76266, Loss: 0.1301\n","  Batch 63400/76266, Loss: 0.2148\n","  Batch 63500/76266, Loss: 0.1561\n","  Batch 63600/76266, Loss: 0.1660\n","  Batch 63700/76266, Loss: 0.2050\n","  Batch 63800/76266, Loss: 0.1905\n","  Batch 63900/76266, Loss: 0.1574\n","  Batch 64000/76266, Loss: 0.1262\n","  Batch 64100/76266, Loss: 0.2140\n","  Batch 64200/76266, Loss: 0.1325\n","  Batch 64300/76266, Loss: 0.1259\n","  Batch 64400/76266, Loss: 0.2574\n","  Batch 64500/76266, Loss: 0.1775\n","  Batch 64600/76266, Loss: 0.1369\n","  Batch 64700/76266, Loss: 0.2303\n","  Batch 64800/76266, Loss: 0.2511\n","  Batch 64900/76266, Loss: 0.2576\n","  Batch 65000/76266, Loss: 0.1562\n","  Batch 65100/76266, Loss: 0.2335\n","  Batch 65200/76266, Loss: 0.1713\n","  Batch 65300/76266, Loss: 0.2396\n","  Batch 65400/76266, Loss: 0.1943\n","  Batch 65500/76266, Loss: 0.1388\n","  Batch 65600/76266, Loss: 0.1457\n","  Batch 65700/76266, Loss: 0.2600\n","  Batch 65800/76266, Loss: 0.2374\n","  Batch 65900/76266, Loss: 0.1909\n","  Batch 66000/76266, Loss: 0.1273\n","  Batch 66100/76266, Loss: 0.1083\n","  Batch 66200/76266, Loss: 0.1783\n","  Batch 66300/76266, Loss: 0.1258\n","  Batch 66400/76266, Loss: 0.1221\n","  Batch 66500/76266, Loss: 0.1081\n","  Batch 66600/76266, Loss: 0.2219\n","  Batch 66700/76266, Loss: 0.2143\n","  Batch 66800/76266, Loss: 0.1458\n","  Batch 66900/76266, Loss: 0.2370\n","  Batch 67000/76266, Loss: 0.1763\n","  Batch 67100/76266, Loss: 0.3211\n","  Batch 67200/76266, Loss: 0.1005\n","  Batch 67300/76266, Loss: 0.1318\n","  Batch 67400/76266, Loss: 0.1727\n","  Batch 67500/76266, Loss: 0.0858\n","  Batch 67600/76266, Loss: 0.1719\n","  Batch 67700/76266, Loss: 0.1761\n","  Batch 67800/76266, Loss: 0.0963\n","  Batch 67900/76266, Loss: 0.2359\n","  Batch 68000/76266, Loss: 0.2676\n","  Batch 68100/76266, Loss: 0.1753\n","  Batch 68200/76266, Loss: 0.2483\n","  Batch 68300/76266, Loss: 0.2782\n","  Batch 68400/76266, Loss: 0.1551\n","  Batch 68500/76266, Loss: 0.1431\n","  Batch 68600/76266, Loss: 0.2635\n","  Batch 68700/76266, Loss: 0.2619\n","  Batch 68800/76266, Loss: 0.1467\n","  Batch 68900/76266, Loss: 0.1698\n","  Batch 69000/76266, Loss: 0.1586\n","  Batch 69100/76266, Loss: 0.1671\n","  Batch 69200/76266, Loss: 0.2694\n","  Batch 69300/76266, Loss: 0.1666\n","  Batch 69400/76266, Loss: 0.0849\n","  Batch 69500/76266, Loss: 0.0905\n","  Batch 69600/76266, Loss: 0.1686\n","  Batch 69700/76266, Loss: 0.2019\n","  Batch 69800/76266, Loss: 0.1346\n","  Batch 69900/76266, Loss: 0.1784\n","  Batch 70000/76266, Loss: 0.2171\n","  Batch 70100/76266, Loss: 0.1999\n","  Batch 70200/76266, Loss: 0.1198\n","  Batch 70300/76266, Loss: 0.2445\n","  Batch 70400/76266, Loss: 0.1354\n","  Batch 70500/76266, Loss: 0.1999\n","  Batch 70600/76266, Loss: 0.2178\n","  Batch 70700/76266, Loss: 0.1480\n","  Batch 70800/76266, Loss: 0.2318\n","  Batch 70900/76266, Loss: 0.1666\n","  Batch 71000/76266, Loss: 0.2108\n","  Batch 71100/76266, Loss: 0.2624\n","  Batch 71200/76266, Loss: 0.2166\n","  Batch 71300/76266, Loss: 0.1687\n","  Batch 71400/76266, Loss: 0.2632\n","  Batch 71500/76266, Loss: 0.1660\n","  Batch 71600/76266, Loss: 0.2125\n","  Batch 71700/76266, Loss: 0.1218\n","  Batch 71800/76266, Loss: 0.1678\n","  Batch 71900/76266, Loss: 0.2423\n","  Batch 72000/76266, Loss: 0.1550\n","  Batch 72100/76266, Loss: 0.1755\n","  Batch 72200/76266, Loss: 0.1578\n","  Batch 72300/76266, Loss: 0.0822\n","  Batch 72400/76266, Loss: 0.3154\n","  Batch 72500/76266, Loss: 0.1848\n","  Batch 72600/76266, Loss: 0.1799\n","  Batch 72700/76266, Loss: 0.1095\n","  Batch 72800/76266, Loss: 0.1803\n","  Batch 72900/76266, Loss: 0.1592\n","  Batch 73000/76266, Loss: 0.1501\n","  Batch 73100/76266, Loss: 0.0970\n","  Batch 73200/76266, Loss: 0.1920\n","  Batch 73300/76266, Loss: 0.1435\n","  Batch 73400/76266, Loss: 0.2002\n","  Batch 73500/76266, Loss: 0.1031\n","  Batch 73600/76266, Loss: 0.1995\n","  Batch 73700/76266, Loss: 0.1604\n","  Batch 73800/76266, Loss: 0.1039\n","  Batch 73900/76266, Loss: 0.2055\n","  Batch 74000/76266, Loss: 0.2389\n","  Batch 74100/76266, Loss: 0.1161\n","  Batch 74200/76266, Loss: 0.1586\n","  Batch 74300/76266, Loss: 0.1167\n","  Batch 74400/76266, Loss: 0.2678\n","  Batch 74500/76266, Loss: 0.2579\n","  Batch 74600/76266, Loss: 0.1669\n","  Batch 74700/76266, Loss: 0.1966\n","  Batch 74800/76266, Loss: 0.1052\n","  Batch 74900/76266, Loss: 0.1706\n","  Batch 75000/76266, Loss: 0.1281\n","  Batch 75100/76266, Loss: 0.1134\n","  Batch 75200/76266, Loss: 0.1010\n","  Batch 75300/76266, Loss: 0.1767\n","  Batch 75400/76266, Loss: 0.1817\n","  Batch 75500/76266, Loss: 0.2451\n","  Batch 75600/76266, Loss: 0.1347\n","  Batch 75700/76266, Loss: 0.1413\n","  Batch 75800/76266, Loss: 0.1703\n","  Batch 75900/76266, Loss: 0.2593\n","  Batch 76000/76266, Loss: 0.1349\n","  Batch 76100/76266, Loss: 0.1150\n","  Batch 76200/76266, Loss: 0.1547\n","Epoch 3 completed in 11576.09s. Average Training Loss: 0.1810\n","Transformer training finished.\n"]}],"source":["# --- Import necessary libraries ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW  # Correct import for AdamW\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    get_linear_schedule_with_warmup,\n","    DistilBertTokenizer,  # Example using DistilBERT\n","    DistilBertForSequenceClassification\n",")\n","import time\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","# Initialize SentimentIntensityAnalyzer\n","analyzer = SentimentIntensityAnalyzer()\n","\n","# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n","\n","# 1. Load or calculate sentiment scores if not in df_processed\n","try:\n","    # If sentiment scores are already in df_processed, use them\n","    sentiment_columns = ['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']\n","    Y = df_processed[sentiment_columns].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","except KeyError:\n","    print(\"Sentiment score columns not found in df_processed. Calculating or loading them...\")\n","    # Option 2: Load from a separate file (if you saved them earlier)\n","    from google.colab import drive\n","    drive.mount('/content/drive')  # Mount Google Drive\n","    sentiment_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n","\n","    sentiment_df = pd.read_csv(sentiment_file_path)\n","    # Merge sentiment scores with the original dataframe based on a common column (e.g., 'review_id')\n","    df_processed = pd.merge(\n","        df_processed,\n","        sentiment_df[['review_id', 'food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']],\n","        on='review_id',\n","        how='left'\n","    )\n","    Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']].copy()\n","\n","# Calculate overall sentiment for each review\n","df_processed['overall_sentiment'] = df_processed['text'].apply(get_overall_sentiment)\n","\n","# Now include 'overall_sentiment' in your target columns\n","Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']].copy()\n","\n","# --- 2. Correct sentiment binning using fixed thresholds ---\n","\n","# Define fixed VADER thresholds and corresponding integer labels\n","# Bins: (-inf, -0.05], (-0.05, 0.05), [0.05, +inf)\n","# We add +/- infinity to ensure all values are covered.\n","bin_edges = [-float('inf'), -0.05, 0.05, float('inf')]\n","# Assign integer labels: 0 for Negative, 1 for Neutral, 2 for Positive\n","bin_labels = [0, 1, 2]\n","label_names = ['negative', 'neutral', 'positive'] # For potential use in reports if needed\n","\n","print(f\"Using fixed bin edges: {bin_edges}\")\n","print(f\"Assigning integer labels: {bin_labels} ({', '.join(label_names)})\")\n","\n","# Ensure Y is a DataFrame (it should be from previous steps)\n","if not isinstance(Y, pd.DataFrame):\n","     raise TypeError(\"Y should be a pandas DataFrame at this stage.\")\n","\n","# Create a new DataFrame for binned labels to avoid modifying Y inplace initially\n","Y_binned = pd.DataFrame(index=Y.index)\n","\n","for column in Y.columns:\n","    # Cast column values to float64 explicitly to avoid dtype issues\n","    # Use .loc to avoid SettingWithCopyWarning if Y is a slice\n","    Y_column_float = Y.loc[:, column].astype(float)\n","\n","    # Use pd.cut for binning with fixed edges\n","    # include_lowest=True: includes the lowest value (-inf) in the first bin\n","    # right=True (default): bins are (edge1, edge2], except first which is [edge1, edge2] due to include_lowest\n","    # If you want bins like [edge1, edge2), use right=False\n","    binned_data = pd.cut(\n","        Y_column_float,\n","        bins=bin_edges,\n","        labels=bin_labels,\n","        include_lowest=True,\n","        right=True # Standard VADER thresholds often use >= 0.05 for positive, <= -0.05 for negative\n","    )\n","\n","    # Check for NaNs introduced by binning (shouldn't happen with inf edges, but good practice)\n","    if binned_data.isnull().any():\n","        print(f\"Warning: NaNs found in '{column}' after binning. Check original data.\")\n","        # Handle NaNs if necessary, e.g., fill with neutral=1 or drop rows\n","        # binned_data = binned_data.fillna(1) # Example: Fill NaN with Neutral\n","\n","    # Assign the binned data (as integers) to the new DataFrame\n","    Y_binned[column] = binned_data.astype(int)\n","\n","# Replace the original Y with the binned version\n","Y = Y_binned\n","print(\"\\nSentiment scores binned using fixed thresholds:\")\n","print(Y.head())\n","print(\"\\nValue counts for 'food_sentiment' (example):\")\n","print(Y['food_sentiment'].value_counts())\n","\n","# --- End of binning modification ---\n","\n","# 3. Prepare data for training and testing\n","# Assuming 'processed_text' column contains preprocessed text data\n","X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n","\n","# Get the first sentiment column for stratification\n","stratify_column = Y.iloc[:, 0]  # Assuming Y is a DataFrame\n","\n","# Split data into training and testing sets\n","X_train, X_test, Y_train, Y_test = train_test_split(\n","    X, Y, test_size=0.2, random_state=42, stratify=stratify_column\n",")\n","\n","# Print dataset sizes\n","print(f\"Training set size: {len(X_train)}\")\n","print(f\"Testing set size: {len(X_test)}\")\n","\n","\n","# --- Tokenization ---\n","# Initialize tokenizer (if not done earlier)\n","model_name = 'distilbert-base-uncased'  # Or your preferred model name\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","\n","# Tokenize and encode the text data\n","train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n","test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n","\n","# --- Convert Y_train and Y_test to NumPy arrays ---\n","Y_train_np = Y_train.values  # Assuming Y_train is a pandas DataFrame\n","Y_test_np = Y_test.values   # Assuming Y_test is a pandas DataFrame\n","\n","\n","# --- Define PyTorch Dataset ---\n","class YelpAspectDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        # Ensure labels are shaped correctly (N_samples, N_aspects)\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        # Handle cases where encodings might be lists of tensors or similar structures\n","        item = {}\n","        for key, val in self.encodings.items():\n","            # Ensure the value associated with the key is indexable and convert to tensor\n","            if isinstance(val, list):\n","                item[key] = torch.tensor(val[idx])\n","            else:  # Assuming it might already be a tensor or numpy array\n","                item[key] = torch.tensor(val[idx]).clone().detach()  # Make sure it's a tensor\n","\n","        # Labels should be FloatTensor for BCEWithLogitsLoss or LongTensor for CrossEntropyLoss\n","        # For multi-label (predicting probability for each class per aspect) or multi-output (predicting one class per aspect)\n","        # Let's assume multi-output: predicting one class (0, 1, 2) per aspect. Labels are LongTensor.\n","        # Shape should be (num_aspects,) for a single item\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","    def __len__(self):\n","        # Use the number of samples in one of the encoding keys (e.g., 'input_ids')\n","        return len(self.encodings['input_ids'])\n","\n","\n","# Create datasets\n","train_dataset = YelpAspectDataset(train_encodings, Y_train_np)\n","test_dataset = YelpAspectDataset(test_encodings, Y_test_np)\n","\n","# --- Define Model ---\n","# We need a multi-output classification head.\n","# Load pre-trained DistilBERT and modify the classifier layer.\n","aspects = ['food', 'service', 'ambiance', 'price', 'context', 'overall']  # Define your aspects\n","sentiment_classes = ['negative', 'neutral', 'positive']  # Define sentiment classes\n","\n","num_aspects = len(aspects)\n","num_classes_per_aspect = len(sentiment_classes)  # 3 classes: positive, negative, neutral\n","\n","# Load the base model\n","model = DistilBertForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=num_aspects * num_classes_per_aspect  # TEMPORARY - see below\n",")\n","\n","# *** IMPORTANT: Modifying the Classifier Head for Multi-Output ***\n","# The standard `DistilBertForSequenceClassification` has ONE output layer for num_labels.\n","# For multi-output (one prediction per aspect), we need to customize.\n","# Option 1: Treat as N independent classification tasks (simpler to implement with standard model if labels handled outside)\n","# Option 2: Create N classification heads (more complex custom model)\n","# Option 3: Reshape the output of a single large head (requires careful loss calculation)\n","\n","# Let's try Option 3 conceptually: A single head predicting scores for all aspect-class combinations.\n","# Then reshape and calculate loss per aspect.\n","# The output dimension should be num_aspects * num_classes_per_aspect\n","# The final layer needs to be replaced or adapted.\n","\n","# Get the original classifier's input dimension\n","original_classifier_in_features = model.classifier.in_features\n","\n","# Replace the classifier head. Output size is num_aspects * num_classes (e.g., 5 * 3 = 15)\n","model.classifier = torch.nn.Linear(original_classifier_in_features, num_aspects * num_classes_per_aspect)\n","# The pre_classifier layer might also need adjustment if present (DistilBERT has one)\n","if hasattr(model, 'pre_classifier') and model.pre_classifier is not None:\n","    original_pre_classifier_in_features = model.pre_classifier.in_features\n","    model.pre_classifier = torch.nn.Linear(original_pre_classifier_in_features, original_classifier_in_features)  # Keep standard pre-classifier -> classifier connection\n","else:\n","    print(\"Model does not have a separate pre_classifier layer.\")\n","\n","print(f\"\\nCustomized DistilBERT model loaded. Output layer size: {model.classifier.out_features}\")\n","\n","# --- Training Setup ---\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(f\"Using device: {device}\")\n","model.to(device)\n","\n","# Dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Adjust batch_size based on GPU memory\n","test_loader = DataLoader(test_dataset, batch_size=32)  # Larger batch size for evaluation is fine\n","\n","# Optimizer and Scheduler\n","optimizer = AdamW(model.parameters(), lr=5e-5)  # Typical learning rate for fine-tuning\n","num_epochs = 3  # Adjust as needed (start small)\n","num_training_steps = num_epochs * len(train_loader)\n","lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","# Loss Function: CrossEntropyLoss is suitable for multi-class classification (per aspect)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# --- Training Loop ---\n","print(\"\\nStarting Transformer Model Training...\")\n","model.train()\n","for epoch in range(num_epochs):\n","    print(f\"--- Epoch {epoch + 1}/{num_epochs} ---\")\n","    epoch_start_time = time.time()\n","    total_loss = 0\n","    for i, batch in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        # Move batch to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)  # Shape: (batch_size, num_aspects)\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Shape: (batch_size, num_aspects * num_classes_per_aspect)\n","\n","        # Calculate loss: Reshape logits and labels for CrossEntropyLoss\n","        # Logits need to be (batch_size * num_aspects, num_classes_per_aspect)\n","        # Labels need to be (batch_size * num_aspects,)\n","        reshaped_logits = logits.view(-1, num_classes_per_aspect)  # (batch * aspects, classes)\n","        reshaped_labels = labels.view(-1)  # (batch * aspects,)\n","\n","        loss = loss_fn(reshaped_logits, reshaped_labels)\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","\n","        if (i + 1) % 100 == 0:  # Print progress every 100 batches\n","            print(f\"  Batch {i + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","    epoch_time = time.time() - epoch_start_time\n","    print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f}s. Average Training Loss: {avg_train_loss:.4f}\")\n","\n","print(\"Transformer training finished.\")\n","# Consider saving the trained model\n","# model.save_pretrained('./my_aspect_sentiment_model')\n","# tokenizer.save_pretrained('./my_aspect_sentiment_model')"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"n6W-tvrY4dDm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744773809189,"user_tz":300,"elapsed":9307,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"}},"outputId":"c9c1300e-4e0e-4c29-f998-94dab1de8fc4"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Saving the fine-tuned model and tokenizer...\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Model and tokenizer saved to /content/drive/My Drive/Colab Notebooks\n"]}],"source":["# --- Save the Model and Tokenizer ---\n","print(\"\\nSaving the fine-tuned model and tokenizer...\")\n","\n","# Define the directory where you want to save them\n","drive.mount('/content/drive')  # Mount Google Drive\n","save_directory = '/content/drive/My Drive/Colab Notebooks' # You can change this path\n","# Save the model's weights and configuration file\n","model.save_pretrained(save_directory)\n","\n","# Save the tokenizer's vocabulary and configuration file\n","tokenizer.save_pretrained(save_directory)\n","\n","print(f\"Model and tokenizer saved to {save_directory}\")\n","\n","# --- (Optional) How to load the model and tokenizer later ---\n","# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","# loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n","# loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n","# print(\"\\nModel and tokenizer loaded successfully (example).\")\n","# loaded_model.to(device) # Remember to move the loaded model to the correct device"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"xBeOe4SyRD9W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744774732124,"user_tz":300,"elapsed":922935,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"}},"outputId":"6e4198fc-ebc6-414a-dfcc-46091e0674df"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating Transformer Model...\n","\n","--- Transformer Classification Report (Per Aspect) ---\n","\n","--- Aspect: food ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.66      0.63      0.65     24235\n","     neutral       0.85      0.74      0.79     77023\n","    positive       0.90      0.95      0.92    203805\n","\n","    accuracy                           0.87    305063\n","   macro avg       0.80      0.77      0.79    305063\n","weighted avg       0.87      0.87      0.87    305063\n","\n","\n","--- Aspect: service ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.73      0.74      0.74     25208\n","     neutral       0.91      0.89      0.90    187376\n","    positive       0.81      0.85      0.83     92479\n","\n","    accuracy                           0.86    305063\n","   macro avg       0.82      0.83      0.82    305063\n","weighted avg       0.87      0.86      0.86    305063\n","\n","\n","--- Aspect: ambiance ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.62      0.49      0.55      3174\n","     neutral       0.97      0.97      0.97    270947\n","    positive       0.76      0.77      0.77     30942\n","\n","    accuracy                           0.95    305063\n","   macro avg       0.78      0.75      0.76    305063\n","weighted avg       0.94      0.95      0.95    305063\n","\n","\n","--- Aspect: price ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.56      0.34      0.43      3099\n","     neutral       0.97      0.98      0.98    286154\n","    positive       0.67      0.62      0.65     15810\n","\n","    accuracy                           0.96    305063\n","   macro avg       0.74      0.65      0.68    305063\n","weighted avg       0.95      0.96      0.96    305063\n","\n","\n","--- Aspect: context ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.57      0.44      0.50      5564\n","     neutral       0.95      0.95      0.95    253527\n","    positive       0.75      0.76      0.75     45972\n","\n","    accuracy                           0.91    305063\n","   macro avg       0.75      0.72      0.73    305063\n","weighted avg       0.91      0.91      0.91    305063\n","\n","\n","--- Aspect: overall ---\n","              precision    recall  f1-score   support\n","\n","    negative       0.85      0.82      0.84     42897\n","     neutral       0.86      0.46      0.60      4096\n","    positive       0.97      0.98      0.97    258070\n","\n","    accuracy                           0.95    305063\n","   macro avg       0.89      0.75      0.80    305063\n","weighted avg       0.95      0.95      0.95    305063\n","\n","\n","--- Transformer Overall Micro/Macro Averages ---\n","Micro Average: Precision=0.9166, Recall=0.9166, F1-Score=0.9166\n","Macro Average: Precision=0.8634, Recall=0.8555, F1-Score=0.8593\n"]}],"source":["print(\"\\nEvaluating Transformer Model...\")\n","model.eval()  # Set model to evaluation mode\n","\n","all_preds_transformer = []\n","all_labels_transformer = []\n","\n","with torch.no_grad():  # Disable gradient calculations for inference\n","    for batch in test_loader:\n","        # Move batch to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)  # Shape: (batch_size, num_aspects)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Shape: (batch_size, num_aspects * num_classes_per_aspect)\n","\n","        # Get predictions: Reshape logits and find the class with max probability for each aspect\n","        # Reshape logits to (batch_size, num_aspects, num_classes_per_aspect)\n","        reshaped_logits = logits.view(input_ids.size(0), num_aspects, num_classes_per_aspect)\n","        predictions = torch.argmax(reshaped_logits, dim=2)  # Get predicted class index along the class dimension\n","\n","        all_preds_transformer.extend(predictions.cpu().numpy())\n","        all_labels_transformer.extend(labels.cpu().numpy())\n","\n","# Convert collected predictions and labels into numpy arrays\n","Y_pred_transformer_np = np.array(all_preds_transformer)  # Shape: (num_test_samples, num_aspects)\n","Y_true_transformer_np = np.array(all_labels_transformer)  # Shape: (num_test_samples, num_aspects)\n","\n","# Calculate metrics per aspect\n","transformer_report = {}\n","print(\"\\n--- Transformer Classification Report (Per Aspect) ---\")\n","\n","all_true_flat_transformer = []\n","all_pred_flat_transformer = []\n","\n","# Assuming 'aspects' is defined earlier and includes 'overall'\n","# aspects = ['food', 'service', 'ambiance', 'price', 'context', 'overall']\n","\n","for i, aspect_name in enumerate(aspects):  # Iterate through all aspects, including 'overall'\n","    print(f\"\\n--- Aspect: {aspect_name} ---\")\n","    true_labels = Y_true_transformer_np[:, i]\n","    pred_labels = Y_pred_transformer_np[:, i]\n","\n","    all_true_flat_transformer.extend(true_labels)\n","    all_pred_flat_transformer.extend(pred_labels)\n","\n","    # Get unique labels in pred_labels and true_labels\n","    unique_labels = np.unique(np.concatenate((pred_labels, true_labels)))\n","\n","    # Filter target_names to include only the present labels\n","    present_target_names = [name for idx, name in enumerate(sentiment_classes) if idx in unique_labels]\n","\n","    report = classification_report(true_labels, pred_labels, target_names=present_target_names, zero_division=0)\n","    print(report)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted', zero_division=0)\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    transformer_report[aspect_name] = {'precision': precision, 'recall': recall, 'f1-score': f1, 'accuracy': accuracy}\n","\n","# Overall Micro and Macro Averages\n","print(\"\\n--- Transformer Overall Micro/Macro Averages ---\")\n","precision_micro_t, recall_micro_t, f1_micro_t, _ = precision_recall_fscore_support(all_true_flat_transformer, all_pred_flat_transformer, average='micro', zero_division=0)\n","precision_macro_t, recall_macro_t, f1_macro_t, _ = precision_recall_fscore_support(all_true_flat_transformer, all_pred_flat_transformer, average='macro', zero_division=0)\n","print(f\"Micro Average: Precision={precision_micro_t:.4f}, Recall={recall_micro_t:.4f}, F1-Score={f1_micro_t:.4f}\")\n","print(f\"Macro Average: Precision={precision_macro_t:.4f}, Recall={recall_macro_t:.4f}, F1-Score={f1_macro_t:.4f}\")\n","\n","transformer_report['overall_micro'] = {'precision': precision_micro_t, 'recall': recall_micro_t, 'f1-score': f1_micro_t}\n","transformer_report['overall_macro'] = {'precision': precision_macro_t, 'recall': recall_macro_t, 'f1-score': f1_macro_t}\n","\n","# Store transformer results for comparison plots\n","transformer_f1_scores = {aspect: metrics['f1-score'] for aspect, metrics in transformer_report.items() if 'overall' not in aspect}  # Exclude overall averages here if needed"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Rpb7WZeUyzg5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744774732216,"user_tz":300,"elapsed":55,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"}},"outputId":"0c08cc39-ce03-4ad2-bcd1-6425bc4b960a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Integer-to-String Label Map for Prediction: {0: 'negative', 1: 'neutral', 2: 'positive'}\n","\n","Using label map for prediction: {0: 'negative', 1: 'neutral', 2: 'positive'}\n","\n","--- Example Prediction ---\n","Review: \"The pizza was amazing, truly authentic Italian style! However, the waiter was quite rude and ignored us for a long time.\"\n","\n","Predicted Sentiments:\n","- Food: Positive\n","- Service: Negative\n","- Ambiance: Neutral\n","- Price: Neutral\n","- Context: Neutral\n","- Overall: Positive\n","\n","--- Example Prediction 2 ---\n","Review: \"Service was terrible, and the food was bad and pricey. Terrible atmosphere.\"\n","\n","Predicted Sentiments:\n","- Food: Neutral\n","- Service: Negative\n","- Ambiance: Negative\n","- Price: Neutral\n","- Context: Neutral\n","- Overall: Negative\n","\n","--- Example Prediction 3 ---\n","Review: \"This restaurant is bad.\"\n","\n","Predicted Sentiments:\n","- Food: Neutral\n","- Service: Negative\n","- Ambiance: Neutral\n","- Price: Neutral\n","- Context: Neutral\n","- Overall: Negative\n","\n","--- Example Prediction 4 ---\n","Review: \"This restaurant is great. The food was yummy. The service was excellent. The price was inexpensive and the ambiance was outstanding. We went for my birthday.\"\n","\n","Predicted Sentiments:\n","- Food: Positive\n","- Service: Neutral\n","- Ambiance: Positive\n","- Price: Neutral\n","- Context: Neutral\n","- Overall: Positive\n"]}],"source":["# --- Define the sentiment classes corresponding to your integer labels ---\n","# This should match the 'bin_labels' used during binning (e.g., 0, 1, 2)\n","# and the 'label_names' (e.g., 'negative', 'neutral', 'positive')\n","sentiment_classes = ['negative', 'neutral', 'positive'] # MUST match the order used for bin_labels=[0, 1, 2]\n","integer_to_string_label_map = { i: label for i, label in enumerate(sentiment_classes) }\n","print(f\"Integer-to-String Label Map for Prediction: {integer_to_string_label_map}\")\n","\n","def predict_sentiment(review_text, model, tokenizer, aspects, int_to_str_map):\n","    \"\"\"Predicts sentiment for all aspects for a given review text.\"\"\"\n","\n","    # Preprocess the input text\n","    processed_text = preprocess_text(review_text) # Assumes preprocess_text is defined elsewhere\n","    if not processed_text:\n","        return {\"error\": \"Review text is empty after preprocessing.\"}\n","\n","    # Tokenize\n","    inputs = tokenizer(processed_text, return_tensors='pt', truncation=True, padding=True, max_length=128) # Adjust max_length if needed\n","\n","    # Move inputs to the same device as the model\n","    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","\n","    # Predict\n","    model.eval() # Ensure model is in eval mode\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits # Shape: (1, num_aspects * num_classes_per_aspect)\n","\n","    # Process logits to get predictions per aspect\n","    num_classes_per_aspect = len(int_to_str_map) # Get number of classes (e.g., 3)\n","    # Reshape to (1, num_aspects, num_classes_per_aspect)\n","    reshaped_logits = logits.view(1, len(aspects), num_classes_per_aspect)\n","\n","    # Get the index of the max logit for each aspect (these are your 0, 1, 2 labels)\n","    predictions_indices = torch.argmax(reshaped_logits, dim=2).squeeze().cpu().numpy() # Shape: (num_aspects,)\n","\n","    # Handle case where there's only one aspect (numpy might return a scalar)\n","    if predictions_indices.ndim == 0:\n","        predictions_indices = [predictions_indices.item()] # Make it a list\n","\n","    # Map predicted integer indices back to sentiment string labels\n","    predicted_sentiments = {}\n","    for i, pred_idx in enumerate(predictions_indices):\n","         # Ensure the predicted index exists in the map, handle potential errors\n","         sentiment_label = int_to_str_map.get(pred_idx, \"Unknown Label\")\n","         predicted_sentiments[aspects[i]] = sentiment_label\n","\n","\n","    # # --- OLD MAPPING LOGIC (REMOVE/REPLACE) ---\n","    # # Map indices back to sentiment labels\n","    # # Create inverse map (0: 'neutral', 1: 'positive', -1: 'negative'} <--- This was likely incorrect\n","    # # inverse_label_map = {v: k for k, v in label_map.items()} <--- Remove this line\n","    # # predicted_sentiments = {aspects[i]: inverse_label_map[pred_idx] for i, pred_idx in enumerate(predictions_indices)} <--- Remove this line\n","    # # --- END OLD MAPPING LOGIC ---\n","\n","    return predicted_sentiments\n","\n","# --- Example Usage (Location: Near the end of the notebook, around page 20) ---\n","\n","# Ensure 'aspects' list is defined correctly (should match training)\n","aspects = ['food', 'service', 'ambiance', 'price', 'context', 'overall']\n","\n","# Define the mapping from integer labels (0, 1, 2) to strings ('negative', 'neutral', 'positive')\n","# This map MUST align with the 'bin_labels' used during binning.\n","# If bin_labels = [0, 1, 2] corresponds to ['negative', 'neutral', 'positive']\n","integer_to_string_label_map = {\n","    0: 'negative',\n","    1: 'neutral',\n","    2: 'positive'\n","}\n","print(f\"\\nUsing label map for prediction: {integer_to_string_label_map}\")\n","\n","\n","new_review = \"The pizza was amazing, truly authentic Italian style! However, the waiter was quite rude and ignored us for a long time.\"\n","# Pass the correct integer-to-string map to the function\n","predicted_results = predict_sentiment(new_review, model, tokenizer, aspects, integer_to_string_label_map)\n","\n","print(\"\\n--- Example Prediction ---\")\n","print(f\"Review: \\\"{new_review}\\\"\")\n","print(\"\\nPredicted Sentiments:\")\n","if \"error\" in predicted_results:\n","    print(predicted_results[\"error\"])\n","else:\n","    for aspect, sentiment in predicted_results.items():\n","        print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\") # Keep .capitalize() for display\n","\n","\n","new_review_2 = \"Service was terrible, and the food was bad and pricey. Terrible atmosphere.\"\n","predicted_results_2 = predict_sentiment(new_review_2, model, tokenizer, aspects, integer_to_string_label_map)\n","print(\"\\n--- Example Prediction 2 ---\")\n","print(f\"Review: \\\"{new_review_2}\\\"\")\n","print(\"\\nPredicted Sentiments:\")\n","if \"error\" in predicted_results_2:\n","    print(predicted_results_2[\"error\"])\n","else:\n","    for aspect, sentiment in predicted_results_2.items():\n","        print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n","\n","new_review_3 = \"This restaurant is bad.\"\n","predicted_results_3 = predict_sentiment(new_review_3, model, tokenizer, aspects, integer_to_string_label_map)\n","print(\"\\n--- Example Prediction 3 ---\") # Corrected print statement index\n","print(f\"Review: \\\"{new_review_3}\\\"\")\n","print(\"\\nPredicted Sentiments:\")\n","if \"error\" in predicted_results_3:\n","    print(predicted_results_3[\"error\"])\n","else:\n","    for aspect, sentiment in predicted_results_3.items():\n","        print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n","\n","new_review_4 = \"This restaurant is great. The food was yummy. The service was excellent. The price was inexpensive and the ambiance was outstanding. We went for my birthday.\"\n","predicted_results_4 = predict_sentiment(new_review_4, model, tokenizer, aspects, integer_to_string_label_map)\n","print(\"\\n--- Example Prediction 4 ---\") # Corrected print statement index\n","print(f\"Review: \\\"{new_review_4}\\\"\")\n","print(\"\\nPredicted Sentiments:\")\n","if \"error\" in predicted_results_4:\n","    print(predicted_results_4[\"error\"])\n","else:\n","    for aspect, sentiment in predicted_results_4.items():\n","        print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n","\n"]},{"cell_type":"code","source":["# --- Get User Input and Predict ---\n","while True:\n","    user_review = input(\"Enter a restaurant review (or type 'exit' to quit): \")\n","    if user_review.lower() == 'exit':\n","        break\n","\n","    predicted_results = predict_sentiment(user_review, model, tokenizer, aspects, integer_to_string_label_map)\n","\n","    print(\"\\n--- User Review Prediction ---\")\n","    print(f\"Review: \\\"{user_review}\\\"\")\n","    print(\"\\nPredicted Sentiments:\")\n","    if \"error\" in predicted_results:\n","        print(predicted_results[\"error\"])\n","    else:\n","        for aspect, sentiment in predicted_results.items():\n","            print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n","\n","    print(\"-\" * 30) # Separator for multiple reviews\n","\n","print(\"Exiting sentiment analysis.\")"],"metadata":{"id":"fS0Sv04En6Gm"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1QqEz_QbL280I6e3MQS1tS-2Fut2KN0J4","timestamp":1744424751362},{"file_id":"1X_B9TN5IhurE_EHPSnR8auvjax3rrED5","timestamp":1744378781643},{"file_id":"1QnqBNH9RmKDxa9oD2c5NTJH5ziinYa0x","timestamp":1744319285930},{"file_id":"13cDaLaW4uRM53OtgMNF1AdYpkYb3w3Ug","timestamp":1743836866981}],"authorship_tag":"ABX9TyP14Y3eHKiS3tyChF7H8DiQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3e367864dd2f4b10bc3ff3b151b46734":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c149b5adec34d498adde80342fc99f5","IPY_MODEL_375cce2e62ae4f359db5ac21b569d5ff","IPY_MODEL_cd8cf040dcdc45be89554a9eea7c8684"],"layout":"IPY_MODEL_963413e6a30749648b268cb980f93fbc"}},"2c149b5adec34d498adde80342fc99f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b098d2b670364d1fae3e9c2260c8aaba","placeholder":"","style":"IPY_MODEL_48ed2b8ddb894e40a431a6cf9085d8d4","value":"tokenizer_config.json:100%"}},"375cce2e62ae4f359db5ac21b569d5ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_28e7b7ed9d3949118d0b8ca32fbb8008","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_284b1c9b827a4431b58ed2f6129e15b7","value":48}},"cd8cf040dcdc45be89554a9eea7c8684":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43867c91be264328be33e5f42342518f","placeholder":"","style":"IPY_MODEL_ad551431d55443349bbecba4ae3caf5f","value":"48.0/48.0[00:00&lt;00:00,5.21kB/s]"}},"963413e6a30749648b268cb980f93fbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b098d2b670364d1fae3e9c2260c8aaba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48ed2b8ddb894e40a431a6cf9085d8d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28e7b7ed9d3949118d0b8ca32fbb8008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"284b1c9b827a4431b58ed2f6129e15b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43867c91be264328be33e5f42342518f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad551431d55443349bbecba4ae3caf5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de0447f1bef6401abd9ddab23700ee90":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d2112bfc66c4475b142279a9dcefb42","IPY_MODEL_a932e1b596684aa497ab7893d38e12ab","IPY_MODEL_7d8dc588f8124031b45e48fe894ad151"],"layout":"IPY_MODEL_36f30a28109347d4a20de30a046fab9f"}},"1d2112bfc66c4475b142279a9dcefb42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b9d40832f574c2c98cc74950d7222a9","placeholder":"","style":"IPY_MODEL_42b3d3042f2e426ca0e68a84f2f511d9","value":"vocab.txt:100%"}},"a932e1b596684aa497ab7893d38e12ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b489cb3528940028b736e6e87221b3b","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2bf6c3867f8a4562881d0465fec97bbd","value":231508}},"7d8dc588f8124031b45e48fe894ad151":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd5b6ace7f3744d1b99b28f18c19df94","placeholder":"","style":"IPY_MODEL_5a9a105f32594f9191222f0e783537be","value":"232k/232k[00:00&lt;00:00,4.80MB/s]"}},"36f30a28109347d4a20de30a046fab9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b9d40832f574c2c98cc74950d7222a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42b3d3042f2e426ca0e68a84f2f511d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b489cb3528940028b736e6e87221b3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bf6c3867f8a4562881d0465fec97bbd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd5b6ace7f3744d1b99b28f18c19df94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a9a105f32594f9191222f0e783537be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"425e4bcc72af422ca761363c62bff12c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_197e1f7cdb5a4fd396dd7486e10d0512","IPY_MODEL_85eb0280d0fd4c7780fdcef9b7bd3414","IPY_MODEL_c2e7cc1ea7ac490e9c9fed249bdb64e7"],"layout":"IPY_MODEL_a84e5f25be0a482da94839e419f98740"}},"197e1f7cdb5a4fd396dd7486e10d0512":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26b5cb29f4a64a1598393d6773988ee5","placeholder":"","style":"IPY_MODEL_e1cb42df5a7745ba88f3f883d96d13b2","value":"tokenizer.json:100%"}},"85eb0280d0fd4c7780fdcef9b7bd3414":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1055df58bb96480a8bf28ef6b3bddbf4","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_52198debe48747d9b87a3e1323dd293d","value":466062}},"c2e7cc1ea7ac490e9c9fed249bdb64e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad420e9062fe4078a263969b3e5d1ebd","placeholder":"","style":"IPY_MODEL_dec6132366164e23b77cbb9fcc85b39d","value":"466k/466k[00:00&lt;00:00,4.33MB/s]"}},"a84e5f25be0a482da94839e419f98740":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26b5cb29f4a64a1598393d6773988ee5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1cb42df5a7745ba88f3f883d96d13b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1055df58bb96480a8bf28ef6b3bddbf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52198debe48747d9b87a3e1323dd293d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ad420e9062fe4078a263969b3e5d1ebd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dec6132366164e23b77cbb9fcc85b39d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13ce28b63b61434e81df5263825f001e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07e1ffac08b94a579799db298af4d555","IPY_MODEL_7d393bff604d4c1c8f41903ff7dbd005","IPY_MODEL_ff7646b32afb46829a7bfa34026d07f7"],"layout":"IPY_MODEL_2abed77c7481455db1099516f214bc7c"}},"07e1ffac08b94a579799db298af4d555":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b74de697ad7740f5b94077d210596b5e","placeholder":"","style":"IPY_MODEL_082a0a018d754c1b8e6ed9f3b416034c","value":"config.json:100%"}},"7d393bff604d4c1c8f41903ff7dbd005":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e8951f5fe9f41618cf4414d02aa4e7b","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_928e11a476fa410eaef36d89bf19cdd1","value":483}},"ff7646b32afb46829a7bfa34026d07f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b266cf63b2048b4b1dd31e699b60f88","placeholder":"","style":"IPY_MODEL_1d94c4a5d5f941c3be377c0ae79aebf4","value":"483/483[00:00&lt;00:00,61.5kB/s]"}},"2abed77c7481455db1099516f214bc7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b74de697ad7740f5b94077d210596b5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"082a0a018d754c1b8e6ed9f3b416034c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e8951f5fe9f41618cf4414d02aa4e7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"928e11a476fa410eaef36d89bf19cdd1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b266cf63b2048b4b1dd31e699b60f88":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d94c4a5d5f941c3be377c0ae79aebf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc2913302a344120a9a4d67b33dfdf6f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13dd0d574aac40b286fc99e79ba44124","IPY_MODEL_843ec795fe564f79b06dd90256788d77","IPY_MODEL_d9d650e7feae43808e14d2d2214b40b4"],"layout":"IPY_MODEL_2019498d22cc40b6a5eac9000351a819"}},"13dd0d574aac40b286fc99e79ba44124":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c31b165833e4dcfa7fe8d77053f69c7","placeholder":"","style":"IPY_MODEL_7288e44b4b794c08a721fff516fef60a","value":"model.safetensors:100%"}},"843ec795fe564f79b06dd90256788d77":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fce7dda606384a51add2608221a2a2fe","max":267954768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b62c1515a6284d07b54b1b6bcf9057b5","value":267954768}},"d9d650e7feae43808e14d2d2214b40b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dafe3be6f4f493588b4f06efa87f367","placeholder":"","style":"IPY_MODEL_2369d2b98c3446d59d40df73c55adb78","value":"268M/268M[00:01&lt;00:00,234MB/s]"}},"2019498d22cc40b6a5eac9000351a819":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c31b165833e4dcfa7fe8d77053f69c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7288e44b4b794c08a721fff516fef60a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fce7dda606384a51add2608221a2a2fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b62c1515a6284d07b54b1b6bcf9057b5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8dafe3be6f4f493588b4f06efa87f367":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2369d2b98c3446d59d40df73c55adb78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}