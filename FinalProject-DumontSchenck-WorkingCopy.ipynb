{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 13036,
     "status": "ok",
     "timestamp": 1743936456719,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "adqiHMoIxsrF",
    "outputId": "5034bd2d-5526-4d51-82b3-1bec935a9e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Text Preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "# Scikit-learn for baseline models and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "# PyTorch and Hugging Face Transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # Correct import for AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DistilBertTokenizer,  # Example using DistilBERT\n",
    "    DistilBertForSequenceClassification\n",
    ")\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 21414,
     "status": "ok",
     "timestamp": 1743936478138,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "sDKxsGKPyC0d",
    "outputId": "eb18b01b-5d40-4dd5-ac87-c2b5287be3de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally. Ensure the file paths are correct.\n",
      "Review data path: yelp_academic_dataset_review.json\n",
      "Business data path: yelp_academic_dataset_business.json\n",
      "\n",
      "Loading review data from: yelp_academic_dataset_review.json\n",
      "Review data loaded in 9.74 seconds.\n",
      "Review dataset shape: (1000000, 9)\n",
      "\n",
      "Review Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count    Dtype         \n",
      "---  ------       --------------    -----         \n",
      " 0   review_id    1000000 non-null  object        \n",
      " 1   user_id      1000000 non-null  object        \n",
      " 2   business_id  1000000 non-null  object        \n",
      " 3   stars        1000000 non-null  int64         \n",
      " 4   useful       1000000 non-null  int64         \n",
      " 5   funny        1000000 non-null  int64         \n",
      " 6   cool         1000000 non-null  int64         \n",
      " 7   text         1000000 non-null  object        \n",
      " 8   date         1000000 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(4), object(4)\n",
      "memory usage: 68.7+ MB\n",
      "\n",
      "Review Data Head:\n",
      "                review_id                 user_id             business_id  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
      "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
      "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
      "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "\n",
      "   stars  useful  funny  cool  \\\n",
      "0      3       0      0     0   \n",
      "1      5       1      0     1   \n",
      "2      3       0      0     0   \n",
      "3      5       1      0     1   \n",
      "4      4       1      0     1   \n",
      "\n",
      "                                                text                date  \n",
      "0  If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
      "1  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
      "2  Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
      "4  Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n",
      "\n",
      "'date' column converted to datetime.\n",
      "\n",
      "Loading business data from: yelp_academic_dataset_business.json\n",
      "Business data loaded in 1.97 seconds.\n",
      "Business dataset shape: (150346, 14)\n",
      "\n",
      "Business Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150346 entries, 0 to 150345\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   business_id   150346 non-null  object \n",
      " 1   name          150346 non-null  object \n",
      " 2   address       150346 non-null  object \n",
      " 3   city          150346 non-null  object \n",
      " 4   state         150346 non-null  object \n",
      " 5   postal_code   150346 non-null  object \n",
      " 6   latitude      150346 non-null  float64\n",
      " 7   longitude     150346 non-null  float64\n",
      " 8   stars         150346 non-null  float64\n",
      " 9   review_count  150346 non-null  int64  \n",
      " 10  is_open       150346 non-null  int64  \n",
      " 11  attributes    136602 non-null  object \n",
      " 12  categories    150243 non-null  object \n",
      " 13  hours         127123 non-null  object \n",
      "dtypes: float64(3), int64(2), object(9)\n",
      "memory usage: 16.1+ MB\n",
      "\n",
      "Business Data Head:\n",
      "              business_id                      name  \\\n",
      "0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n",
      "1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n",
      "2  tUFrWirKiKi_TAnsVWINQQ                    Target   \n",
      "3  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n",
      "4  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n",
      "\n",
      "                           address           city state postal_code  \\\n",
      "0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n",
      "1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n",
      "2             5255 E Broadway Blvd         Tucson    AZ       85711   \n",
      "3                      935 Race St   Philadelphia    PA       19107   \n",
      "4                    101 Walnut St     Green Lane    PA       18054   \n",
      "\n",
      "    latitude   longitude  stars  review_count  is_open  \\\n",
      "0  34.426679 -119.711197    5.0             7        0   \n",
      "1  38.551126  -90.335695    3.0            15        1   \n",
      "2  32.223236 -110.880452    3.5            22        0   \n",
      "3  39.955505  -75.155564    4.0            80        1   \n",
      "4  40.338183  -75.471659    4.5            13        1   \n",
      "\n",
      "                                          attributes  \\\n",
      "0                      {'ByAppointmentOnly': 'True'}   \n",
      "1             {'BusinessAcceptsCreditCards': 'True'}   \n",
      "2  {'BikeParking': 'True', 'BusinessAcceptsCredit...   \n",
      "3  {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n",
      "4  {'BusinessAcceptsCreditCards': 'True', 'Wheelc...   \n",
      "\n",
      "                                          categories  \\\n",
      "0  Doctors, Traditional Chinese Medicine, Naturop...   \n",
      "1  Shipping Centers, Local Services, Notaries, Ma...   \n",
      "2  Department Stores, Shopping, Fashion, Home & G...   \n",
      "3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n",
      "4                          Brewpubs, Breweries, Food   \n",
      "\n",
      "                                               hours  \n",
      "0                                               None  \n",
      "1  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n",
      "2  {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...  \n",
      "3  {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...  \n",
      "4  {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  \n",
      "\n",
      "Example Business Categories: Doctors, Traditional Chinese Medicine, Naturopathic/Holistic, Acupuncture, Health & Medical, Nutritionists\n"
     ]
    }
   ],
   "source": [
    "\n",
    "review_file_path = r\"yelp_academic_dataset_review.json\" # Use raw string\n",
    "business_file_path = r\"yelp_academic_dataset_business.json\" # Use raw string\n",
    "print(\"Running locally. Ensure the file paths are correct.\")\n",
    "print(f\"Review data path: {review_file_path}\")\n",
    "print(f\"Business data path: {business_file_path}\")\n",
    "\n",
    "# --- Load Review Data ---\n",
    "# Load the data (line-delimited JSON)\n",
    "# Consider loading in chunks or using nrows for large files during development\n",
    "print(f\"\\nLoading review data from: {review_file_path}\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    # Using pd.read_json is generally faster if file format is consistent\n",
    "    # Increase nrows if needed for more data, remove for full dataset\n",
    "    df_reviews = pd.read_json(review_file_path, lines=True, nrows=1000000) # Load more reviews initially if memory allows\n",
    "    # Note: Reading the full review file (~8GB) might require significant RAM or chunking.\n",
    "except ValueError as e:\n",
    "     print(f\"Error reading review JSON with pandas: {e}. Trying line-by-line.\")\n",
    "     data = []\n",
    "     # Limit lines read during line-by-line loading as well\n",
    "     line_limit = 1000000 # Adjust as needed\n",
    "     with open(review_file_path, 'r', encoding='utf-8') as f:\n",
    "         for i, line in enumerate(f):\n",
    "             if i < line_limit:\n",
    "                 try:\n",
    "                     data.append(json.loads(line))\n",
    "                 except json.JSONDecodeError:\n",
    "                     print(f\"Skipping malformed line {i+1} in reviews\")\n",
    "                     continue\n",
    "             else:\n",
    "                 break\n",
    "     df_reviews = pd.DataFrame(data)\n",
    "\n",
    "loading_time = time.time() - start_time\n",
    "print(f\"Review data loaded in {loading_time:.2f} seconds.\")\n",
    "print(f\"Review dataset shape: {df_reviews.shape}\")\n",
    "print(\"\\nReview Data Info:\")\n",
    "df_reviews.info()\n",
    "print(\"\\nReview Data Head:\")\n",
    "print(df_reviews.head())\n",
    "\n",
    "# Convert 'date' column to datetime objects\n",
    "df_reviews['date'] = pd.to_datetime(df_reviews['date'])\n",
    "print(\"\\n'date' column converted to datetime.\")\n",
    "\n",
    "# --- Load Business Data ---\n",
    "print(f\"\\nLoading business data from: {business_file_path}\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    df_business = pd.read_json(business_file_path, lines=True)\n",
    "except ValueError as e:\n",
    "    print(f\"Error reading business JSON with pandas: {e}. Trying line-by-line.\")\n",
    "    business_data = []\n",
    "    with open(business_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                business_data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed line in businesses\")\n",
    "                continue\n",
    "    df_business = pd.DataFrame(business_data)\n",
    "\n",
    "loading_time = time.time() - start_time\n",
    "print(f\"Business data loaded in {loading_time:.2f} seconds.\")\n",
    "print(f\"Business dataset shape: {df_business.shape}\")\n",
    "print(\"\\nBusiness Data Info:\")\n",
    "df_business.info()\n",
    "print(\"\\nBusiness Data Head:\")\n",
    "print(df_business.head())\n",
    "print(\"\\nExample Business Categories:\", df_business['categories'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1743936478482,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "YNowSd2VyJLF",
    "outputId": "bd4dde0c-a186-461f-bc85-6f90770d37d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Businesses found in target cities (8 cities): 9125\n",
      "Restaurants/Food businesses found in target location: 4099\n",
      "Number of unique target restaurant business IDs: 4099\n",
      "\n",
      "Reviews filtered by date (2019-01-01 to 2024-12-31). Shape: (217126, 9)\n",
      "Reviews filtered by target business IDs. Shape: (20377, 9)\n",
      "\n",
      "Final shape for processing (filtered by date and location/category): (20377, 5)\n",
      "\n",
      "Selected relevant columns ('review_id', 'text', 'stars', 'date', 'business_id').\n",
      "                     review_id  \\\n",
      "194093  3CmdoGKBZUX3Nb5IfbztMg   \n",
      "208672  KY8dRN_k2EoR_QujKAegTQ   \n",
      "209964  eetCBZ2roKWnKAGzBsC2qw   \n",
      "210571  krP6fy3aJ4AwdoyOb1YlwQ   \n",
      "214202  F0EkyfrHAaAc5DAJEWaDog   \n",
      "\n",
      "                                                     text  stars  \\\n",
      "194093  My favorite coffee shop in New Orleans for sur...      5   \n",
      "208672  Delete out at this Dave and busters was a litt...      3   \n",
      "209964  Believe the hype. Great food at reasonable pri...      5   \n",
      "210571  I love love this place! Fresh squeezed orange ...      5   \n",
      "214202  Wow!  This place was a great find on our trip ...      5   \n",
      "\n",
      "                      date             business_id  \n",
      "194093 2019-01-27 15:08:14  itAhmbhHOyQQparfwicjDQ  \n",
      "208672 2019-02-14 17:06:51  UmjITdXHhEF46ho6IhaGQg  \n",
      "209964 2019-02-28 02:17:13  vN6v8m4DO45Z4pp8yxxF_w  \n",
      "210571 2019-02-03 16:40:01  vwOJ4yZH1I85-T3dJyr2ig  \n",
      "214202 2019-03-12 01:48:31  0yGH62WGQy_W3JoR6L5Kew  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_19828\\2530891825.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  businesses_in_location['categories'] = businesses_in_location['categories'].fillna('')\n"
     ]
    }
   ],
   "source": [
    "# Filter data based on your criteria (date range AND location/category)\n",
    "\n",
    "# 1. Filter Businesses by Location and Category\n",
    "\n",
    "# Define target location (New Orleans Greater Metropolitan Area)\n",
    "# List relevant cities. Convert to lowercase for case-insensitive matching.\n",
    "target_cities = ['new orleans', 'metairie', 'kenner', 'gretna', 'harahan', 'westwego', 'chalmette', 'slidell'] # Add more as needed\n",
    "# Convert city names in DataFrame to lowercase for consistent matching\n",
    "df_business['city_lower'] = df_business['city'].str.lower()\n",
    "\n",
    "# Filter by city\n",
    "businesses_in_location = df_business[df_business['city_lower'].isin(target_cities)]\n",
    "print(f\"\\nBusinesses found in target cities ({len(target_cities)} cities): {businesses_in_location.shape[0]}\")\n",
    "\n",
    "# Filter by category (must contain 'Restaurants' or 'Food')\n",
    "# Handle potential None/NaN values in 'categories'\n",
    "businesses_in_location['categories'] = businesses_in_location['categories'].fillna('')\n",
    "restaurants_in_location = businesses_in_location[\n",
    "    businesses_in_location['categories'].str.contains('Restaurant|Food', case=False, regex=True)\n",
    "].copy()\n",
    "print(f\"Restaurants/Food businesses found in target location: {restaurants_in_location.shape[0]}\")\n",
    "\n",
    "# Get the business IDs of these restaurants\n",
    "target_business_ids = set(restaurants_in_location['business_id'])\n",
    "print(f\"Number of unique target restaurant business IDs: {len(target_business_ids)}\")\n",
    "\n",
    "if not target_business_ids:\n",
    "    print(\"\\nWarning: No restaurant business IDs found for the specified location. Check city names and categories.\")\n",
    "    # Handle this case - perhaps stop execution or proceed with only date filtering?\n",
    "    # For now, we'll let it proceed, resulting in an empty review dataframe later.\n",
    "\n",
    "# 2. Filter Reviews by Date\n",
    "start_date = datetime(2019, 1, 1)\n",
    "end_date = datetime(2024, 12, 31) # Use end of 2024 for completeness\n",
    "\n",
    "df_reviews_filtered_date = df_reviews[(df_reviews['date'] >= start_date) & (df_reviews['date'] <= end_date)].copy()\n",
    "print(f\"\\nReviews filtered by date ({start_date.date()} to {end_date.date()}). Shape: {df_reviews_filtered_date.shape}\")\n",
    "\n",
    "# 3. Filter Date-Filtered Reviews by Target Business IDs\n",
    "if target_business_ids:\n",
    "    df_filtered = df_reviews_filtered_date[\n",
    "        df_reviews_filtered_date['business_id'].isin(target_business_ids)\n",
    "    ].copy()\n",
    "    print(f\"Reviews filtered by target business IDs. Shape: {df_filtered.shape}\")\n",
    "else:\n",
    "    print(\"\\nSkipping business ID filtering as no target IDs were found.\")\n",
    "    df_filtered = pd.DataFrame(columns=df_reviews_filtered_date.columns) # Create empty DataFrame matching columns\n",
    "\n",
    "\n",
    "# Select relevant columns for final processing\n",
    "# Keep 'business_id' if you might need it later, otherwise drop it\n",
    "df_processed = df_filtered[['review_id', 'text', 'stars', 'date', 'business_id']].copy()\n",
    "\n",
    "print(f\"\\nFinal shape for processing (filtered by date and location/category): {df_processed.shape}\")\n",
    "\n",
    "if df_processed.empty:\n",
    "     print(\"\\nWARNING: No reviews match the filtering criteria (Date + Location/Category). Subsequent steps will fail.\")\n",
    "     # Consider stopping execution here if the dataframe is empty.\n",
    "else:\n",
    "    print(\"\\nSelected relevant columns ('review_id', 'text', 'stars', 'date', 'business_id').\")\n",
    "    print(df_processed.head())\n",
    "\n",
    "# --- Cleanup (Optional: remove intermediate dataframes to save memory) ---\n",
    "# del df_reviews, df_business, df_reviews_filtered_date, businesses_in_location, restaurants_in_location\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# print(\"\\nIntermediate dataframes cleaned up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 44466,
     "status": "ok",
     "timestamp": 1743936522949,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "cjQ70p3A84vL",
    "outputId": "08716ed3-5110-4db5-df50-85190931d392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\jacob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\jacob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\jacob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jacob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jacob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jacob\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Output file with sentiment scores will be saved to: reviews_with_sentiment_scores.csv\n",
      "\n",
      "Starting sentiment analysis process on 20377 filtered reviews.\n",
      "Analyzing review sentiments (this may take a while)...\n",
      "Sentiment analysis complete.\n",
      "\n",
      "Successfully exported 20377 reviews with sentiment scores to:\n",
      "reviews_with_sentiment_scores.csv\n",
      "\n",
      "--- NEXT STEPS ---\n",
      "1. Explore the calculated sentiment scores (e.g., correlations with 'stars').\n",
      "2. Refine the aspect keyword lists for better classification accuracy.\n",
      "3. Consider alternative sentiment lexicons or models if needed.\n",
      "4. Use these sentiment scores as features for further analysis or modeling.\n",
      "\n",
      "Preview of DataFrame with Sentiment Scores:\n",
      "                review_id             business_id                date  stars  \\\n",
      "0  3CmdoGKBZUX3Nb5IfbztMg  itAhmbhHOyQQparfwicjDQ 2019-01-27 15:08:14      5   \n",
      "1  KY8dRN_k2EoR_QujKAegTQ  UmjITdXHhEF46ho6IhaGQg 2019-02-14 17:06:51      3   \n",
      "2  eetCBZ2roKWnKAGzBsC2qw  vN6v8m4DO45Z4pp8yxxF_w 2019-02-28 02:17:13      5   \n",
      "3  krP6fy3aJ4AwdoyOb1YlwQ  vwOJ4yZH1I85-T3dJyr2ig 2019-02-03 16:40:01      5   \n",
      "4  F0EkyfrHAaAc5DAJEWaDog  0yGH62WGQy_W3JoR6L5Kew 2019-03-12 01:48:31      5   \n",
      "\n",
      "                                                text  food_sentiment  \\\n",
      "0  My favorite coffee shop in New Orleans for sur...        0.000000   \n",
      "1  Delete out at this Dave and busters was a litt...        0.000000   \n",
      "2  Believe the hype. Great food at reasonable pri...        0.695213   \n",
      "3  I love love this place! Fresh squeezed orange ...        0.514760   \n",
      "4  Wow!  This place was a great find on our trip ...        0.000000   \n",
      "\n",
      "   service_sentiment  ambiance_sentiment  price_sentiment  context_sentiment  \\\n",
      "0           0.000000             0.46365         0.000000                0.0   \n",
      "1           0.138145             0.00000         0.077773                0.0   \n",
      "2           0.000000             0.00000         0.000000                0.0   \n",
      "3           0.000000             0.18202         0.000000                0.0   \n",
      "4           0.000000             0.13616         0.000000                0.0   \n",
      "\n",
      "   other_sentiment  \n",
      "0         0.340000  \n",
      "1         0.098891  \n",
      "2         0.561125  \n",
      "3         0.633280  \n",
      "4         0.749280  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re # For basic text cleaning\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Download NLTK data (only needs to be done once) ---\n",
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError: # Changed to catch LookupError\n",
    "    print(\"Downloading NLTK 'punkt' tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError: # Changed to catch LookupError\n",
    "    print(\"Downloading NLTK 'stopwords'...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "except LookupError: # Changed to catch LookupError\n",
    "    print(\"Downloading NLTK 'vader_lexicon'...\")\n",
    "    nltk.download('vader_lexicon', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab') # Check if the resource is already present\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK 'punkt_tab'...\")\n",
    "    nltk.download('punkt_tab') # Download specifically 'punkt_tab'\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to your processed data (assuming df_processed is loaded elsewhere)\n",
    "# If df_processed is not loaded, you'll need to load it first, e.g.:\n",
    "# input_data_file = 'path/to/your/processed_reviews.csv'\n",
    "# df_processed = pd.read_csv(input_data_file)\n",
    "\n",
    "# Define where to save the file with sentiment scores\n",
    "try:\n",
    "    # Assume Google Drive is mounted if in Colab\n",
    "    output_sentiment_file = 'reviews_with_sentiment_scores.csv'\n",
    "    print(f\"Output file with sentiment scores will be saved to: {output_sentiment_file}\")\n",
    "except NameError: # If 'drive' object doesn't exist (running locally)\n",
    "    # Adjust local path if necessary\n",
    "    output_sentiment_file = r\"reviews_with_sentiment_scores.csv\"\n",
    "    print(f\"Output file with sentiment scores will be saved to: {output_sentiment_file}\")\n",
    "\n",
    "# --- Aspect Keyword Lists (Illustrative Examples - EXPAND THESE SIGNIFICANTLY) ---\n",
    "# Based on the paper, these lists should be comprehensive.\n",
    "# Consider synonyms, related terms, common phrases.\n",
    "aspect_keywords = {\n",
    "    'food': ['food', 'dish', 'meal', 'taste', 'flavor', 'delicious', 'menu', 'ingredients', 'fresh', 'chicken', 'beef', 'pizza', 'sushi', 'burger', 'fries', 'dessert', 'drink', 'beverage', 'appetizer', 'entree', 'quality'],\n",
    "    'service': ['service', 'staff', 'waiter', 'waitress', 'host', 'bartender', 'manager', 'friendly', 'attentive', 'rude', 'slow', 'fast', 'order', 'wait', 'helpful', 'customer service', 'server'],\n",
    "    'ambiance': ['ambiance', 'atmosphere', 'decor', 'music', 'lighting', 'vibe', 'setting', 'environment', 'clean', 'noisy', 'quiet', 'comfortable', 'cozy', 'view', 'patio', 'inside', 'outside'],\n",
    "    'price': ['price', 'cost', 'value', 'cheap', 'expensive', 'affordable', 'overpriced', 'deal', 'bill', 'charge', 'worth', 'money', 'budget', 'dollar', 'paid'],\n",
    "    'context': ['occasion', 'birthday', 'anniversary', 'date', 'friends', 'family', 'kids', 'group', 'business', 'lunch', 'dinner', 'celebration', 'visit', 'experience', 'time', 'recommend', 'location', 'neighborhood', 'parking', 'reservation'] # Context is broad\n",
    "}\n",
    "\n",
    "# --- Initialize Sentiment Analyzer ---\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower() # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = text.strip() # Remove leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "def classify_sentence_aspect(sentence, aspect_kw):\n",
    "    \"\"\"\n",
    "    Classifies a sentence into one aspect based on keyword counts.\n",
    "    Follows the paper's method: assign to aspect with most keywords.\n",
    "    \"\"\"\n",
    "    # Clean and tokenize the sentence\n",
    "    cleaned_sentence = clean_text(sentence)\n",
    "    words = word_tokenize(cleaned_sentence)\n",
    "    # Remove stop words for better keyword matching\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    scores = {aspect: 0 for aspect in aspect_kw}\n",
    "    max_score = 0\n",
    "    best_aspect = 'other' # Default if no keywords match\n",
    "\n",
    "    # Count keywords for each aspect\n",
    "    for aspect, keywords in aspect_kw.items():\n",
    "        # Use a set for faster checking\n",
    "        keyword_set = set(keywords)\n",
    "        # Count occurrences of keywords in the sentence words\n",
    "        score = sum(1 for word in words if word in keyword_set)\n",
    "        scores[aspect] = score\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_aspect = aspect\n",
    "        # Handle ties (optional, could assign to 'multiple' or prioritize)\n",
    "        # elif score == max_score and max_score > 0:\n",
    "        #     best_aspect = 'multiple' # Or handle differently\n",
    "\n",
    "    # Ensure we only classify if at least one keyword was found\n",
    "    if max_score == 0:\n",
    "        best_aspect = 'other'\n",
    "\n",
    "    return best_aspect\n",
    "\n",
    "def get_sentence_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    Calculates the VADER compound sentiment score for a sentence.\n",
    "    Ranges from -1 (most negative) to +1 (most positive).\n",
    "    The paper used AFINN (-5 to +5), VADER is a common alternative.\n",
    "    \"\"\"\n",
    "    # VADER's polarity_scores returns dict: {'neg': %, 'neu': %, 'pos': %, 'compound': score}\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    return vs['compound'] # Use the compound score as the overall sentiment\n",
    "\n",
    "def analyze_review_sentiment(review_text, aspect_kw):\n",
    "    \"\"\"\n",
    "    Analyzes a full review text to get weighted sentiment scores per aspect.\n",
    "    Implements the methodology from the paper.\n",
    "    \"\"\"\n",
    "    if not isinstance(review_text, str) or not review_text.strip():\n",
    "        # Return default scores for empty or invalid reviews\n",
    "        return {f'{aspect}_sentiment': 0.0 for aspect in aspect_kw} | {'other_sentiment': 0.0}\n",
    "\n",
    "    # 1. Tokenize into sentences\n",
    "    sentences = sent_tokenize(review_text)\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    if total_sentences == 0:\n",
    "        # Handle reviews that couldn't be sentence-tokenized\n",
    "         return {f'{aspect}_sentiment': 0.0 for aspect in aspect_kw} | {'other_sentiment': 0.0}\n",
    "\n",
    "    # 2. Classify aspect and get sentiment for each sentence\n",
    "    sentence_results = []\n",
    "    for sentence in sentences:\n",
    "        aspect = classify_sentence_aspect(sentence, aspect_kw)\n",
    "        sentiment = get_sentence_sentiment(sentence)\n",
    "        sentence_results.append({'aspect': aspect, 'sentiment': sentiment})\n",
    "\n",
    "    # 3. Aggregate sentiment scores and count sentences per aspect\n",
    "    aspect_sentiments_sum = defaultdict(float)\n",
    "    aspect_sentence_counts = defaultdict(int)\n",
    "\n",
    "    for result in sentence_results:\n",
    "        aspect = result['aspect']\n",
    "        sentiment = result['sentiment']\n",
    "        aspect_sentiments_sum[aspect] += sentiment\n",
    "        aspect_sentence_counts[aspect] += 1\n",
    "\n",
    "    # 4. Calculate weighted sentiment scores (as per paper's formula)\n",
    "    weighted_scores = {}\n",
    "    all_aspects = list(aspect_kw.keys()) + ['other'] # Include 'other' category\n",
    "\n",
    "    for aspect in all_aspects:\n",
    "        sum_score = aspect_sentiments_sum[aspect]\n",
    "        count = aspect_sentence_counts[aspect]\n",
    "\n",
    "        # Calculate weight (proportion of sentences for this aspect)\n",
    "        weight = count / total_sentences if total_sentences > 0 else 0\n",
    "\n",
    "        # Weighted score = Sum * Weight (or proportion)\n",
    "        # Note: The paper's formula description is slightly ambiguous.\n",
    "        # Sentiment_ij = Sentiment Score_ij * (# sentences Attribute j / # sentences Attribute_ij)\n",
    "        # This implies weighting the *average* score per attribute.\n",
    "        # Let's calculate both average and weighted sum for clarity.\n",
    "        # average_score = sum_score / count if count > 0 else 0.0\n",
    "        # weighted_score_avg_based = average_score * weight # Avg score weighted by proportion\n",
    "\n",
    "        # Alternative interpretation: Weight the *total* sentiment sum for the aspect\n",
    "        # This seems more aligned with capturing overall impact.\n",
    "        weighted_score_sum_based = sum_score * weight\n",
    "\n",
    "        # Store the weighted score based on the sum interpretation\n",
    "        weighted_scores[f'{aspect}_sentiment'] = weighted_score_sum_based\n",
    "\n",
    "    return weighted_scores\n",
    "\n",
    "\n",
    "# --- Main Processing Logic ---\n",
    "\n",
    "# Check if df_processed exists and is populated\n",
    "# IMPORTANT: Ensure df_processed is loaded before this point if not already in memory.\n",
    "# Example:\n",
    "# try:\n",
    "#     df_processed = pd.read_csv('path/to/your/filtered_reviews.csv')\n",
    "#     print(f\"Loaded {len(df_processed)} reviews.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Error: Processed review file not found.\")\n",
    "#     df_processed = pd.DataFrame() # Create empty df to avoid error\n",
    "\n",
    "if 'df_processed' not in locals() or not isinstance(df_processed, pd.DataFrame) or df_processed.empty:\n",
    "    print(\"\\nError: The 'df_processed' DataFrame is empty or does not exist.\")\n",
    "    print(\"Please ensure your data loading step ran successfully.\")\n",
    "    # Optionally, raise an error:\n",
    "    # raise ValueError(\"Cannot proceed without data in df_processed.\")\n",
    "else:\n",
    "    print(f\"\\nStarting sentiment analysis process on {len(df_processed)} filtered reviews.\")\n",
    "\n",
    "    # Make a copy to avoid modifying the original DataFrame directly\n",
    "    df_analysis = df_processed.copy()\n",
    "\n",
    "    # --- Apply Sentiment Analysis ---\n",
    "    print(\"Analyzing review sentiments (this may take a while)...\")\n",
    "\n",
    "    # Use .apply() to process the 'text' column of each review\n",
    "    # The result of apply will be a Series of dictionaries\n",
    "    sentiment_results = df_analysis['text'].apply(lambda text: analyze_review_sentiment(text, aspect_keywords))\n",
    "\n",
    "    # Convert the Series of dictionaries into separate columns in the DataFrame\n",
    "    # pd.json_normalize is efficient for this\n",
    "    sentiment_df = pd.json_normalize(sentiment_results)\n",
    "\n",
    "    # Add these new sentiment score columns to the analysis DataFrame\n",
    "    df_analysis = pd.concat([df_analysis.reset_index(drop=True), sentiment_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(\"Sentiment analysis complete.\")\n",
    "\n",
    "    # --- Select and Reorder Columns for Output ---\n",
    "    # Keep original identifiers and add the new sentiment scores\n",
    "    output_columns = ['review_id', 'business_id', 'date', 'stars', 'text'] + \\\n",
    "                     [f'{aspect}_sentiment' for aspect in aspect_keywords] + \\\n",
    "                     ['other_sentiment'] # Include 'other' if calculated\n",
    "\n",
    "    # Ensure all expected columns exist before selecting\n",
    "    final_columns = [col for col in output_columns if col in df_analysis.columns]\n",
    "    df_final_output = df_analysis[final_columns]\n",
    "\n",
    "    # --- Export to CSV ---\n",
    "    try:\n",
    "        # Use index=False to avoid writing the DataFrame index as a column\n",
    "        df_final_output.to_csv(output_sentiment_file, index=False, encoding='utf-8')\n",
    "        print(f\"\\nSuccessfully exported {len(df_final_output)} reviews with sentiment scores to:\")\n",
    "        print(output_sentiment_file)\n",
    "        print(\"\\n--- NEXT STEPS ---\")\n",
    "        print(\"1. Explore the calculated sentiment scores (e.g., correlations with 'stars').\")\n",
    "        print(\"2. Refine the aspect keyword lists for better classification accuracy.\")\n",
    "        print(\"3. Consider alternative sentiment lexicons or models if needed.\")\n",
    "        print(\"4. Use these sentiment scores as features for further analysis or modeling.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError exporting file: {e}\")\n",
    "        print(\"Please check the output path and ensure you have write permissions.\")\n",
    "\n",
    "# Optional: Display the first few rows with the new scores\n",
    "if 'df_final_output' in locals():\n",
    "    print(\"\\nPreview of DataFrame with Sentiment Scores:\")\n",
    "    print(df_final_output.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 16190,
     "status": "ok",
     "timestamp": 1743936539140,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "HSAHlEOvyNlH",
    "outputId": "8d95ff30-6cc3-4623-c0d8-d2c31a4a3d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting text preprocessing...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting text preprocessing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 39\u001b[0m df_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_processed\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     40\u001b[0m processing_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText preprocessing completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessing_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_processed' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize lemmatizer and stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Download the necessary NLTK data if it's not already present\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans and preprocesses text data.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Apply preprocessing (can take time on large datasets)\n",
    "print(\"\\nStarting text preprocessing...\")\n",
    "start_time = time.time()\n",
    "df_processed['processed_text'] = df_processed['text'].apply(preprocess_text)\n",
    "processing_time = time.time() - start_time\n",
    "print(f\"Text preprocessing completed in {processing_time:.2f} seconds.\")\n",
    "\n",
    "# Display some processed text examples\n",
    "print(\"\\nOriginal vs Processed Text Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(\"Original:\", df_processed['text'].iloc[i][:200] + \"...\") # Show first 200 chars\n",
    "    print(\"Processed:\", df_processed['processed_text'].iloc[i])\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Drop rows where processed text is empty\n",
    "df_processed = df_processed[df_processed['processed_text'] != '']\n",
    "print(f\"\\nShape after removing empty processed texts: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>business_id</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194093</th>\n",
       "      <td>3CmdoGKBZUX3Nb5IfbztMg</td>\n",
       "      <td>My favorite coffee shop in New Orleans for sur...</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-01-27 15:08:14</td>\n",
       "      <td>itAhmbhHOyQQparfwicjDQ</td>\n",
       "      <td>favorite coffee shop new orleans sure perfect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208672</th>\n",
       "      <td>KY8dRN_k2EoR_QujKAegTQ</td>\n",
       "      <td>Delete out at this Dave and busters was a litt...</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-02-14 17:06:51</td>\n",
       "      <td>UmjITdXHhEF46ho6IhaGQg</td>\n",
       "      <td>delete dave buster little different one ive do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209964</th>\n",
       "      <td>eetCBZ2roKWnKAGzBsC2qw</td>\n",
       "      <td>Believe the hype. Great food at reasonable pri...</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-02-28 02:17:13</td>\n",
       "      <td>vN6v8m4DO45Z4pp8yxxF_w</td>\n",
       "      <td>believe hype great food reasonable price hubby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210571</th>\n",
       "      <td>krP6fy3aJ4AwdoyOb1YlwQ</td>\n",
       "      <td>I love love this place! Fresh squeezed orange ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-02-03 16:40:01</td>\n",
       "      <td>vwOJ4yZH1I85-T3dJyr2ig</td>\n",
       "      <td>love love place fresh squeezed orange juice be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214202</th>\n",
       "      <td>F0EkyfrHAaAc5DAJEWaDog</td>\n",
       "      <td>Wow!  This place was a great find on our trip ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-03-12 01:48:31</td>\n",
       "      <td>0yGH62WGQy_W3JoR6L5Kew</td>\n",
       "      <td>wow place great find trip new orleans zulu cof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996505</th>\n",
       "      <td>Sc9urRlJiH7ePDjEhNHPDg</td>\n",
       "      <td>Love the ambiance and the drinks are the best!...</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-05-31 16:40:43</td>\n",
       "      <td>EHhwVLuIIrq4somySU0m8A</td>\n",
       "      <td>love ambiance drink best city view amazing rom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998615</th>\n",
       "      <td>di98vQb74WlVOq8cK7X-og</td>\n",
       "      <td>This place sucked was dirty bartender sucked s...</td>\n",
       "      <td>1</td>\n",
       "      <td>2019-09-07 11:10:13</td>\n",
       "      <td>WSi08gApEgsS62TIE78J_g</td>\n",
       "      <td>place sucked dirty bartender sucked food ive l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998620</th>\n",
       "      <td>Alo3AwR_n_i1B7fo2q3fLg</td>\n",
       "      <td>This is the best Mexican food I've ever had......</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-08-18 00:02:01</td>\n",
       "      <td>AhVvuzwWQU__CligbY4r0Q</td>\n",
       "      <td>best mexican food ive ever ive mexicoyelp wont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999358</th>\n",
       "      <td>TAey23zuki7wBIhgQuy4cA</td>\n",
       "      <td>Great friend oysters, char boiled oysters a pl...</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-02-18 18:17:39</td>\n",
       "      <td>DcBLYSvOuWcNReolRVr12A</td>\n",
       "      <td>great friend oyster char boiled oyster place g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999909</th>\n",
       "      <td>heNuIap58Z-1co224RzImQ</td>\n",
       "      <td>This place has really good lunch specials. I r...</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-03-15 21:47:03</td>\n",
       "      <td>ft6I8fOrKQtVdM_trVXk8A</td>\n",
       "      <td>place really good lunch special recently met f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20377 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     review_id  \\\n",
       "194093  3CmdoGKBZUX3Nb5IfbztMg   \n",
       "208672  KY8dRN_k2EoR_QujKAegTQ   \n",
       "209964  eetCBZ2roKWnKAGzBsC2qw   \n",
       "210571  krP6fy3aJ4AwdoyOb1YlwQ   \n",
       "214202  F0EkyfrHAaAc5DAJEWaDog   \n",
       "...                        ...   \n",
       "996505  Sc9urRlJiH7ePDjEhNHPDg   \n",
       "998615  di98vQb74WlVOq8cK7X-og   \n",
       "998620  Alo3AwR_n_i1B7fo2q3fLg   \n",
       "999358  TAey23zuki7wBIhgQuy4cA   \n",
       "999909  heNuIap58Z-1co224RzImQ   \n",
       "\n",
       "                                                     text  stars  \\\n",
       "194093  My favorite coffee shop in New Orleans for sur...      5   \n",
       "208672  Delete out at this Dave and busters was a litt...      3   \n",
       "209964  Believe the hype. Great food at reasonable pri...      5   \n",
       "210571  I love love this place! Fresh squeezed orange ...      5   \n",
       "214202  Wow!  This place was a great find on our trip ...      5   \n",
       "...                                                   ...    ...   \n",
       "996505  Love the ambiance and the drinks are the best!...      5   \n",
       "998615  This place sucked was dirty bartender sucked s...      1   \n",
       "998620  This is the best Mexican food I've ever had......      5   \n",
       "999358  Great friend oysters, char boiled oysters a pl...      4   \n",
       "999909  This place has really good lunch specials. I r...      3   \n",
       "\n",
       "                      date             business_id  \\\n",
       "194093 2019-01-27 15:08:14  itAhmbhHOyQQparfwicjDQ   \n",
       "208672 2019-02-14 17:06:51  UmjITdXHhEF46ho6IhaGQg   \n",
       "209964 2019-02-28 02:17:13  vN6v8m4DO45Z4pp8yxxF_w   \n",
       "210571 2019-02-03 16:40:01  vwOJ4yZH1I85-T3dJyr2ig   \n",
       "214202 2019-03-12 01:48:31  0yGH62WGQy_W3JoR6L5Kew   \n",
       "...                    ...                     ...   \n",
       "996505 2019-05-31 16:40:43  EHhwVLuIIrq4somySU0m8A   \n",
       "998615 2019-09-07 11:10:13  WSi08gApEgsS62TIE78J_g   \n",
       "998620 2019-08-18 00:02:01  AhVvuzwWQU__CligbY4r0Q   \n",
       "999358 2019-02-18 18:17:39  DcBLYSvOuWcNReolRVr12A   \n",
       "999909 2019-03-15 21:47:03  ft6I8fOrKQtVdM_trVXk8A   \n",
       "\n",
       "                                           processed_text  \n",
       "194093  favorite coffee shop new orleans sure perfect ...  \n",
       "208672  delete dave buster little different one ive do...  \n",
       "209964  believe hype great food reasonable price hubby...  \n",
       "210571  love love place fresh squeezed orange juice be...  \n",
       "214202  wow place great find trip new orleans zulu cof...  \n",
       "...                                                   ...  \n",
       "996505  love ambiance drink best city view amazing rom...  \n",
       "998615  place sucked dirty bartender sucked food ive l...  \n",
       "998620  best mexican food ive ever ive mexicoyelp wont...  \n",
       "999358  great friend oyster char boiled oyster place g...  \n",
       "999909  place really good lunch special recently met f...  \n",
       "\n",
       "[20377 rows x 6 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 963,
     "status": "ok",
     "timestamp": 1743936540103,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "IsTW9F96yahm",
    "outputId": "9292d05d-4c27-4bee-8f07-a8c2d7f851a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16301\n",
      "Testing set size: 4076\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n",
    "\n",
    "# 1. Load or calculate sentiment scores if not in df_processed\n",
    "\n",
    "# If sentiment scores are already in df_processed, use them\n",
    "sentiment_columns = ['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']\n",
    "\n",
    "sentiment_file_path = \"reviews_with_sentiment_scores.csv\"\n",
    "sentiment_df = pd.read_csv(sentiment_file_path)\n",
    "sentiment_df\n",
    "# Merge sentiment scores with the original dataframe based on a common column (e.g., 'review_id')\n",
    "df_processed = pd.merge(\n",
    "    df_processed,\n",
    "    sentiment_df[['review_id', 'food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']],\n",
    "    on='review_id',\n",
    "    how='left'\n",
    ")\n",
    "Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']].copy()\n",
    "\n",
    "# 2. Correct quantile binning and discretization of sentiment scores\n",
    "for column in Y.columns:\n",
    "    # Cast column values to float64 explicitly to avoid dtype issues\n",
    "    Y[column] = Y[column].astype(float)\n",
    "\n",
    "    # Calculate quantile bin edges\n",
    "    quantile_positions = [0, 1/3, 2/3, 1]\n",
    "    bin_edges = Y[column].quantile(quantile_positions).drop_duplicates().tolist()  # Remove duplicates\n",
    "\n",
    "    # Adjust labels to match the number of bins dynamically\n",
    "    num_bins = len(bin_edges) - 1  # Number of bins = number of edges - 1\n",
    "    labels = list(range(num_bins))  # Create sequential labels (e.g., 0, 1, 2)\n",
    "\n",
    "    # Use pd.cut for binning and assign it safely\n",
    "    Y[column] = pd.cut(\n",
    "        Y[column], bins=bin_edges, labels=labels, include_lowest=True\n",
    "    ).astype(int)  # Ensure integer dtype for labels\n",
    "\n",
    "# 3. Prepare data for training and testing\n",
    "# Assuming 'processed_text' column contains preprocessed text data\n",
    "X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n",
    "\n",
    "# Get the first sentiment column for stratification\n",
    "stratify_column = Y.iloc[:, 0]  # Assuming Y is a DataFrame\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=stratify_column\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "# ... (rest of the code, including TF-IDF, tokenization, and model training) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 2019,
     "status": "ok",
     "timestamp": 1743936542147,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "PfCwazyuyeeZ",
    "outputId": "df5de17a-3fb5-4f92-c52c-e6a2aa63bb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Baseline Model (Logistic Regression with OneVsRest)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputClassifier(estimator=LogisticRegression(random_state=42,\n",
       "                                                   solver=&#x27;liblinear&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MultiOutputClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.multioutput.MultiOutputClassifier.html\">?<span>Documentation for MultiOutputClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MultiOutputClassifier(estimator=LogisticRegression(random_state=42,\n",
       "                                                   solver=&#x27;liblinear&#x27;))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>estimator: LogisticRegression</div></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(random_state=42, solver=&#x27;liblinear&#x27;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(random_state=42, solver=&#x27;liblinear&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiOutputClassifier(estimator=LogisticRegression(random_state=42,\n",
       "                                                   solver='liblinear'))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n",
    "\n",
    "# 1. Load or calculate sentiment scores if not in df_processed\n",
    "# ... (Your existing code for loading or calculating sentiment scores) ...\n",
    "\n",
    "\n",
    "# 2. Correct quantile binning and discretization of sentiment scores\n",
    "# ... (Your existing code for binning and discretization) ...\n",
    "\n",
    "\n",
    "# 3. Prepare data for training and testing\n",
    "# Assuming 'processed_text' column contains preprocessed text data\n",
    "X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n",
    "\n",
    "# ... (Your existing code for splitting data into train/test sets) ...\n",
    "\n",
    "\n",
    "# --- TF-IDF Vectorization ---\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit the vectorizer to the training data and transform it\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# --- Baseline Model Training and Evaluation ---\n",
    "# Using OneVsRestClassifier: Trains one classifier per class (or per output)\n",
    "# Here, we wrap Logistic Regression. One LR model will be trained for each aspect.\n",
    "print(\"\\nTraining Baseline Model (Logistic Regression with OneVsRest)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a base estimator\n",
    "log_reg = LogisticRegression(solver='liblinear', random_state=42) # Liblinear often good for high-dim sparse data\n",
    "\n",
    "# Wrap it with OneVsRestClassifier for multi-label/multi-output scenario\n",
    "# It trains one classifier per column in Y\n",
    "baseline_model = MultiOutputClassifier(log_reg) # Simpler API for multi-output integer targets\n",
    "\n",
    "# Train the model\n",
    "baseline_model.fit(X_train_tfidf, Y_train) # Use the numeric labels DataFrame/Array\n",
    "\n",
    "# ... (Rest of your code for baseline evaluation) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 41665803,
     "status": "ok",
     "timestamp": 1743978207951,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "U1W_x4T5ylgS",
    "outputId": "099bbfeb-8f32-434d-ef29-38cb98f9ba46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16301\n",
      "Testing set size: 4076\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec835ea03344e26b1df9d3ba047e314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jacob\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5721504820bf4064ae25cbbc41d9cb0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addaae611837451f829f131d077aac91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b8d526d2144337b8f1b8587450411d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc165218880f4dc9adc9904c2c7f3d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Customized DistilBERT model loaded. Output layer size: 15\n",
      "Using device: cpu\n",
      "\n",
      "Starting Transformer Model Training...\n",
      "--- Epoch 1/3 ---\n",
      "  Batch 100/1019, Loss: 0.5191\n",
      "  Batch 200/1019, Loss: 0.4853\n",
      "  Batch 300/1019, Loss: 0.4184\n",
      "  Batch 400/1019, Loss: 0.2737\n",
      "  Batch 500/1019, Loss: 0.4662\n",
      "  Batch 600/1019, Loss: 0.3350\n",
      "  Batch 700/1019, Loss: 0.4010\n",
      "  Batch 800/1019, Loss: 0.4185\n",
      "  Batch 900/1019, Loss: 0.3405\n",
      "  Batch 1000/1019, Loss: 0.2974\n",
      "Epoch 1 completed in 9406.53s. Average Training Loss: 0.4463\n",
      "--- Epoch 2/3 ---\n",
      "  Batch 100/1019, Loss: 0.3732\n",
      "  Batch 200/1019, Loss: 0.2610\n",
      "  Batch 300/1019, Loss: 0.2588\n",
      "  Batch 400/1019, Loss: 0.2806\n",
      "  Batch 500/1019, Loss: 0.1786\n",
      "  Batch 600/1019, Loss: 0.2699\n",
      "  Batch 700/1019, Loss: 0.3153\n",
      "  Batch 800/1019, Loss: 0.2558\n",
      "  Batch 900/1019, Loss: 0.2719\n",
      "  Batch 1000/1019, Loss: 0.3589\n",
      "Epoch 2 completed in 8876.44s. Average Training Loss: 0.3099\n",
      "--- Epoch 3/3 ---\n",
      "  Batch 100/1019, Loss: 0.1920\n",
      "  Batch 200/1019, Loss: 0.2755\n",
      "  Batch 300/1019, Loss: 0.1581\n",
      "  Batch 400/1019, Loss: 0.3048\n",
      "  Batch 500/1019, Loss: 0.4033\n",
      "  Batch 600/1019, Loss: 0.2978\n",
      "  Batch 700/1019, Loss: 0.2432\n",
      "  Batch 800/1019, Loss: 0.1614\n",
      "  Batch 900/1019, Loss: 0.1831\n",
      "  Batch 1000/1019, Loss: 0.2046\n",
      "Epoch 3 completed in 8850.46s. Average Training Loss: 0.2426\n",
      "Transformer training finished.\n"
     ]
    }
   ],
   "source": [
    "# --- Import necessary libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # Correct import for AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DistilBertTokenizer,  # Example using DistilBERT\n",
    "    DistilBertForSequenceClassification\n",
    ")\n",
    "import time\n",
    "\n",
    "\n",
    "# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n",
    "\n",
    "# 1. Load or calculate sentiment scores if not in df_processed\n",
    "try:\n",
    "    # If sentiment scores are already in df_processed, use them\n",
    "    sentiment_columns = ['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']\n",
    "    Y = df_processed[sentiment_columns].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "except KeyError:\n",
    "    print(\"Sentiment score columns not found in df_processed. Calculating or loading them...\")\n",
    "    # Option 2: Load from a separate file (if you saved them earlier)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  # Mount Google Drive\n",
    "    sentiment_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n",
    "\n",
    "    sentiment_df = pd.read_csv(sentiment_file_path)\n",
    "    # Merge sentiment scores with the original dataframe based on a common column (e.g., 'review_id')\n",
    "    df_processed = pd.merge(\n",
    "        df_processed,\n",
    "        sentiment_df[['review_id', 'food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']],\n",
    "        on='review_id',\n",
    "        how='left'\n",
    "    )\n",
    "    Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']].copy()\n",
    "\n",
    "# 2. Correct quantile binning and discretization of sentiment scores\n",
    "for column in Y.columns:\n",
    "    # Cast column values to float64 explicitly to avoid dtype issues\n",
    "    Y[column] = Y[column].astype(float)\n",
    "\n",
    "    # Calculate quantile bin edges\n",
    "    quantile_positions = [0, 1/3, 2/3, 1]\n",
    "    bin_edges = Y[column].quantile(quantile_positions).drop_duplicates().tolist()  # Remove duplicates\n",
    "\n",
    "    # Adjust labels to match the number of bins dynamically\n",
    "    num_bins = len(bin_edges) - 1  # Number of bins = number of edges - 1\n",
    "    labels = list(range(num_bins))  # Create sequential labels (e.g., 0, 1, 2)\n",
    "\n",
    "    # Use pd.cut for binning and assign it safely\n",
    "    Y[column] = pd.cut(\n",
    "        Y[column], bins=bin_edges, labels=labels, include_lowest=True\n",
    "    ).astype(int)  # Ensure integer dtype for labels\n",
    "\n",
    "# 3. Prepare data for training and testing\n",
    "# Assuming 'processed_text' column contains preprocessed text data\n",
    "X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n",
    "\n",
    "# Get the first sentiment column for stratification\n",
    "stratify_column = Y.iloc[:, 0]  # Assuming Y is a DataFrame\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=stratify_column\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# --- Tokenization ---\n",
    "# Initialize tokenizer (if not done earlier)\n",
    "model_name = 'distilbert-base-uncased'  # Or your preferred model name\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n",
    "\n",
    "# --- Convert Y_train and Y_test to NumPy arrays ---\n",
    "Y_train_np = Y_train.values  # Assuming Y_train is a pandas DataFrame\n",
    "Y_test_np = Y_test.values   # Assuming Y_test is a pandas DataFrame\n",
    "\n",
    "\n",
    "# --- Define PyTorch Dataset ---\n",
    "class YelpAspectDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        # Ensure labels are shaped correctly (N_samples, N_aspects)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # Handle cases where encodings might be lists of tensors or similar structures\n",
    "        item = {}\n",
    "        for key, val in self.encodings.items():\n",
    "            # Ensure the value associated with the key is indexable and convert to tensor\n",
    "            if isinstance(val, list):\n",
    "                item[key] = torch.tensor(val[idx])\n",
    "            else:  # Assuming it might already be a tensor or numpy array\n",
    "                item[key] = torch.tensor(val[idx]).clone().detach()  # Make sure it's a tensor\n",
    "\n",
    "        # Labels should be FloatTensor for BCEWithLogitsLoss or LongTensor for CrossEntropyLoss\n",
    "        # For multi-label (predicting probability for each class per aspect) or multi-output (predicting one class per aspect)\n",
    "        # Let's assume multi-output: predicting one class (0, 1, 2) per aspect. Labels are LongTensor.\n",
    "        # Shape should be (num_aspects,) for a single item\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # Use the number of samples in one of the encoding keys (e.g., 'input_ids')\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = YelpAspectDataset(train_encodings, Y_train_np)\n",
    "test_dataset = YelpAspectDataset(test_encodings, Y_test_np)\n",
    "\n",
    "# --- Define Model ---\n",
    "# We need a multi-output classification head.\n",
    "# Load pre-trained DistilBERT and modify the classifier layer.\n",
    "aspects = ['food', 'service', 'ambiance', 'price', 'context']  # Define your aspects\n",
    "sentiment_classes = ['negative', 'neutral', 'positive']  # Define sentiment classes\n",
    "\n",
    "num_aspects = len(aspects)\n",
    "num_classes_per_aspect = len(sentiment_classes)  # 3 classes: positive, negative, neutral\n",
    "\n",
    "# Load the base model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_aspects * num_classes_per_aspect  # TEMPORARY - see below\n",
    ")\n",
    "\n",
    "# *** IMPORTANT: Modifying the Classifier Head for Multi-Output ***\n",
    "# The standard `DistilBertForSequenceClassification` has ONE output layer for num_labels.\n",
    "# For multi-output (one prediction per aspect), we need to customize.\n",
    "# Option 1: Treat as N independent classification tasks (simpler to implement with standard model if labels handled outside)\n",
    "# Option 2: Create N classification heads (more complex custom model)\n",
    "# Option 3: Reshape the output of a single large head (requires careful loss calculation)\n",
    "\n",
    "# Let's try Option 3 conceptually: A single head predicting scores for all aspect-class combinations.\n",
    "# Then reshape and calculate loss per aspect.\n",
    "# The output dimension should be num_aspects * num_classes_per_aspect\n",
    "# The final layer needs to be replaced or adapted.\n",
    "\n",
    "# Get the original classifier's input dimension\n",
    "original_classifier_in_features = model.classifier.in_features\n",
    "\n",
    "# Replace the classifier head. Output size is num_aspects * num_classes (e.g., 5 * 3 = 15)\n",
    "model.classifier = torch.nn.Linear(original_classifier_in_features, num_aspects * num_classes_per_aspect)\n",
    "# The pre_classifier layer might also need adjustment if present (DistilBERT has one)\n",
    "if hasattr(model, 'pre_classifier') and model.pre_classifier is not None:\n",
    "    original_pre_classifier_in_features = model.pre_classifier.in_features\n",
    "    model.pre_classifier = torch.nn.Linear(original_pre_classifier_in_features, original_classifier_in_features)  # Keep standard pre-classifier -> classifier connection\n",
    "else:\n",
    "    print(\"Model does not have a separate pre_classifier layer.\")\n",
    "\n",
    "print(f\"\\nCustomized DistilBERT model loaded. Output layer size: {model.classifier.out_features}\")\n",
    "\n",
    "# --- Training Setup ---\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Adjust batch_size based on GPU memory\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)  # Larger batch size for evaluation is fine\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)  # Typical learning rate for fine-tuning\n",
    "num_epochs = 3  # Adjust as needed (start small)\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Loss Function: CrossEntropyLoss is suitable for multi-class classification (per aspect)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"\\nStarting Transformer Model Training...\")\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"--- Epoch {epoch + 1}/{num_epochs} ---\")\n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)  # Shape: (batch_size, num_aspects)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # Shape: (batch_size, num_aspects * num_classes_per_aspect)\n",
    "\n",
    "        # Calculate loss: Reshape logits and labels for CrossEntropyLoss\n",
    "        # Logits need to be (batch_size * num_aspects, num_classes_per_aspect)\n",
    "        # Labels need to be (batch_size * num_aspects,)\n",
    "        reshaped_logits = logits.view(-1, num_classes_per_aspect)  # (batch * aspects, classes)\n",
    "        reshaped_labels = labels.view(-1)  # (batch * aspects,)\n",
    "\n",
    "        loss = loss_fn(reshaped_logits, reshaped_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:  # Print progress every 100 batches\n",
    "            print(f\"  Batch {i + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f}s. Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "print(\"Transformer training finished.\")\n",
    "# Consider saving the trained model\n",
    "# model.save_pretrained('./my_aspect_sentiment_model')\n",
    "# tokenizer.save_pretrained('./my_aspect_sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1743980017976,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "n6W-tvrY4dDm",
    "outputId": "067c1869-9990-40bb-a3cb-eb46de264009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the fine-tuned model and tokenizer...\n",
      "Model and tokenizer saved to my_aspect_sentiment_model\n"
     ]
    }
   ],
   "source": [
    "# --- Save the Model and Tokenizer ---\n",
    "print(\"\\nSaving the fine-tuned model and tokenizer...\")\n",
    "\n",
    "# Define the directory where you want to save them\n",
    "save_directory = \"my_aspect_sentiment_model\" # You can change this path\n",
    "\n",
    "# Save the model's weights and configuration file\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the tokenizer's vocabulary and configuration file\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")\n",
    "\n",
    "# --- (Optional) How to load the model and tokenizer later ---\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
    "# loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "# print(\"\\nModel and tokenizer loaded successfully (example).\")\n",
    "# loaded_model.to(device) # Remember to move the loaded model to the correct device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # Correct import for AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DistilBertTokenizer,  # Example using DistilBERT\n",
    "    DistilBertForSequenceClassification\n",
    ")\n",
    "import time\n",
    "\n",
    "aspects = ['food', 'service', 'ambiance', 'price', 'context']  # Define your aspects\n",
    "sentiment_classes = ['negative', 'neutral', 'positive']  # Define sentiment classes\n",
    "\n",
    "num_aspects = len(aspects)\n",
    "num_classes_per_aspect = len(sentiment_classes)  # 3 classes: positive, negative, neutral\n",
    "\n",
    "save_directory = \"my_aspect_sentiment_model\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(save_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 882047,
     "status": "ok",
     "timestamp": 1743982838745,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "bJXIs83zyqgy",
    "outputId": "3f00bada-d533-466a-b2d1-7b39556bc3ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Transformer Model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m all_labels_transformer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# Disable gradient calculations for inference\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_loader\u001b[49m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating Transformer Model...\")\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "all_preds_transformer = []\n",
    "all_labels_transformer = []\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculations for inference\n",
    "    for batch in test_loader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device) # Shape: (batch_size, num_aspects)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits # Shape: (batch_size, num_aspects * num_classes_per_aspect)\n",
    "\n",
    "        # Get predictions: Reshape logits and find the class with max probability for each aspect\n",
    "        # Reshape logits to (batch_size, num_aspects, num_classes_per_aspect)\n",
    "        reshaped_logits = logits.view(input_ids.size(0), num_aspects, num_classes_per_aspect)\n",
    "        predictions = torch.argmax(reshaped_logits, dim=2) # Get predicted class index along the class dimension\n",
    "\n",
    "        all_preds_transformer.extend(predictions.cpu().numpy())\n",
    "        all_labels_transformer.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert collected predictions and labels into numpy arrays\n",
    "Y_pred_transformer_np = np.array(all_preds_transformer) # Shape: (num_test_samples, num_aspects)\n",
    "Y_true_transformer_np = np.array(all_labels_transformer) # Shape: (num_test_samples, num_aspects)\n",
    "\n",
    "# Calculate metrics per aspect\n",
    "transformer_report = {}\n",
    "print(\"\\n--- Transformer Classification Report (Per Aspect) ---\")\n",
    "\n",
    "all_true_flat_transformer = []\n",
    "all_pred_flat_transformer = []\n",
    "\n",
    "\n",
    "for i, aspect_name in enumerate(aspects):\n",
    "    print(f\"\\n--- Aspect: {aspect_name} ---\")\n",
    "    true_labels = Y_true_transformer_np[:, i]\n",
    "    pred_labels = Y_pred_transformer_np[:, i]\n",
    "\n",
    "    all_true_flat_transformer.extend(true_labels)\n",
    "    all_pred_flat_transformer.extend(pred_labels)\n",
    "\n",
    "    # Get unique labels in pred_labels and true_labels\n",
    "    unique_labels = np.unique(np.concatenate((pred_labels, true_labels)))\n",
    "\n",
    "    # Filter target_names to include only the present labels\n",
    "    present_target_names = [name for idx, name in enumerate(sentiment_classes) if idx in unique_labels]\n",
    "\n",
    "    report = classification_report(true_labels, pred_labels, target_names=present_target_names, zero_division=0)\n",
    "    print(report)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    transformer_report[aspect_name] = {'precision': precision, 'recall': recall, 'f1-score': f1, 'accuracy': accuracy}\n",
    "\n",
    "# Overall Micro and Macro Averages\n",
    "print(\"\\n--- Transformer Overall Micro/Macro Averages ---\")\n",
    "precision_micro_t, recall_micro_t, f1_micro_t, _ = precision_recall_fscore_support(all_true_flat_transformer, all_pred_flat_transformer, average='micro', zero_division=0)\n",
    "precision_macro_t, recall_macro_t, f1_macro_t, _ = precision_recall_fscore_support(all_true_flat_transformer, all_pred_flat_transformer, average='macro', zero_division=0)\n",
    "print(f\"Micro Average: Precision={precision_micro_t:.4f}, Recall={recall_micro_t:.4f}, F1-Score={f1_micro_t:.4f}\")\n",
    "print(f\"Macro Average: Precision={precision_macro_t:.4f}, Recall={recall_macro_t:.4f}, F1-Score={f1_macro_t:.4f}\")\n",
    "\n",
    "transformer_report['overall_micro'] = {'precision': precision_micro_t, 'recall': recall_micro_t, 'f1-score': f1_micro_t}\n",
    "transformer_report['overall_macro'] = {'precision': precision_macro_t, 'recall': recall_macro_t, 'f1-score': f1_macro_t}\n",
    "\n",
    "\n",
    "# Store transformer results for comparison plots\n",
    "transformer_f1_scores = {aspect: metrics['f1-score'] for aspect, metrics in transformer_report.items() if 'overall' not in aspect}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 2037,
     "status": "ok",
     "timestamp": 1743983143351,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "ZRcLY0GrfsML",
    "outputId": "77173f58-3c0f-4846-9105-80465419a698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 16301\n",
      "Testing set size: 4076\n",
      "\n",
      "Training Baseline Model (Logistic Regression with OneVsRest)...\n",
      "\n",
      "--- Baseline (Logistic Regression) Classification Report (Per Aspect) ---\n",
      "\n",
      "--- Aspect: food ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.68      0.86      0.76      1655\n",
      "     neutral       0.44      0.17      0.25      1062\n",
      "    positive       0.67      0.77      0.72      1359\n",
      "\n",
      "    accuracy                           0.65      4076\n",
      "   macro avg       0.60      0.60      0.57      4076\n",
      "weighted avg       0.61      0.65      0.61      4076\n",
      "\n",
      "\n",
      "--- Aspect: service ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.91      0.84      2445\n",
      "     neutral       0.83      0.02      0.04       238\n",
      "    positive       0.76      0.64      0.69      1393\n",
      "\n",
      "    accuracy                           0.77      4076\n",
      "   macro avg       0.79      0.52      0.52      4076\n",
      "weighted avg       0.77      0.77      0.74      4076\n",
      "\n",
      "\n",
      "--- Aspect: ambiance ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.99      0.94      3532\n",
      "     neutral       0.79      0.35      0.48       544\n",
      "\n",
      "    accuracy                           0.90      4076\n",
      "   macro avg       0.85      0.67      0.71      4076\n",
      "weighted avg       0.89      0.90      0.88      4076\n",
      "\n",
      "\n",
      "--- Aspect: price ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      1.00      0.97      3832\n",
      "     neutral       0.65      0.11      0.18       244\n",
      "\n",
      "    accuracy                           0.94      4076\n",
      "   macro avg       0.80      0.55      0.58      4076\n",
      "weighted avg       0.93      0.94      0.92      4076\n",
      "\n",
      "\n",
      "--- Aspect: context ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.94      0.88      2894\n",
      "     neutral       0.78      0.54      0.64      1182\n",
      "\n",
      "    accuracy                           0.82      4076\n",
      "   macro avg       0.81      0.74      0.76      4076\n",
      "weighted avg       0.82      0.82      0.81      4076\n",
      "\n",
      "\n",
      "--- Baseline Overall Micro/Macro Averages ---\n",
      "Micro Average: Precision=0.8168, Recall=0.8168, F1-Score=0.8168\n",
      "Macro Average: Precision=0.7470, Recall=0.6585, F1-Score=0.6792\n"
     ]
    }
   ],
   "source": [
    "# --- Import necessary libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n",
    "\n",
    "# 1. Load or calculate sentiment scores if not in df_processed\n",
    "try:\n",
    "    # If sentiment scores are already in df_processed, use them\n",
    "    sentiment_columns = ['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']\n",
    "    Y = df_processed[sentiment_columns].copy()  # Use .copy() to avoid SettingWithCopyWarning\n",
    "except KeyError:\n",
    "    print(\"Sentiment score columns not found in df_processed. Calculating or loading them...\")\n",
    "    # Option 2: Load from a separate file (if you saved them earlier)\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  # Mount Google Drive\n",
    "    sentiment_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n",
    "\n",
    "    sentiment_df = pd.read_csv(sentiment_file_path)\n",
    "    # Merge sentiment scores with the original dataframe based on a common column (e.g., 'review_id')\n",
    "    df_processed = pd.merge(\n",
    "        df_processed,\n",
    "        sentiment_df[['review_id', 'food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']],\n",
    "        on='review_id',\n",
    "        how='left'\n",
    "    )\n",
    "    Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']].copy()\n",
    "\n",
    "# 2. Correct quantile binning and discretization of sentiment scores\n",
    "for column in Y.columns:\n",
    "    # Cast column values to float64 explicitly to avoid dtype issues\n",
    "    Y[column] = Y[column].astype(float)\n",
    "\n",
    "    # Calculate quantile bin edges\n",
    "    quantile_positions = [0, 1/3, 2/3, 1]\n",
    "    bin_edges = Y[column].quantile(quantile_positions).drop_duplicates().tolist()  # Remove duplicates\n",
    "\n",
    "    # Adjust labels to match the number of bins dynamically\n",
    "    num_bins = len(bin_edges) - 1  # Number of bins = number of edges - 1\n",
    "    labels = list(range(num_bins))  # Create sequential labels (e.g., 0, 1, 2)\n",
    "\n",
    "    # Use pd.cut for binning and assign it safely\n",
    "    Y[column] = pd.cut(\n",
    "        Y[column], bins=bin_edges, labels=labels, include_lowest=True\n",
    "    ).astype(int)  # Ensure integer dtype for labels\n",
    "\n",
    "# 3. Prepare data for training and testing\n",
    "# Assuming 'processed_text' column contains preprocessed text data\n",
    "X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n",
    "\n",
    "# Get the first sentiment column for stratification\n",
    "stratify_column = Y.iloc[:, 0]  # Assuming Y is a DataFrame\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=stratify_column\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# --- TF-IDF Vectorization ---\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit the vectorizer to the training data and transform it\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data using the fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# --- Baseline Model Training and Evaluation ---\n",
    "# Using OneVsRestClassifier: Trains one classifier per class (or per output)\n",
    "# Here, we wrap Logistic Regression. One LR model will be trained for each aspect.\n",
    "print(\"\\nTraining Baseline Model (Logistic Regression with OneVsRest)...\")\n",
    "\n",
    "# Create a base estimator\n",
    "log_reg = LogisticRegression(solver='liblinear', random_state=42) # Liblinear often good for high-dim sparse data\n",
    "\n",
    "# Wrap it with OneVsRestClassifier for multi-label/multi-output scenario\n",
    "# It trains one classifier per column in Y\n",
    "baseline_model = MultiOutputClassifier(log_reg) # Simpler API for multi-output integer targets\n",
    "\n",
    "# Train the model\n",
    "baseline_model.fit(X_train_tfidf, Y_train) # Use the numeric labels DataFrame/Array\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred_baseline = baseline_model.predict(X_test_tfidf)\n",
    "\n",
    "# --- Baseline Evaluation ---\n",
    "# Assuming 'aspects' and 'sentiment_classes' are defined (from your Transformer code)\n",
    "\n",
    "baseline_report = {} # To store results in a dictionary\n",
    "print(\"\\n--- Baseline (Logistic Regression) Classification Report (Per Aspect) ---\")\n",
    "\n",
    "baseline_f1_scores = {} # Store F1-scores per aspect\n",
    "all_true_flat_baseline = []  # To store flattened true labels for overall metrics\n",
    "all_pred_flat_baseline = []  # To store flattened predicted labels for overall metrics\n",
    "\n",
    "for i, aspect_name in enumerate(aspects):\n",
    "    print(f\"\\n--- Aspect: {aspect_name} ---\")\n",
    "    true_labels = Y_test.iloc[:, i].values # Get true labels for this aspect\n",
    "    pred_labels = Y_pred_baseline[:, i] # Get predicted labels for this aspect\n",
    "\n",
    "    # Get unique labels in pred_labels and true_labels\n",
    "    unique_labels = np.unique(np.concatenate((pred_labels, true_labels)))\n",
    "\n",
    "    # Filter target_names to include only the present labels\n",
    "    present_target_names = [name for idx, name in enumerate(sentiment_classes) if idx in unique_labels]\n",
    "\n",
    "\n",
    "    report = classification_report(true_labels, pred_labels, target_names=present_target_names, zero_division=0)\n",
    "    print(report)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    baseline_report[aspect_name] = {'precision': precision, 'recall': recall, 'f1-score': f1, 'accuracy': accuracy}\n",
    "    baseline_f1_scores[aspect_name] = f1  # Store F1-score for this aspect\n",
    "\n",
    "    all_true_flat_baseline.extend(true_labels)  # Extend for overall metrics\n",
    "    all_pred_flat_baseline.extend(pred_labels)  # Extend for overall metrics\n",
    "\n",
    "# Overall Micro and Macro Averages for Baseline\n",
    "print(\"\\n--- Baseline Overall Micro/Macro Averages ---\")\n",
    "precision_micro_b, recall_micro_b, f1_micro_b, _ = precision_recall_fscore_support(all_true_flat_baseline, all_pred_flat_baseline, average='micro', zero_division=0)\n",
    "precision_macro_b, recall_macro_b, f1_macro_b, _ = precision_recall_fscore_support(all_true_flat_baseline, all_pred_flat_baseline, average='macro', zero_division=0)\n",
    "print(f\"Micro Average: Precision={precision_micro_b:.4f}, Recall={recall_micro_b:.4f}, F1-Score={f1_micro_b:.4f}\")\n",
    "print(f\"Macro Average: Precision={precision_macro_b:.4f}, Recall={recall_macro_b:.4f}, F1-Score={f1_macro_b:.4f}\")\n",
    "\n",
    "baseline_report['overall_micro'] = {'precision': precision_micro_b, 'recall': recall_micro_b, 'f1-score': f1_micro_b}\n",
    "baseline_report['overall_macro'] = {'precision': precision_macro_b, 'recall': recall_macro_b, 'f1-score': f1_macro_b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1743983266093,
     "user": {
      "displayName": "Aaron Dumont",
      "userId": "02170992473081287978"
     },
     "user_tz": 300
    },
    "id": "Rpb7WZeUyzg5",
    "outputId": "45db9b56-f00e-4a6d-9fbb-9f8347aa5efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Prediction ---\n",
      "Review: \"The pizza was amazing, truly authentic Italian style! However, the waiter was quite rude and ignored us for a long time. The restaurant itself is beautiful, very romantic ambiance. A bit pricey, but worth it for a special occasion.\"\n",
      "\n",
      "Predicted Sentiments:\n",
      "- Food: Negative\n",
      "- Service: Negative\n",
      "- Ambiance: Neutral\n",
      "- Price: Neutral\n",
      "- Context: Negative\n",
      "\n",
      "--- Example Prediction 2 ---\n",
      "Review: \"Everything was very cheap and affordable, and the food was delicious\"\n",
      "\n",
      "Predicted Sentiments:\n",
      "- Food: Positive\n",
      "- Service: Negative\n",
      "- Ambiance: Negative\n",
      "- Price: Negative\n",
      "- Context: Negative\n",
      "\n",
      "--- Example Prediction 2 ---\n",
      "Review: \"The food and service was bad. Everything was bad. Nothing was good, it was all bad!\"\n",
      "\n",
      "Predicted Sentiments:\n",
      "- Food: Negative\n",
      "- Service: Negative\n",
      "- Ambiance: Negative\n",
      "- Price: Negative\n",
      "- Context: Negative\n",
      "\n",
      "--- Example Prediction 2 ---\n",
      "Review: \"Everything was fantastic! We loved the food and service, it was awesome! The atmosphere was perfect and it was all super cheap as well!\"\n",
      "\n",
      "Predicted Sentiments:\n",
      "- Food: Positive\n",
      "- Service: Negative\n",
      "- Ambiance: Negative\n",
      "- Price: Negative\n",
      "- Context: Negative\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review_text, model, tokenizer, aspects, label_map):\n",
    "    \"\"\"Predicts sentiment for all aspects for a given review text.\"\"\"\n",
    "    # Preprocess the input text\n",
    "    processed_text = preprocess_text(review_text)\n",
    "    if not processed_text:\n",
    "        return \"Review text is empty after preprocessing.\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(processed_text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Predict\n",
    "    model.eval() # Ensure model is in eval mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits # Shape: (1, num_aspects * num_classes_per_aspect)\n",
    "\n",
    "    # Process logits to get predictions per aspect\n",
    "    # Reshape to (1, num_aspects, num_classes_per_aspect)\n",
    "    reshaped_logits = logits.view(1, len(aspects), len(label_map))\n",
    "    # Get the index of the max logit for each aspect\n",
    "    predictions_indices = torch.argmax(reshaped_logits, dim=2).squeeze().cpu().numpy() # Shape: (num_aspects,)\n",
    "\n",
    "    # Map indices back to sentiment labels\n",
    "    # Create inverse map {0: 'neutral', 1: 'positive', 2: 'negative'}\n",
    "    inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    predicted_sentiments = {aspects[i]: inverse_label_map[pred_idx] for i, pred_idx in enumerate(predictions_indices)}\n",
    "\n",
    "    return predicted_sentiments\n",
    "\n",
    "# Define the label_mapping dictionary\n",
    "# This assumes you have 3 sentiment classes: positive, neutral, and negative\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}  # Create the label_mapping here\n",
    "\n",
    "# Example usage:\n",
    "new_review = \"The pizza was amazing, truly authentic Italian style! However, the waiter was quite rude and ignored us for a long time. The restaurant itself is beautiful, very romantic ambiance. A bit pricey, but worth it for a special occasion.\"\n",
    "\n",
    "predicted_results = predict_sentiment(new_review, model, tokenizer, aspects, label_mapping)\n",
    "\n",
    "print(\"\\n--- Example Prediction ---\")\n",
    "print(f\"Review: \\\"{new_review}\\\"\")\n",
    "print(\"\\nPredicted Sentiments:\")\n",
    "for aspect, sentiment in predicted_results.items():\n",
    "    print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n",
    "\n",
    "# Example 2:\n",
    "new_review_2 = \"Everything was very cheap and affordable, and the food was delicious\"\n",
    "predicted_results_2 = predict_sentiment(new_review_2, model, tokenizer, aspects, label_mapping)\n",
    "print(\"\\n--- Example Prediction 2 ---\")\n",
    "print(f\"Review: \\\"{new_review_2}\\\"\")\n",
    "print(\"\\nPredicted Sentiments:\")\n",
    "for aspect, sentiment in predicted_results_2.items():\n",
    "    print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n",
    "\n",
    "# Example 3:\n",
    "new_review_2 = \"The food and service was bad. Everything was bad. Nothing was good, it was all bad!\"\n",
    "predicted_results_2 = predict_sentiment(new_review_2, model, tokenizer, aspects, label_mapping)\n",
    "print(\"\\n--- Example Prediction 2 ---\")\n",
    "print(f\"Review: \\\"{new_review_2}\\\"\")\n",
    "print(\"\\nPredicted Sentiments:\")\n",
    "for aspect, sentiment in predicted_results_2.items():\n",
    "    print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n",
    "\n",
    "# Example 3:\n",
    "new_review_2 = \"Everything was fantastic! We loved the food and service, it was awesome! The atmosphere was perfect and it was all super cheap as well!\"\n",
    "predicted_results_2 = predict_sentiment(new_review_2, model, tokenizer, aspects, label_mapping)\n",
    "print(\"\\n--- Example Prediction 2 ---\")\n",
    "print(f\"Review: \\\"{new_review_2}\\\"\")\n",
    "print(\"\\nPredicted Sentiments:\")\n",
    "for aspect, sentiment in predicted_results_2.items():\n",
    "    print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Prediction 2 ---\n",
      "Review: \"Everything was fantastic! We loved the food and service, it was awesome! The atmosphere was perfect and it was all super cheap as well!\"\n",
      "\n",
      "Predicted Sentiments:\n",
      "- Food: Positive\n",
      "- Service: Negative\n",
      "- Ambiance: Negative\n",
      "- Price: Negative\n",
      "- Context: Negative\n"
     ]
    }
   ],
   "source": [
    "new_review_1 = \"The food was bad, the waiter was slow, the building was dirty, and the prices were expensive!\"\n",
    "predicted_results_1 = predict_sentiment(new_review_2, model, tokenizer, aspects, label_mapping)\n",
    "print(\"\\n--- Example Prediction 2 ---\")\n",
    "print(f\"Review: \\\"{new_review_2}\\\"\")\n",
    "print(\"\\nPredicted Sentiments:\")\n",
    "for aspect, sentiment in predicted_results_2.items():\n",
    "    print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Prediction 1 ---\n",
      "Review: \"The food was good, the waiter was fast!\"\n",
      "\n",
      "Predicted Sentiments:\n",
      "- Food: Positive\n",
      "- Service: Positive\n",
      "- Ambiance: Negative\n",
      "- Price: Negative\n",
      "- Context: Negative\n"
     ]
    }
   ],
   "source": [
    "new_review_2 = \"The food was good, the waiter was fast!\"\n",
    "predicted_results_2 = predict_sentiment(new_review_2, model, tokenizer, aspects, label_mapping)\n",
    "print(\"\\n--- Example Prediction 1 ---\")\n",
    "print(f\"Review: \\\"{new_review_2}\\\"\")\n",
    "print(\"\\nPredicted Sentiments:\")\n",
    "for aspect, sentiment in predicted_results_2.items():\n",
    "    print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZ+qAgidmYJUw1HthLd4Jz",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "13cDaLaW4uRM53OtgMNF1AdYpkYb3w3Ug",
     "timestamp": 1743836866981
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
