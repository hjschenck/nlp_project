{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17808,"status":"ok","timestamp":1744591241075,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"adqiHMoIxsrF","outputId":"b288a002-9d09-4bb4-82e3-4b0faf5c2b5d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Libraries imported successfully.\n"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import json\n","import re\n","import time\n","from datetime import datetime\n","\n","# Text Preprocessing\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download necessary NLTK data (run once)\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')\n","try:\n","    nltk.data.find('corpora/wordnet')\n","except LookupError:\n","    nltk.download('wordnet')\n","\n","# Scikit-learn for baseline models and metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import (\n","    accuracy_score,\n","    precision_recall_fscore_support,\n","    roc_auc_score,\n","    classification_report,\n","    confusion_matrix,\n","    ConfusionMatrixDisplay,\n","    roc_curve,\n","    auc\n",")\n","from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n","\n","# PyTorch and Hugging Face Transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW  # Correct import for AdamW\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    get_linear_schedule_with_warmup,\n","    DistilBertTokenizer,  # Example using DistilBERT\n","    DistilBertForSequenceClassification\n",")\n","\n","# Plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","torch.manual_seed(42)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(42)\n","\n","print(\"Libraries imported successfully.\")\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41487,"status":"ok","timestamp":1744591282561,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"sDKxsGKPyC0d","outputId":"e9dc84e2-c805-4741-bd82-953e2b686eb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Google Drive mounted.\n","Review data path: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_review.json\n","Business data path: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_business.json\n","\n","Loading review data from: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_review.json\n","Review data loaded in 18.77 seconds.\n","Review dataset shape: (1000000, 9)\n","\n","Review Data Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000000 entries, 0 to 999999\n","Data columns (total 9 columns):\n"," #   Column       Non-Null Count    Dtype         \n","---  ------       --------------    -----         \n"," 0   review_id    1000000 non-null  object        \n"," 1   user_id      1000000 non-null  object        \n"," 2   business_id  1000000 non-null  object        \n"," 3   stars        1000000 non-null  int64         \n"," 4   useful       1000000 non-null  int64         \n"," 5   funny        1000000 non-null  int64         \n"," 6   cool         1000000 non-null  int64         \n"," 7   text         1000000 non-null  object        \n"," 8   date         1000000 non-null  datetime64[ns]\n","dtypes: datetime64[ns](1), int64(4), object(4)\n","memory usage: 68.7+ MB\n","\n","Review Data Head:\n","                review_id                 user_id             business_id  \\\n","0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n","1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n","2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n","3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n","4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n","\n","   stars  useful  funny  cool  \\\n","0      3       0      0     0   \n","1      5       1      0     1   \n","2      3       0      0     0   \n","3      5       1      0     1   \n","4      4       1      0     1   \n","\n","                                                text                date  \n","0  If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n","1  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n","2  Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n","3  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n","4  Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n","\n","'date' column converted to datetime.\n","\n","Loading business data from: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_business.json\n","Business data loaded in 5.07 seconds.\n","Business dataset shape: (150346, 14)\n","\n","Business Data Info:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 150346 entries, 0 to 150345\n","Data columns (total 14 columns):\n"," #   Column        Non-Null Count   Dtype  \n","---  ------        --------------   -----  \n"," 0   business_id   150346 non-null  object \n"," 1   name          150346 non-null  object \n"," 2   address       150346 non-null  object \n"," 3   city          150346 non-null  object \n"," 4   state         150346 non-null  object \n"," 5   postal_code   150346 non-null  object \n"," 6   latitude      150346 non-null  float64\n"," 7   longitude     150346 non-null  float64\n"," 8   stars         150346 non-null  float64\n"," 9   review_count  150346 non-null  int64  \n"," 10  is_open       150346 non-null  int64  \n"," 11  attributes    136602 non-null  object \n"," 12  categories    150243 non-null  object \n"," 13  hours         127123 non-null  object \n","dtypes: float64(3), int64(2), object(9)\n","memory usage: 16.1+ MB\n","\n","Business Data Head:\n","              business_id                      name  \\\n","0  Pns2l4eNsfO8kk83dixA6A  Abby Rappoport, LAC, CMQ   \n","1  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n","2  tUFrWirKiKi_TAnsVWINQQ                    Target   \n","3  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n","4  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n","\n","                           address           city state postal_code  \\\n","0           1616 Chapala St, Ste 2  Santa Barbara    CA       93101   \n","1  87 Grasso Plaza Shopping Center         Affton    MO       63123   \n","2             5255 E Broadway Blvd         Tucson    AZ       85711   \n","3                      935 Race St   Philadelphia    PA       19107   \n","4                    101 Walnut St     Green Lane    PA       18054   \n","\n","    latitude   longitude  stars  review_count  is_open  \\\n","0  34.426679 -119.711197    5.0             7        0   \n","1  38.551126  -90.335695    3.0            15        1   \n","2  32.223236 -110.880452    3.5            22        0   \n","3  39.955505  -75.155564    4.0            80        1   \n","4  40.338183  -75.471659    4.5            13        1   \n","\n","                                          attributes  \\\n","0                      {'ByAppointmentOnly': 'True'}   \n","1             {'BusinessAcceptsCreditCards': 'True'}   \n","2  {'BikeParking': 'True', 'BusinessAcceptsCredit...   \n","3  {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n","4  {'BusinessAcceptsCreditCards': 'True', 'Wheelc...   \n","\n","                                          categories  \\\n","0  Doctors, Traditional Chinese Medicine, Naturop...   \n","1  Shipping Centers, Local Services, Notaries, Ma...   \n","2  Department Stores, Shopping, Fashion, Home & G...   \n","3  Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n","4                          Brewpubs, Breweries, Food   \n","\n","                                               hours  \n","0                                               None  \n","1  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n","2  {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...  \n","3  {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...  \n","4  {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  \n","\n","Example Business Categories: Doctors, Traditional Chinese Medicine, Naturopathic/Holistic, Acupuncture, Health & Medical, Nutritionists\n"]}],"source":["# --- File Paths ---\n","# Mount Google Drive\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    # Adjust the file paths accordingly\n","    review_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_review.json'\n","    business_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/yelp_academic_dataset_business.json'\n","    print(\"Google Drive mounted.\")\n","    print(f\"Review data path: {review_file_path}\")\n","    print(f\"Business data path: {business_file_path}\")\n","except ModuleNotFoundError:\n","    # If not in Colab, use the local path directly (adjust if needed)\n","    review_file_path = r\"G:\\My Drive\\CMPS 6730 - NLP\\FinalProject\\yelp_academic_dataset_review.json\" # Use raw string\n","    business_file_path = r\"G:\\My Drive\\CMPS 6730 - NLP\\FinalProject\\yelp_academic_dataset_business.json\" # Use raw string\n","    print(\"Running locally. Ensure the file paths are correct.\")\n","    print(f\"Review data path: {review_file_path}\")\n","    print(f\"Business data path: {business_file_path}\")\n","\n","# --- Load Review Data ---\n","# Load the data (line-delimited JSON)\n","# Consider loading in chunks or using nrows for large files during development\n","print(f\"\\nLoading review data from: {review_file_path}\")\n","start_time = time.time()\n","try:\n","    # Using pd.read_json is generally faster if file format is consistent\n","    # Increase nrows if needed for more data, remove for full dataset\n","    df_reviews = pd.read_json(review_file_path, lines=True, nrows=1000000) # Load more reviews initially if memory allows\n","    # Note: Reading the full review file (~8GB) might require significant RAM or chunking.\n","except ValueError as e:\n","     print(f\"Error reading review JSON with pandas: {e}. Trying line-by-line.\")\n","     data = []\n","     # Limit lines read during line-by-line loading as well\n","     line_limit = 1000000 # Adjust as needed\n","     with open(review_file_path, 'r', encoding='utf-8') as f:\n","         for i, line in enumerate(f):\n","             if i < line_limit:\n","                 try:\n","                     data.append(json.loads(line))\n","                 except json.JSONDecodeError:\n","                     print(f\"Skipping malformed line {i+1} in reviews\")\n","                     continue\n","             else:\n","                 break\n","     df_reviews = pd.DataFrame(data)\n","\n","loading_time = time.time() - start_time\n","print(f\"Review data loaded in {loading_time:.2f} seconds.\")\n","print(f\"Review dataset shape: {df_reviews.shape}\")\n","print(\"\\nReview Data Info:\")\n","df_reviews.info()\n","print(\"\\nReview Data Head:\")\n","print(df_reviews.head())\n","\n","# Convert 'date' column to datetime objects\n","df_reviews['date'] = pd.to_datetime(df_reviews['date'])\n","print(\"\\n'date' column converted to datetime.\")\n","\n","# --- Load Business Data ---\n","print(f\"\\nLoading business data from: {business_file_path}\")\n","start_time = time.time()\n","try:\n","    df_business = pd.read_json(business_file_path, lines=True)\n","except ValueError as e:\n","    print(f\"Error reading business JSON with pandas: {e}. Trying line-by-line.\")\n","    business_data = []\n","    with open(business_file_path, 'r', encoding='utf-8') as f:\n","        for line in f:\n","            try:\n","                business_data.append(json.loads(line))\n","            except json.JSONDecodeError:\n","                print(f\"Skipping malformed line in businesses\")\n","                continue\n","    df_business = pd.DataFrame(business_data)\n","\n","loading_time = time.time() - start_time\n","print(f\"Business data loaded in {loading_time:.2f} seconds.\")\n","print(f\"Business dataset shape: {df_business.shape}\")\n","print(\"\\nBusiness Data Info:\")\n","df_business.info()\n","print(\"\\nBusiness Data Head:\")\n","print(df_business.head())\n","print(\"\\nExample Business Categories:\", df_business['categories'].iloc[0])"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1744591282748,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"YNowSd2VyJLF","outputId":"d3ae24bf-a9c4-4906-c328-f9ff89685127"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Businesses found in target cities (8 cities): 9125\n","Restaurants/Food businesses found in target location: 4099\n","Number of unique target restaurant business IDs: 4099\n","\n","Reviews filtered by date (2019-01-01 to 2024-12-31). Shape: (217126, 9)\n","Reviews filtered by target business IDs. Shape: (20377, 9)\n","\n","Final shape for processing (filtered by date and location/category): (20377, 5)\n","\n","Selected relevant columns ('review_id', 'text', 'stars', 'date', 'business_id').\n","                     review_id  \\\n","194093  3CmdoGKBZUX3Nb5IfbztMg   \n","208672  KY8dRN_k2EoR_QujKAegTQ   \n","209964  eetCBZ2roKWnKAGzBsC2qw   \n","210571  krP6fy3aJ4AwdoyOb1YlwQ   \n","214202  F0EkyfrHAaAc5DAJEWaDog   \n","\n","                                                     text  stars  \\\n","194093  My favorite coffee shop in New Orleans for sur...      5   \n","208672  Delete out at this Dave and busters was a litt...      3   \n","209964  Believe the hype. Great food at reasonable pri...      5   \n","210571  I love love this place! Fresh squeezed orange ...      5   \n","214202  Wow!  This place was a great find on our trip ...      5   \n","\n","                      date             business_id  \n","194093 2019-01-27 15:08:14  itAhmbhHOyQQparfwicjDQ  \n","208672 2019-02-14 17:06:51  UmjITdXHhEF46ho6IhaGQg  \n","209964 2019-02-28 02:17:13  vN6v8m4DO45Z4pp8yxxF_w  \n","210571 2019-02-03 16:40:01  vwOJ4yZH1I85-T3dJyr2ig  \n","214202 2019-03-12 01:48:31  0yGH62WGQy_W3JoR6L5Kew  \n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-1e8372d0df49>:17: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  businesses_in_location['categories'] = businesses_in_location['categories'].fillna('')\n"]}],"source":["# Filter data based on your criteria (date range AND location/category)\n","\n","# 1. Filter Businesses by Location and Category\n","\n","# Define target location (New Orleans Greater Metropolitan Area)\n","# List relevant cities. Convert to lowercase for case-insensitive matching.\n","target_cities = ['new orleans', 'metairie', 'kenner', 'gretna', 'harahan', 'westwego', 'chalmette', 'slidell'] # Add more as needed\n","# Convert city names in DataFrame to lowercase for consistent matching\n","df_business['city_lower'] = df_business['city'].str.lower()\n","\n","# Filter by city\n","businesses_in_location = df_business[df_business['city_lower'].isin(target_cities)]\n","print(f\"\\nBusinesses found in target cities ({len(target_cities)} cities): {businesses_in_location.shape[0]}\")\n","\n","# Filter by category (must contain 'Restaurants' or 'Food')\n","# Handle potential None/NaN values in 'categories'\n","businesses_in_location['categories'] = businesses_in_location['categories'].fillna('')\n","restaurants_in_location = businesses_in_location[\n","    businesses_in_location['categories'].str.contains('Restaurant|Food', case=False, regex=True)\n","].copy()\n","print(f\"Restaurants/Food businesses found in target location: {restaurants_in_location.shape[0]}\")\n","\n","# Get the business IDs of these restaurants\n","target_business_ids = set(restaurants_in_location['business_id'])\n","print(f\"Number of unique target restaurant business IDs: {len(target_business_ids)}\")\n","\n","if not target_business_ids:\n","    print(\"\\nWarning: No restaurant business IDs found for the specified location. Check city names and categories.\")\n","    # Handle this case - perhaps stop execution or proceed with only date filtering?\n","    # For now, we'll let it proceed, resulting in an empty review dataframe later.\n","\n","# 2. Filter Reviews by Date\n","start_date = datetime(2019, 1, 1)\n","end_date = datetime(2024, 12, 31) # Use end of 2024 for completeness\n","\n","df_reviews_filtered_date = df_reviews[(df_reviews['date'] >= start_date) & (df_reviews['date'] <= end_date)].copy()\n","print(f\"\\nReviews filtered by date ({start_date.date()} to {end_date.date()}). Shape: {df_reviews_filtered_date.shape}\")\n","\n","# 3. Filter Date-Filtered Reviews by Target Business IDs\n","if target_business_ids:\n","    df_filtered = df_reviews_filtered_date[\n","        df_reviews_filtered_date['business_id'].isin(target_business_ids)\n","    ].copy()\n","    print(f\"Reviews filtered by target business IDs. Shape: {df_filtered.shape}\")\n","else:\n","    print(\"\\nSkipping business ID filtering as no target IDs were found.\")\n","    df_filtered = pd.DataFrame(columns=df_reviews_filtered_date.columns) # Create empty DataFrame matching columns\n","\n","\n","# Select relevant columns for final processing\n","# Keep 'business_id' if you might need it later, otherwise drop it\n","df_processed = df_filtered[['review_id', 'text', 'stars', 'date', 'business_id']].copy()\n","\n","print(f\"\\nFinal shape for processing (filtered by date and location/category): {df_processed.shape}\")\n","\n","if df_processed.empty:\n","     print(\"\\nWARNING: No reviews match the filtering criteria (Date + Location/Category). Subsequent steps will fail.\")\n","     # Consider stopping execution here if the dataframe is empty.\n","else:\n","    print(\"\\nSelected relevant columns ('review_id', 'text', 'stars', 'date', 'business_id').\")\n","    print(df_processed.head())\n","\n","# --- Cleanup (Optional: remove intermediate dataframes to save memory) ---\n","# del df_reviews, df_business, df_reviews_filtered_date, businesses_in_location, restaurants_in_location\n","# import gc\n","# gc.collect()\n","# print(\"\\nIntermediate dataframes cleaned up.\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"dzRnsH9WoR5Z","executionInfo":{"status":"ok","timestamp":1744591282779,"user_tz":300,"elapsed":31,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30979,"status":"ok","timestamp":1744591313757,"user":{"displayName":"Aaron Dumont","userId":"02170992473081287978"},"user_tz":300},"id":"cjQ70p3A84vL","outputId":"12b06c63-3a55-465f-e6bc-61ed51d3730f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Downloading NLTK 'vader_lexicon'...\n","Downloading NLTK 'punkt_tab'...\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Output file with sentiment scores will be saved to: /content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv\n","\n","Starting sentiment analysis process on 20377 filtered reviews.\n","Analyzing review sentiments (this may take a while)...\n","Sentiment analysis complete.\n","\n","Successfully exported 20377 reviews with sentiment scores to:\n","/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv\n","\n","--- NEXT STEPS ---\n","1. Explore the calculated sentiment scores (e.g., correlations with 'stars').\n","2. Refine the aspect keyword lists for better classification accuracy.\n","3. Consider alternative sentiment lexicons or models if needed.\n","4. Use these sentiment scores as features for further analysis or modeling.\n","\n","Preview of DataFrame with Sentiment Scores:\n","                review_id             business_id                date  stars  \\\n","0  3CmdoGKBZUX3Nb5IfbztMg  itAhmbhHOyQQparfwicjDQ 2019-01-27 15:08:14      5   \n","1  KY8dRN_k2EoR_QujKAegTQ  UmjITdXHhEF46ho6IhaGQg 2019-02-14 17:06:51      3   \n","2  eetCBZ2roKWnKAGzBsC2qw  vN6v8m4DO45Z4pp8yxxF_w 2019-02-28 02:17:13      5   \n","3  krP6fy3aJ4AwdoyOb1YlwQ  vwOJ4yZH1I85-T3dJyr2ig 2019-02-03 16:40:01      5   \n","4  F0EkyfrHAaAc5DAJEWaDog  0yGH62WGQy_W3JoR6L5Kew 2019-03-12 01:48:31      5   \n","\n","                                                text  food_sentiment  \\\n","0  My favorite coffee shop in New Orleans for sur...        1.607300   \n","1  Delete out at this Dave and busters was a litt...        0.409773   \n","2  Believe the hype. Great food at reasonable pri...        0.926950   \n","3  I love love this place! Fresh squeezed orange ...        0.514760   \n","4  Wow!  This place was a great find on our trip ...        0.000000   \n","\n","   service_sentiment  ambiance_sentiment  price_sentiment  context_sentiment  \\\n","0           0.000000             0.00000           0.0000            0.00000   \n","1           0.276291             0.00000           0.0657            0.00000   \n","2           0.000000             0.00000           0.0000            0.00000   \n","3           0.000000             0.18202           0.0000            0.14354   \n","4           0.000000             0.13616           0.0000            0.12498   \n","\n","   other_sentiment  \n","0         0.000000  \n","1        -0.324109  \n","2         0.448900  \n","3         0.173100  \n","4         0.249560  \n"]}],"source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import re # For basic text cleaning\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","\n","# --- Download NLTK data (only needs to be done once) ---\n","!pip install nltk\n","import nltk\n","\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError: # Changed to catch LookupError\n","    print(\"Downloading NLTK 'punkt' tokenizer...\")\n","    nltk.download('punkt', quiet=True)\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError: # Changed to catch LookupError\n","    print(\"Downloading NLTK 'stopwords'...\")\n","    nltk.download('stopwords', quiet=True)\n","try:\n","    nltk.data.find('sentiment/vader_lexicon.zip')\n","except LookupError: # Changed to catch LookupError\n","    print(\"Downloading NLTK 'vader_lexicon'...\")\n","    nltk.download('vader_lexicon', quiet=True)\n","\n","try:\n","    nltk.data.find('tokenizers/punkt_tab') # Check if the resource is already present\n","except LookupError:\n","    print(\"Downloading NLTK 'punkt_tab'...\")\n","    nltk.download('punkt_tab') # Download specifically 'punkt_tab'\n","\n","\n","# --- Configuration ---\n","# Define the path to your processed data (assuming df_processed is loaded elsewhere)\n","# If df_processed is not loaded, you'll need to load it first, e.g.:\n","# input_data_file = 'path/to/your/processed_reviews.csv'\n","# df_processed = pd.read_csv(input_data_file)\n","\n","# Define where to save the file with sentiment scores\n","try:\n","    # Assume Google Drive is mounted if in Colab\n","    output_sentiment_file = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n","    print(f\"Output file with sentiment scores will be saved to: {output_sentiment_file}\")\n","except NameError: # If 'drive' object doesn't exist (running locally)\n","    # Adjust local path if necessary\n","    output_sentiment_file = r\"G:\\My Drive\\CMPS 6730 - NLP\\FinalProject\\reviews_with_sentiment_scores.csv\"\n","    print(f\"Output file with sentiment scores will be saved to: {output_sentiment_file}\")\n","\n","# --- Aspect Keyword Lists (Illustrative Examples - EXPAND THESE SIGNIFICANTLY) ---\n","# Based on the paper, these lists should be comprehensive.\n","# Consider synonyms, related terms, common phrases.\n","\n","# --- Aspect Keyword Lists (EXPANDED - CONTINUE ADDING MORE!) ---\n","aspect_keywords = {\n","    'food': [\n","        # General Food\n","        'food', 'dish', 'meal', 'plate', 'menu', 'cuisine', 'recipe', 'ingredients', 'eat', 'ate', 'dining',\n","        # Taste & Quality\n","        'taste', 'flavor', 'delicious', 'tasty', 'yummy', 'savory', 'sweet', 'sour', 'bitter', 'spicy', 'hot',\n","        'fresh', 'quality', 'authentic', 'homemade', 'cooked', 'preparation', 'appetizer', 'entree', 'dessert',\n","        'portion', 'serving', 'burnt', 'undercooked', 'overcooked', 'bland', 'seasoning',\n","        # Specific Items (Examples - Add many more common ones)\n","        'chicken', 'beef', 'pork', 'fish', 'seafood', 'shrimp', 'crab', 'lobster', 'steak', 'burger', 'sandwich',\n","        'pizza', 'pasta', 'sushi', 'taco', 'salad', 'soup', 'bread', 'fries', 'rice', 'noodles', 'vegetables',\n","        'cake', 'pie', 'ice cream', 'coffee', 'tea', 'drink', 'beverage', 'wine', 'beer', 'cocktail'\n","    ],\n","    'service': [\n","        # Staff General\n","        'service', 'staff', 'server', 'waiter', 'waitress', 'waitstaff', 'host', 'hostess', 'bartender', 'manager', 'employee',\n","        # Staff Behavior\n","        'friendly', 'attentive', 'helpful', 'professional', 'polite', 'courteous', 'welcoming', 'accommodating',\n","        'rude', 'unfriendly', 'inattentive', 'slow', 'ignored', 'forgetful', 'unprofessional', 'arrogant', 'bad', 'terrible'\n","        # Process\n","        'wait', 'waiting', 'order', 'refill', 'check', 'bill', 'reservation', 'seated', 'prompt', 'quick', 'efficient',\n","        'mistake', 'error', 'issue', 'problem', 'complaint', 'request'\n","    ],\n","    'ambiance': [\n","        # General Feel\n","        'ambiance', 'atmosphere', 'vibe', 'decor', 'setting', 'environment', 'interior', 'design', 'layout',\n","        # Sensory\n","        'music', 'lighting', 'loud', 'noisy', 'quiet', 'sound', 'smell',\n","        # Comfort & Cleanliness\n","        'comfortable', 'cozy', 'relaxing', 'upscale', 'casual', 'romantic', 'view',\n","        'clean', 'dirty', 'tidy', 'messy', 'hygiene', 'restroom', 'bathroom', 'tables',\n","        # Space\n","        'crowded', 'spacious', 'seating', 'booth', 'patio', 'outdoor'\n","    ],\n","    'price': [\n","        # General Cost\n","        'price', 'cost', 'value', 'money', 'budget', 'bill', 'charge', 'worth', 'pay', 'paid', 'tab',\n","        # Affordability\n","        'cheap', 'expensive', 'affordable', 'overpriced', 'reasonable', 'deal', 'bargain', 'pricey', 'costly',\n","        'value for money', 'rip-off', 'discount', 'coupon', 'special'\n","    ],\n","    'context': [\n","        # Occasion\n","        'occasion', 'birthday', 'anniversary', 'celebration', 'date', 'romantic', 'special', 'holiday',\n","        # Company\n","        'friends', 'family', 'kids', 'children', 'group', 'party', 'business', 'work', 'solo', 'couple',\n","        # Time/Meal Type\n","        'lunch', 'dinner', 'brunch', 'breakfast', 'late night', 'happy hour',\n","        # Location Related (can overlap with ambiance)\n","        'location', 'neighborhood', 'parking', 'visit', 'trip', 'tourist'\n","     ]\n","}\n","# --- Initialize Sentiment Analyzer ---\n","analyzer = SentimentIntensityAnalyzer()\n","stop_words = set(stopwords.words('english'))\n","\n","# --- Helper Functions ---\n","\n","def clean_text(text):\n","    \"\"\"Basic text cleaning.\"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    text = text.lower() # Lowercase\n","    text = re.sub(r'\\d+', '', text) # Remove numbers\n","    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n","    text = text.strip() # Remove leading/trailing whitespace\n","    return text\n","\n","def classify_sentence_aspect(sentence, aspect_kw):\n","    \"\"\"\n","    Classifies a sentence into one aspect based on keyword counts.\n","    Follows the paper's method: assign to aspect with most keywords.\n","    \"\"\"\n","    # Clean and tokenize the sentence\n","    cleaned_sentence = clean_text(sentence)\n","    words = word_tokenize(cleaned_sentence)\n","    # Remove stop words for better keyword matching\n","    words = [word for word in words if word not in stop_words]\n","\n","    scores = {aspect: 0 for aspect in aspect_kw}\n","    max_score = 0\n","    best_aspect = 'other' # Default if no keywords match\n","\n","    # Count keywords for each aspect\n","    for aspect, keywords in aspect_kw.items():\n","        # Use a set for faster checking\n","        keyword_set = set(keywords)\n","        # Count occurrences of keywords in the sentence words\n","        score = sum(1 for word in words if word in keyword_set)\n","        scores[aspect] = score\n","        if score > max_score:\n","            max_score = score\n","            best_aspect = aspect\n","        # Handle ties (optional, could assign to 'multiple' or prioritize)\n","        # elif score == max_score and max_score > 0:\n","        #     best_aspect = 'multiple' # Or handle differently\n","\n","    # Ensure we only classify if at least one keyword was found\n","    if max_score == 0:\n","        best_aspect = 'other'\n","\n","    return best_aspect\n","\n","def get_sentence_sentiment(sentence):\n","    \"\"\"\n","    Calculates the VADER compound sentiment score for a sentence.\n","    Ranges from -1 (most negative) to +1 (most positive).\n","    The paper used AFINN (-5 to +5), VADER is a common alternative.\n","    \"\"\"\n","    # VADER's polarity_scores returns dict: {'neg': %, 'neu': %, 'pos': %, 'compound': score}\n","    vs = analyzer.polarity_scores(sentence)\n","    return vs['compound'] # Use the compound score as the overall sentiment\n","\n","def analyze_review_sentiment(review_text, aspect_kw):\n","    \"\"\"\n","    Analyzes a full review text to get weighted sentiment scores per aspect.\n","    Implements the methodology from the paper.\n","    \"\"\"\n","    if not isinstance(review_text, str) or not review_text.strip():\n","        # Return default scores for empty or invalid reviews\n","        return {f'{aspect}_sentiment': 0.0 for aspect in aspect_kw} | {'other_sentiment': 0.0}\n","\n","    # 1. Tokenize into sentences\n","    sentences = sent_tokenize(review_text)\n","    total_sentences = len(sentences)\n","\n","    if total_sentences == 0:\n","        # Handle reviews that couldn't be sentence-tokenized\n","         return {f'{aspect}_sentiment': 0.0 for aspect in aspect_kw} | {'other_sentiment': 0.0}\n","\n","    # 2. Classify aspect and get sentiment for each sentence\n","    sentence_results = []\n","    for sentence in sentences:\n","        aspect = classify_sentence_aspect(sentence, aspect_kw)\n","        sentiment = get_sentence_sentiment(sentence)\n","        sentence_results.append({'aspect': aspect, 'sentiment': sentiment})\n","\n","    # 3. Aggregate sentiment scores and count sentences per aspect\n","    aspect_sentiments_sum = defaultdict(float)\n","    aspect_sentence_counts = defaultdict(int)\n","\n","    for result in sentence_results:\n","        aspect = result['aspect']\n","        sentiment = result['sentiment']\n","        aspect_sentiments_sum[aspect] += sentiment\n","        aspect_sentence_counts[aspect] += 1\n","\n","    # 4. Calculate weighted sentiment scores (as per paper's formula)\n","    weighted_scores = {}\n","    all_aspects = list(aspect_kw.keys()) + ['other'] # Include 'other' category\n","\n","    for aspect in all_aspects:\n","        sum_score = aspect_sentiments_sum[aspect]\n","        count = aspect_sentence_counts[aspect]\n","\n","        # Calculate weight (proportion of sentences for this aspect)\n","        weight = count / total_sentences if total_sentences > 0 else 0\n","\n","        # Weighted score = Sum * Weight (or proportion)\n","        # Note: The paper's formula description is slightly ambiguous.\n","        # Sentiment_ij = Sentiment Score_ij * (# sentences Attribute j / # sentences Attribute_ij)\n","        # This implies weighting the *average* score per attribute.\n","        # Let's calculate both average and weighted sum for clarity.\n","        # average_score = sum_score / count if count > 0 else 0.0\n","        # weighted_score_avg_based = average_score * weight # Avg score weighted by proportion\n","\n","        # Alternative interpretation: Weight the *total* sentiment sum for the aspect\n","        # This seems more aligned with capturing overall impact.\n","        weighted_score_sum_based = sum_score * weight\n","\n","        # Store the weighted score based on the sum interpretation\n","        weighted_scores[f'{aspect}_sentiment'] = weighted_score_sum_based\n","\n","    return weighted_scores\n","\n","def get_overall_sentiment(review_text):  # <-- Add this function here\n","    \"\"\"Calculates the overall sentiment score for the entire review.\"\"\"\n","    if not isinstance(review_text, str) or not review_text.strip():\n","        return 0.0  # Return 0 for empty or invalid reviews\n","\n","    # Use VADER to get the compound sentiment score\n","    scores = analyzer.polarity_scores(review_text)\n","    return scores['compound']\n","\n","# --- Main Processing Logic ---\n","\n","# Check if df_processed exists and is populated\n","# IMPORTANT: Ensure df_processed is loaded before this point if not already in memory.\n","# Example:\n","# try:\n","#     df_processed = pd.read_csv('path/to/your/filtered_reviews.csv')\n","#     print(f\"Loaded {len(df_processed)} reviews.\")\n","# except FileNotFoundError:\n","#     print(\"Error: Processed review file not found.\")\n","#     df_processed = pd.DataFrame() # Create empty df to avoid error\n","\n","if 'df_processed' not in locals() or not isinstance(df_processed, pd.DataFrame) or df_processed.empty:\n","    print(\"\\nError: The 'df_processed' DataFrame is empty or does not exist.\")\n","    print(\"Please ensure your data loading step ran successfully.\")\n","    # Optionally, raise an error:\n","    # raise ValueError(\"Cannot proceed without data in df_processed.\")\n","else:\n","    print(f\"\\nStarting sentiment analysis process on {len(df_processed)} filtered reviews.\")\n","\n","    # Make a copy to avoid modifying the original DataFrame directly\n","    df_analysis = df_processed.copy()\n","\n","    # --- Apply Sentiment Analysis ---\n","    print(\"Analyzing review sentiments (this may take a while)...\")\n","\n","    # Use .apply() to process the 'text' column of each review\n","    # The result of apply will be a Series of dictionaries\n","    sentiment_results = df_analysis['text'].apply(lambda text: analyze_review_sentiment(text, aspect_keywords))\n","\n","    # Convert the Series of dictionaries into separate columns in the DataFrame\n","    # pd.json_normalize is efficient for this\n","    sentiment_df = pd.json_normalize(sentiment_results)\n","\n","    # Add these new sentiment score columns to the analysis DataFrame\n","    df_analysis = pd.concat([df_analysis.reset_index(drop=True), sentiment_df.reset_index(drop=True)], axis=1)\n","\n","    print(\"Sentiment analysis complete.\")\n","\n","    # --- Select and Reorder Columns for Output ---\n","    # Keep original identifiers and add the new sentiment scores\n","    output_columns = ['review_id', 'business_id', 'date', 'stars', 'text'] + \\\n","                     [f'{aspect}_sentiment' for aspect in aspect_keywords] + \\\n","                     ['other_sentiment'] # Include 'other' if calculated\n","\n","    # Ensure all expected columns exist before selecting\n","    final_columns = [col for col in output_columns if col in df_analysis.columns]\n","    df_final_output = df_analysis[final_columns]\n","\n","    # --- Export to CSV ---\n","    try:\n","        # Use index=False to avoid writing the DataFrame index as a column\n","        df_final_output.to_csv(output_sentiment_file, index=False, encoding='utf-8')\n","        print(f\"\\nSuccessfully exported {len(df_final_output)} reviews with sentiment scores to:\")\n","        print(output_sentiment_file)\n","        print(\"\\n--- NEXT STEPS ---\")\n","        print(\"1. Explore the calculated sentiment scores (e.g., correlations with 'stars').\")\n","        print(\"2. Refine the aspect keyword lists for better classification accuracy.\")\n","        print(\"3. Consider alternative sentiment lexicons or models if needed.\")\n","        print(\"4. Use these sentiment scores as features for further analysis or modeling.\")\n","\n","    except Exception as e:\n","        print(f\"\\nError exporting file: {e}\")\n","        print(\"Please check the output path and ensure you have write permissions.\")\n","\n","# Optional: Display the first few rows with the new scores\n","if 'df_final_output' in locals():\n","    print(\"\\nPreview of DataFrame with Sentiment Scores:\")\n","    print(df_final_output.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSAHlEOvyNlH","outputId":"f576be45-3959-42c8-87ab-fcd50909a9aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting text preprocessing...\n"]}],"source":["# Initialize lemmatizer and stopwords\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","import re\n","import time\n","\n","# Download the necessary NLTK data if it's not already present\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    nltk.download('punkt_tab')\n","\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text(text):\n","    \"\"\"Cleans and preprocesses text data.\"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    # Lowercase\n","    text = text.lower()\n","    # Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","    # Remove user @ references and '#' from tweet\n","    text = re.sub(r'\\@\\w+|\\#','', text)\n","    # Remove punctuation\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    # Tokenize\n","    tokens = word_tokenize(text)\n","    # Remove stopwords and lemmatize\n","    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word.isalpha()]\n","    return ' '.join(processed_tokens)\n","\n","# Apply preprocessing (can take time on large datasets)\n","print(\"\\nStarting text preprocessing...\")\n","start_time = time.time()\n","df_processed['processed_text'] = df_processed['text'].apply(preprocess_text)\n","processing_time = time.time() - start_time\n","print(f\"Text preprocessing completed in {processing_time:.2f} seconds.\")\n","\n","# Display some processed text examples\n","print(\"\\nOriginal vs Processed Text Examples:\")\n","for i in range(3):\n","    print(f\"--- Example {i+1} ---\")\n","    print(\"Original:\", df_processed['text'].iloc[i][:200] + \"...\") # Show first 200 chars\n","    print(\"Processed:\", df_processed['processed_text'].iloc[i])\n","    print(\"-\" * 20)\n","\n","# Drop rows where processed text is empty\n","df_processed = df_processed[df_processed['processed_text'] != '']\n","print(f\"\\nShape after removing empty processed texts: {df_processed.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsTW9F96yahm"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer  # Import SentimentIntensityAnalyzer\n","\n","\n","# Initialize SentimentIntensityAnalyzer (if not already initialized)\n","analyzer = SentimentIntensityAnalyzer()\n","\n","def get_overall_sentiment(review_text):  # <-- Add this function here\n","    \"\"\"Calculates the overall sentiment score for the entire review.\"\"\"\n","    if not isinstance(review_text, str) or not review_text.strip():\n","        return 0.0  # Return 0 for empty or invalid reviews\n","\n","    # Use VADER to get the compound sentiment score\n","    scores = analyzer.polarity_scores(review_text)\n","    return scores['compound']\n","# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n","\n","# 1. Load or calculate sentiment scores if not in df_processed\n","try:\n","    # If sentiment scores are already in df_processed, use them\n","    sentiment_columns = ['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']\n","    Y = df_processed[sentiment_columns].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","except KeyError:\n","    print(\"Sentiment score columns not found in df_processed. Calculating or loading them...\")\n","    # Calculate overall sentiment for each review\n","    df_processed['overall_sentiment'] = df_processed['text'].apply(get_overall_sentiment)\n","    # Option 2: Load from a separate file (if you saved them earlier)\n","    from google.colab import drive\n","    drive.mount('/content/drive')  # Mount Google Drive\n","    sentiment_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n","\n","    sentiment_df = pd.read_csv(sentiment_file_path)\n","    # Merge sentiment scores with the original dataframe based on a common column (e.g., 'review_id')\n","    df_processed = pd.merge(\n","        df_processed,\n","        sentiment_df[['review_id', 'food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment']],\n","        on='review_id',\n","        how='left'\n","    )\n","    Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']].copy()\n","\n","# --- 2. Correct sentiment binning using fixed thresholds ---\n","\n","# Define fixed VADER thresholds and corresponding integer labels\n","# Bins: (-inf, -0.05], (-0.05, 0.05), [0.05, +inf)\n","# We add +/- infinity to ensure all values are covered.\n","bin_edges = [-float('inf'), -0.05, 0.05, float('inf')]\n","# Assign integer labels: 0 for Negative, 1 for Neutral, 2 for Positive\n","bin_labels = [0, 1, 2]\n","label_names = ['negative', 'neutral', 'positive'] # For potential use in reports if needed\n","\n","print(f\"Using fixed bin edges: {bin_edges}\")\n","print(f\"Assigning integer labels: {bin_labels} ({', '.join(label_names)})\")\n","\n","# Ensure Y is a DataFrame (it should be from previous steps)\n","if not isinstance(Y, pd.DataFrame):\n","     raise TypeError(\"Y should be a pandas DataFrame at this stage.\")\n","\n","# Create a new DataFrame for binned labels to avoid modifying Y inplace initially\n","Y_binned = pd.DataFrame(index=Y.index)\n","\n","for column in Y.columns:\n","    # Cast column values to float64 explicitly to avoid dtype issues\n","    # Use .loc to avoid SettingWithCopyWarning if Y is a slice\n","    Y_column_float = Y.loc[:, column].astype(float)\n","\n","    # Use pd.cut for binning with fixed edges\n","    # include_lowest=True: includes the lowest value (-inf) in the first bin\n","    # right=True (default): bins are (edge1, edge2], except first which is [edge1, edge2] due to include_lowest\n","    # If you want bins like [edge1, edge2), use right=False\n","    binned_data = pd.cut(\n","        Y_column_float,\n","        bins=bin_edges,\n","        labels=bin_labels,\n","        include_lowest=True,\n","        right=True # Standard VADER thresholds often use >= 0.05 for positive, <= -0.05 for negative\n","    )\n","\n","    # Check for NaNs introduced by binning (shouldn't happen with inf edges, but good practice)\n","    if binned_data.isnull().any():\n","        print(f\"Warning: NaNs found in '{column}' after binning. Check original data.\")\n","        # Handle NaNs if necessary, e.g., fill with neutral=1 or drop rows\n","        # binned_data = binned_data.fillna(1) # Example: Fill NaN with Neutral\n","\n","    # Assign the binned data (as integers) to the new DataFrame\n","    Y_binned[column] = binned_data.astype(int)\n","\n","# Replace the original Y with the binned version\n","Y = Y_binned\n","print(\"\\nSentiment scores binned using fixed thresholds:\")\n","print(Y.head())\n","print(\"\\nValue counts for 'food_sentiment' (example):\")\n","print(Y['food_sentiment'].value_counts())\n","\n","# --- End of binning modification ---\n","\n","# 3. Prepare data for training and testing\n","# Assuming 'processed_text' column contains preprocessed text data\n","X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n","\n","# Get the first sentiment column for stratification\n","stratify_column = Y.iloc[:, 0]  # Assuming Y is a DataFrame\n","\n","# Split data into training and testing sets\n","X_train, X_test, Y_train, Y_test = train_test_split(\n","    X, Y, test_size=0.2, random_state=42, stratify=stratify_column\n",")\n","\n","# Print dataset sizes\n","print(f\"Training set size: {len(X_train)}\")\n","print(f\"Testing set size: {len(X_test)}\")\n","\n","# ... (rest of the code, including TF-IDF, tokenization, and model training) ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfCwazyuyeeZ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n","\n","\n","\n","# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n","\n","# 1. Load or calculate sentiment scores if not in df_processed\n","# ... (Your existing code for loading or calculating sentiment scores) ...\n","\n","\n","# 2. Correct quantile binning and discretization of sentiment scores\n","# ... (Your existing code for binning and discretization) ...\n","\n","\n","# 3. Prepare data for training and testing\n","# Assuming 'processed_text' column contains preprocessed text data\n","X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n","\n","# ... (Your existing code for splitting data into train/test sets) ...\n","\n","\n","# --- TF-IDF Vectorization ---\n","# Create a TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n","\n","# Fit the vectorizer to the training data and transform it\n","X_train_tfidf = vectorizer.fit_transform(X_train)\n","\n","# Transform the testing data using the fitted vectorizer\n","X_test_tfidf = vectorizer.transform(X_test)\n","\n","# --- Baseline Model Training and Evaluation ---\n","# Using OneVsRestClassifier: Trains one classifier per class (or per output)\n","# Here, we wrap Logistic Regression. One LR model will be trained for each aspect.\n","print(\"\\nTraining Baseline Model (Logistic Regression with OneVsRest)...\")\n","start_time = time.time()\n","\n","# Create a base estimator\n","log_reg = LogisticRegression(solver='liblinear', random_state=42) # Liblinear often good for high-dim sparse data\n","\n","# Wrap it with OneVsRestClassifier for multi-label/multi-output scenario\n","# It trains one classifier per column in Y\n","baseline_model = MultiOutputClassifier(log_reg) # Simpler API for multi-output integer targets\n","\n","# Train the model\n","baseline_model.fit(X_train_tfidf, Y_train) # Use the numeric labels DataFrame/Array\n","\n","# ... (Rest of your code for baseline evaluation) ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U1W_x4T5ylgS"},"outputs":[],"source":["# --- Import necessary libraries ---\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer  # Import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW  # Correct import for AdamW\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    get_linear_schedule_with_warmup,\n","    DistilBertTokenizer,  # Example using DistilBERT\n","    DistilBertForSequenceClassification\n",")\n","import time\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","# Initialize SentimentIntensityAnalyzer\n","analyzer = SentimentIntensityAnalyzer()\n","\n","# Assuming df_processed is already loaded and contains the preprocessed text data in the 'processed_text' column\n","\n","# 1. Load or calculate sentiment scores if not in df_processed\n","try:\n","    # If sentiment scores are already in df_processed, use them\n","    sentiment_columns = ['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']\n","    Y = df_processed[sentiment_columns].copy()  # Use .copy() to avoid SettingWithCopyWarning\n","except KeyError:\n","    print(\"Sentiment score columns not found in df_processed. Calculating or loading them...\")\n","    # Option 2: Load from a separate file (if you saved them earlier)\n","    from google.colab import drive\n","    drive.mount('/content/drive')  # Mount Google Drive\n","    sentiment_file_path = '/content/drive/My Drive/CMPS 6730 - NLP/FinalProject/reviews_with_sentiment_scores.csv'\n","\n","    sentiment_df = pd.read_csv(sentiment_file_path)\n","    # Merge sentiment scores with the original dataframe based on a common column (e.g., 'review_id')\n","    df_processed = pd.merge(\n","        df_processed,\n","        sentiment_df[['review_id', 'food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']],\n","        on='review_id',\n","        how='left'\n","    )\n","    Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']].copy()\n","\n","# Calculate overall sentiment for each review\n","df_processed['overall_sentiment'] = df_processed['text'].apply(get_overall_sentiment)\n","\n","# Now include 'overall_sentiment' in your target columns\n","Y = df_processed[['food_sentiment', 'service_sentiment', 'ambiance_sentiment', 'price_sentiment', 'context_sentiment', 'overall_sentiment']].copy()\n","\n","# --- 2. Correct sentiment binning using fixed thresholds ---\n","\n","# Define fixed VADER thresholds and corresponding integer labels\n","# Bins: (-inf, -0.05], (-0.05, 0.05), [0.05, +inf)\n","# We add +/- infinity to ensure all values are covered.\n","bin_edges = [-float('inf'), -0.05, 0.05, float('inf')]\n","# Assign integer labels: 0 for Negative, 1 for Neutral, 2 for Positive\n","bin_labels = [0, 1, 2]\n","label_names = ['negative', 'neutral', 'positive'] # For potential use in reports if needed\n","\n","print(f\"Using fixed bin edges: {bin_edges}\")\n","print(f\"Assigning integer labels: {bin_labels} ({', '.join(label_names)})\")\n","\n","# Ensure Y is a DataFrame (it should be from previous steps)\n","if not isinstance(Y, pd.DataFrame):\n","     raise TypeError(\"Y should be a pandas DataFrame at this stage.\")\n","\n","# Create a new DataFrame for binned labels to avoid modifying Y inplace initially\n","Y_binned = pd.DataFrame(index=Y.index)\n","\n","for column in Y.columns:\n","    # Cast column values to float64 explicitly to avoid dtype issues\n","    # Use .loc to avoid SettingWithCopyWarning if Y is a slice\n","    Y_column_float = Y.loc[:, column].astype(float)\n","\n","    # Use pd.cut for binning with fixed edges\n","    # include_lowest=True: includes the lowest value (-inf) in the first bin\n","    # right=True (default): bins are (edge1, edge2], except first which is [edge1, edge2] due to include_lowest\n","    # If you want bins like [edge1, edge2), use right=False\n","    binned_data = pd.cut(\n","        Y_column_float,\n","        bins=bin_edges,\n","        labels=bin_labels,\n","        include_lowest=True,\n","        right=True # Standard VADER thresholds often use >= 0.05 for positive, <= -0.05 for negative\n","    )\n","\n","    # Check for NaNs introduced by binning (shouldn't happen with inf edges, but good practice)\n","    if binned_data.isnull().any():\n","        print(f\"Warning: NaNs found in '{column}' after binning. Check original data.\")\n","        # Handle NaNs if necessary, e.g., fill with neutral=1 or drop rows\n","        # binned_data = binned_data.fillna(1) # Example: Fill NaN with Neutral\n","\n","    # Assign the binned data (as integers) to the new DataFrame\n","    Y_binned[column] = binned_data.astype(int)\n","\n","# Replace the original Y with the binned version\n","Y = Y_binned\n","print(\"\\nSentiment scores binned using fixed thresholds:\")\n","print(Y.head())\n","print(\"\\nValue counts for 'food_sentiment' (example):\")\n","print(Y['food_sentiment'].value_counts())\n","\n","# --- End of binning modification ---\n","\n","# 3. Prepare data for training and testing\n","# Assuming 'processed_text' column contains preprocessed text data\n","X = df_processed['processed_text'].copy()  # Use .copy() to ensure a deep copy\n","\n","# Get the first sentiment column for stratification\n","stratify_column = Y.iloc[:, 0]  # Assuming Y is a DataFrame\n","\n","# Split data into training and testing sets\n","X_train, X_test, Y_train, Y_test = train_test_split(\n","    X, Y, test_size=0.2, random_state=42, stratify=stratify_column\n",")\n","\n","# Print dataset sizes\n","print(f\"Training set size: {len(X_train)}\")\n","print(f\"Testing set size: {len(X_test)}\")\n","\n","\n","# --- Tokenization ---\n","# Initialize tokenizer (if not done earlier)\n","model_name = 'distilbert-base-uncased'  # Or your preferred model name\n","tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n","\n","# Tokenize and encode the text data\n","train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n","test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n","\n","# --- Convert Y_train and Y_test to NumPy arrays ---\n","Y_train_np = Y_train.values  # Assuming Y_train is a pandas DataFrame\n","Y_test_np = Y_test.values   # Assuming Y_test is a pandas DataFrame\n","\n","\n","# --- Define PyTorch Dataset ---\n","class YelpAspectDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        # Ensure labels are shaped correctly (N_samples, N_aspects)\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        # Handle cases where encodings might be lists of tensors or similar structures\n","        item = {}\n","        for key, val in self.encodings.items():\n","            # Ensure the value associated with the key is indexable and convert to tensor\n","            if isinstance(val, list):\n","                item[key] = torch.tensor(val[idx])\n","            else:  # Assuming it might already be a tensor or numpy array\n","                item[key] = torch.tensor(val[idx]).clone().detach()  # Make sure it's a tensor\n","\n","        # Labels should be FloatTensor for BCEWithLogitsLoss or LongTensor for CrossEntropyLoss\n","        # For multi-label (predicting probability for each class per aspect) or multi-output (predicting one class per aspect)\n","        # Let's assume multi-output: predicting one class (0, 1, 2) per aspect. Labels are LongTensor.\n","        # Shape should be (num_aspects,) for a single item\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","    def __len__(self):\n","        # Use the number of samples in one of the encoding keys (e.g., 'input_ids')\n","        return len(self.encodings['input_ids'])\n","\n","\n","# Create datasets\n","train_dataset = YelpAspectDataset(train_encodings, Y_train_np)\n","test_dataset = YelpAspectDataset(test_encodings, Y_test_np)\n","\n","# --- Define Model ---\n","# We need a multi-output classification head.\n","# Load pre-trained DistilBERT and modify the classifier layer.\n","aspects = ['food', 'service', 'ambiance', 'price', 'context', 'overall']  # Define your aspects\n","sentiment_classes = ['negative', 'neutral', 'positive']  # Define sentiment classes\n","\n","num_aspects = len(aspects)\n","num_classes_per_aspect = len(sentiment_classes)  # 3 classes: positive, negative, neutral\n","\n","# Load the base model\n","model = DistilBertForSequenceClassification.from_pretrained(\n","    model_name,\n","    num_labels=num_aspects * num_classes_per_aspect  # TEMPORARY - see below\n",")\n","\n","# *** IMPORTANT: Modifying the Classifier Head for Multi-Output ***\n","# The standard `DistilBertForSequenceClassification` has ONE output layer for num_labels.\n","# For multi-output (one prediction per aspect), we need to customize.\n","# Option 1: Treat as N independent classification tasks (simpler to implement with standard model if labels handled outside)\n","# Option 2: Create N classification heads (more complex custom model)\n","# Option 3: Reshape the output of a single large head (requires careful loss calculation)\n","\n","# Let's try Option 3 conceptually: A single head predicting scores for all aspect-class combinations.\n","# Then reshape and calculate loss per aspect.\n","# The output dimension should be num_aspects * num_classes_per_aspect\n","# The final layer needs to be replaced or adapted.\n","\n","# Get the original classifier's input dimension\n","original_classifier_in_features = model.classifier.in_features\n","\n","# Replace the classifier head. Output size is num_aspects * num_classes (e.g., 5 * 3 = 15)\n","model.classifier = torch.nn.Linear(original_classifier_in_features, num_aspects * num_classes_per_aspect)\n","# The pre_classifier layer might also need adjustment if present (DistilBERT has one)\n","if hasattr(model, 'pre_classifier') and model.pre_classifier is not None:\n","    original_pre_classifier_in_features = model.pre_classifier.in_features\n","    model.pre_classifier = torch.nn.Linear(original_pre_classifier_in_features, original_classifier_in_features)  # Keep standard pre-classifier -> classifier connection\n","else:\n","    print(\"Model does not have a separate pre_classifier layer.\")\n","\n","print(f\"\\nCustomized DistilBERT model loaded. Output layer size: {model.classifier.out_features}\")\n","\n","# --- Training Setup ---\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","print(f\"Using device: {device}\")\n","model.to(device)\n","\n","# Dataloaders\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)  # Adjust batch_size based on GPU memory\n","test_loader = DataLoader(test_dataset, batch_size=32)  # Larger batch size for evaluation is fine\n","\n","# Optimizer and Scheduler\n","optimizer = AdamW(model.parameters(), lr=5e-5)  # Typical learning rate for fine-tuning\n","num_epochs = 3  # Adjust as needed (start small)\n","num_training_steps = num_epochs * len(train_loader)\n","lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","# Loss Function: CrossEntropyLoss is suitable for multi-class classification (per aspect)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# --- Training Loop ---\n","print(\"\\nStarting Transformer Model Training...\")\n","model.train()\n","for epoch in range(num_epochs):\n","    print(f\"--- Epoch {epoch + 1}/{num_epochs} ---\")\n","    epoch_start_time = time.time()\n","    total_loss = 0\n","    for i, batch in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        # Move batch to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)  # Shape: (batch_size, num_aspects)\n","\n","        # Forward pass\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Shape: (batch_size, num_aspects * num_classes_per_aspect)\n","\n","        # Calculate loss: Reshape logits and labels for CrossEntropyLoss\n","        # Logits need to be (batch_size * num_aspects, num_classes_per_aspect)\n","        # Labels need to be (batch_size * num_aspects,)\n","        reshaped_logits = logits.view(-1, num_classes_per_aspect)  # (batch * aspects, classes)\n","        reshaped_labels = labels.view(-1)  # (batch * aspects,)\n","\n","        loss = loss_fn(reshaped_logits, reshaped_labels)\n","        total_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","\n","        if (i + 1) % 100 == 0:  # Print progress every 100 batches\n","            print(f\"  Batch {i + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","    epoch_time = time.time() - epoch_start_time\n","    print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f}s. Average Training Loss: {avg_train_loss:.4f}\")\n","\n","print(\"Transformer training finished.\")\n","# Consider saving the trained model\n","# model.save_pretrained('./my_aspect_sentiment_model')\n","# tokenizer.save_pretrained('./my_aspect_sentiment_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n6W-tvrY4dDm"},"outputs":[],"source":["# --- Save the Model and Tokenizer ---\n","print(\"\\nSaving the fine-tuned model and tokenizer...\")\n","\n","# Define the directory where you want to save them\n","drive.mount('/content/drive')  # Mount Google Drive\n","save_directory = '/content/drive/My Drive/Colab Notebooks' # You can change this path\n","# Save the model's weights and configuration file\n","model.save_pretrained(save_directory)\n","\n","# Save the tokenizer's vocabulary and configuration file\n","tokenizer.save_pretrained(save_directory)\n","\n","print(f\"Model and tokenizer saved to {save_directory}\")\n","\n","# --- (Optional) How to load the model and tokenizer later ---\n","# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","# loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n","# loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n","# print(\"\\nModel and tokenizer loaded successfully (example).\")\n","# loaded_model.to(device) # Remember to move the loaded model to the correct device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xBeOe4SyRD9W"},"outputs":[],"source":["print(\"\\nEvaluating Transformer Model...\")\n","model.eval()  # Set model to evaluation mode\n","\n","all_preds_transformer = []\n","all_labels_transformer = []\n","\n","with torch.no_grad():  # Disable gradient calculations for inference\n","    for batch in test_loader:\n","        # Move batch to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)  # Shape: (batch_size, num_aspects)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  # Shape: (batch_size, num_aspects * num_classes_per_aspect)\n","\n","        # Get predictions: Reshape logits and find the class with max probability for each aspect\n","        # Reshape logits to (batch_size, num_aspects, num_classes_per_aspect)\n","        reshaped_logits = logits.view(input_ids.size(0), num_aspects, num_classes_per_aspect)\n","        predictions = torch.argmax(reshaped_logits, dim=2)  # Get predicted class index along the class dimension\n","\n","        all_preds_transformer.extend(predictions.cpu().numpy())\n","        all_labels_transformer.extend(labels.cpu().numpy())\n","\n","# Convert collected predictions and labels into numpy arrays\n","Y_pred_transformer_np = np.array(all_preds_transformer)  # Shape: (num_test_samples, num_aspects)\n","Y_true_transformer_np = np.array(all_labels_transformer)  # Shape: (num_test_samples, num_aspects)\n","\n","# Calculate metrics per aspect\n","transformer_report = {}\n","print(\"\\n--- Transformer Classification Report (Per Aspect) ---\")\n","\n","all_true_flat_transformer = []\n","all_pred_flat_transformer = []\n","\n","# Assuming 'aspects' is defined earlier and includes 'overall'\n","# aspects = ['food', 'service', 'ambiance', 'price', 'context', 'overall']\n","\n","for i, aspect_name in enumerate(aspects):  # Iterate through all aspects, including 'overall'\n","    print(f\"\\n--- Aspect: {aspect_name} ---\")\n","    true_labels = Y_true_transformer_np[:, i]\n","    pred_labels = Y_pred_transformer_np[:, i]\n","\n","    all_true_flat_transformer.extend(true_labels)\n","    all_pred_flat_transformer.extend(pred_labels)\n","\n","    # Get unique labels in pred_labels and true_labels\n","    unique_labels = np.unique(np.concatenate((pred_labels, true_labels)))\n","\n","    # Filter target_names to include only the present labels\n","    present_target_names = [name for idx, name in enumerate(sentiment_classes) if idx in unique_labels]\n","\n","    report = classification_report(true_labels, pred_labels, target_names=present_target_names, zero_division=0)\n","    print(report)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted', zero_division=0)\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    transformer_report[aspect_name] = {'precision': precision, 'recall': recall, 'f1-score': f1, 'accuracy': accuracy}\n","\n","# Overall Micro and Macro Averages\n","print(\"\\n--- Transformer Overall Micro/Macro Averages ---\")\n","precision_micro_t, recall_micro_t, f1_micro_t, _ = precision_recall_fscore_support(all_true_flat_transformer, all_pred_flat_transformer, average='micro', zero_division=0)\n","precision_macro_t, recall_macro_t, f1_macro_t, _ = precision_recall_fscore_support(all_true_flat_transformer, all_pred_flat_transformer, average='macro', zero_division=0)\n","print(f\"Micro Average: Precision={precision_micro_t:.4f}, Recall={recall_micro_t:.4f}, F1-Score={f1_micro_t:.4f}\")\n","print(f\"Macro Average: Precision={precision_macro_t:.4f}, Recall={recall_macro_t:.4f}, F1-Score={f1_macro_t:.4f}\")\n","\n","transformer_report['overall_micro'] = {'precision': precision_micro_t, 'recall': recall_micro_t, 'f1-score': f1_micro_t}\n","transformer_report['overall_macro'] = {'precision': precision_macro_t, 'recall': recall_macro_t, 'f1-score': f1_macro_t}\n","\n","# Store transformer results for comparison plots\n","transformer_f1_scores = {aspect: metrics['f1-score'] for aspect, metrics in transformer_report.items() if 'overall' not in aspect}  # Exclude overall averages here if needed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJXIs83zyqgy"},"outputs":[],"source":["print(\"\\nEvaluating Transformer Model...\")\n","model.eval() # Set model to evaluation mode\n","\n","all_preds_transformer = []\n","all_labels_transformer = []\n","\n","with torch.no_grad(): # Disable gradient calculations for inference\n","    for batch in test_loader:\n","        # Move batch to device\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device) # Shape: (batch_size, num_aspects)\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits # Shape: (batch_size, num_aspects * num_classes_per_aspect)\n","\n","        # Get predictions: Reshape logits and find the class with max probability for each aspect\n","        # Reshape logits to (batch_size, num_aspects, num_classes_per_aspect)\n","        reshaped_logits = logits.view(input_ids.size(0), num_aspects, num_classes_per_aspect)\n","        predictions = torch.argmax(reshaped_logits, dim=2) # Get predicted class index along the class dimension\n","\n","        all_preds_transformer.extend(predictions.cpu().numpy())\n","        all_labels_transformer.extend(labels.cpu().numpy())\n","\n","# Convert collected predictions and labels into numpy arrays\n","Y_pred_transformer_np = np.array(all_preds_transformer) # Shape: (num_test_samples, num_aspects)\n","Y_true_transformer_np = np.array(all_labels_transformer) # Shape: (num_test_samples, num_aspects)\n","\n","# Calculate metrics per aspect\n","transformer_report = {}\n","print(\"\\n--- Transformer Classification Report (Per Aspect) ---\")\n","\n","all_true_flat_transformer = []\n","all_pred_flat_transformer = []\n","\n","\n","for i, aspect_name in enumerate(aspects):\n","    print(f\"\\n--- Aspect: {aspect_name} ---\")\n","    true_labels = Y_true_transformer_np[:, i]\n","    pred_labels = Y_pred_transformer_np[:, i]\n","\n","    all_true_flat_transformer.extend(true_labels)\n","    all_pred_flat_transformer.extend(pred_labels)\n","\n","    # Get unique labels in pred_labels and true_labels\n","    unique_labels = np.unique(np.concatenate((pred_labels, true_labels)))\n","\n","    # Filter target_names to include only the present labels\n","    present_target_names = [name for idx, name in enumerate(sentiment_classes) if idx in unique_labels]\n","\n","    report = classification_report(true_labels, pred_labels, target_names=present_target_names, zero_division=0)\n","    print(report)\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='weighted', zero_division=0)\n","    accuracy = accuracy_score(true_labels, pred_labels)\n","    transformer_report[aspect_name] = {'precision': precision, 'recall': recall, 'f1-score': f1, 'accuracy': accuracy}\n","\n","# Overall Micro and Macro Averages\n","print(\"\\n--- Transformer Overall Micro/Macro Averages ---\")\n","precision_micro_t, recall_micro_t, f1_micro_t, _ = precision_recall_fscore_support(all_true_flat_transformer, all_pred_flat_transformer, average='micro', zero_division=0)\n","precision_macro_t, recall_macro_t, f1_macro_t, _ = precision_recall_fscore_support(all_true_flat_transformer, all_pred_flat_transformer, average='macro', zero_division=0)\n","print(f\"Micro Average: Precision={precision_micro_t:.4f}, Recall={recall_micro_t:.4f}, F1-Score={f1_micro_t:.4f}\")\n","print(f\"Macro Average: Precision={precision_macro_t:.4f}, Recall={recall_macro_t:.4f}, F1-Score={f1_macro_t:.4f}\")\n","\n","transformer_report['overall_micro'] = {'precision': precision_micro_t, 'recall': recall_micro_t, 'f1-score': f1_micro_t}\n","transformer_report['overall_macro'] = {'precision': precision_macro_t, 'recall': recall_macro_t, 'f1-score': f1_macro_t}\n","\n","\n","# Store transformer results for comparison plots\n","transformer_f1_scores = {aspect: metrics['f1-score'] for aspect, metrics in transformer_report.items() if 'overall' not in aspect}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rpb7WZeUyzg5"},"outputs":[],"source":["# --- Define the sentiment classes corresponding to your integer labels ---\n","# This should match the 'bin_labels' used during binning (e.g., 0, 1, 2)\n","# and the 'label_names' (e.g., 'negative', 'neutral', 'positive')\n","sentiment_classes = ['negative', 'neutral', 'positive'] # MUST match the order used for bin_labels=[0, 1, 2]\n","integer_to_string_label_map = { i: label for i, label in enumerate(sentiment_classes) }\n","print(f\"Integer-to-String Label Map for Prediction: {integer_to_string_label_map}\")\n","\n","def predict_sentiment(review_text, model, tokenizer, aspects, int_to_str_map):\n","    \"\"\"Predicts sentiment for all aspects for a given review text.\"\"\"\n","\n","    # Preprocess the input text\n","    processed_text = preprocess_text(review_text) # Assumes preprocess_text is defined elsewhere\n","    if not processed_text:\n","        return {\"error\": \"Review text is empty after preprocessing.\"}\n","\n","    # Tokenize\n","    inputs = tokenizer(processed_text, return_tensors='pt', truncation=True, padding=True, max_length=128) # Adjust max_length if needed\n","\n","    # Move inputs to the same device as the model\n","    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","\n","    # Predict\n","    model.eval() # Ensure model is in eval mode\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        logits = outputs.logits # Shape: (1, num_aspects * num_classes_per_aspect)\n","\n","    # Process logits to get predictions per aspect\n","    num_classes_per_aspect = len(int_to_str_map) # Get number of classes (e.g., 3)\n","    # Reshape to (1, num_aspects, num_classes_per_aspect)\n","    reshaped_logits = logits.view(1, len(aspects), num_classes_per_aspect)\n","\n","    # Get the index of the max logit for each aspect (these are your 0, 1, 2 labels)\n","    predictions_indices = torch.argmax(reshaped_logits, dim=2).squeeze().cpu().numpy() # Shape: (num_aspects,)\n","\n","    # Handle case where there's only one aspect (numpy might return a scalar)\n","    if predictions_indices.ndim == 0:\n","        predictions_indices = [predictions_indices.item()] # Make it a list\n","\n","    # Map predicted integer indices back to sentiment string labels\n","    predicted_sentiments = {}\n","    for i, pred_idx in enumerate(predictions_indices):\n","         # Ensure the predicted index exists in the map, handle potential errors\n","         sentiment_label = int_to_str_map.get(pred_idx, \"Unknown Label\")\n","         predicted_sentiments[aspects[i]] = sentiment_label\n","\n","\n","    # # --- OLD MAPPING LOGIC (REMOVE/REPLACE) ---\n","    # # Map indices back to sentiment labels\n","    # # Create inverse map (0: 'neutral', 1: 'positive', -1: 'negative'} <--- This was likely incorrect\n","    # # inverse_label_map = {v: k for k, v in label_map.items()} <--- Remove this line\n","    # # predicted_sentiments = {aspects[i]: inverse_label_map[pred_idx] for i, pred_idx in enumerate(predictions_indices)} <--- Remove this line\n","    # # --- END OLD MAPPING LOGIC ---\n","\n","    return predicted_sentiments\n","\n","# --- Example Usage (Location: Near the end of the notebook, around page 20) ---\n","\n","# Ensure 'aspects' list is defined correctly (should match training)\n","aspects = ['food', 'service', 'ambiance', 'price', 'context', 'overall']\n","\n","# Define the mapping from integer labels (0, 1, 2) to strings ('negative', 'neutral', 'positive')\n","# This map MUST align with the 'bin_labels' used during binning.\n","# If bin_labels = [0, 1, 2] corresponds to ['negative', 'neutral', 'positive']\n","integer_to_string_label_map = {\n","    0: 'negative',\n","    1: 'neutral',\n","    2: 'positive'\n","}\n","print(f\"\\nUsing label map for prediction: {integer_to_string_label_map}\")\n","\n","\n","new_review = \"The pizza was amazing, truly authentic Italian style! However, the waiter was quite rude and ignored us for a long time.\"\n","# Pass the correct integer-to-string map to the function\n","predicted_results = predict_sentiment(new_review, model, tokenizer, aspects, integer_to_string_label_map)\n","\n","print(\"\\n--- Example Prediction ---\")\n","print(f\"Review: \\\"{new_review}\\\"\")\n","print(\"\\nPredicted Sentiments:\")\n","if \"error\" in predicted_results:\n","    print(predicted_results[\"error\"])\n","else:\n","    for aspect, sentiment in predicted_results.items():\n","        print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\") # Keep .capitalize() for display\n","\n","\n","new_review_2 = \"Service was terrible, and the food was bad and pricey. Terrible atmosphere.\"\n","predicted_results_2 = predict_sentiment(new_review_2, model, tokenizer, aspects, integer_to_string_label_map)\n","print(\"\\n--- Example Prediction 2 ---\")\n","print(f\"Review: \\\"{new_review_2}\\\"\")\n","print(\"\\nPredicted Sentiments:\")\n","if \"error\" in predicted_results_2:\n","    print(predicted_results_2[\"error\"])\n","else:\n","    for aspect, sentiment in predicted_results_2.items():\n","        print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n","\n","new_review_3 = \"This restaurant is bad.\"\n","predicted_results_3 = predict_sentiment(new_review_3, model, tokenizer, aspects, integer_to_string_label_map)\n","print(\"\\n--- Example Prediction 3 ---\") # Corrected print statement index\n","print(f\"Review: \\\"{new_review_3}\\\"\")\n","print(\"\\nPredicted Sentiments:\")\n","if \"error\" in predicted_results_3:\n","    print(predicted_results_3[\"error\"])\n","else:\n","    for aspect, sentiment in predicted_results_3.items():\n","        print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")\n","\n","new_review_4 = \"This restaurant is great. The food was yummy. The service was excellent. The price was inexpensive and the ambiance was outstanding. We went for my birthday.\"\n","predicted_results_4 = predict_sentiment(new_review_4, model, tokenizer, aspects, integer_to_string_label_map)\n","print(\"\\n--- Example Prediction 4 ---\") # Corrected print statement index\n","print(f\"Review: \\\"{new_review_4}\\\"\")\n","print(\"\\nPredicted Sentiments:\")\n","if \"error\" in predicted_results_4:\n","    print(predicted_results_4[\"error\"])\n","else:\n","    for aspect, sentiment in predicted_results_4.items():\n","        print(f\"- {aspect.capitalize()}: {sentiment.capitalize()}\")"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1QqEz_QbL280I6e3MQS1tS-2Fut2KN0J4","timestamp":1744424751362},{"file_id":"1X_B9TN5IhurE_EHPSnR8auvjax3rrED5","timestamp":1744378781643},{"file_id":"1QnqBNH9RmKDxa9oD2c5NTJH5ziinYa0x","timestamp":1744319285930},{"file_id":"13cDaLaW4uRM53OtgMNF1AdYpkYb3w3Ug","timestamp":1743836866981}],"authorship_tag":"ABX9TyO3TMggbZUk1LhUv70NuJj/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}